\newpage
\section{Methodik} \label{sec:Methodik}

\fixme{Vertiefung relevanter Themenbereiche basierend auf den Erkenntnissen aus der Artefakt-Entwicklung erfolgt unsystematisch...}

Abgrenzung Herleitung des Themas und Relevanz systematisch vs. DSR unsystematische Literaturrecherche bedarfsspezifisch?

\subsection{Systematische Literaturrecherche} \label{sec:Systematische Literaturrecherche}

Die systematische Literaturrecherche für diese Seminararbeit folgt einer Anzahl ausgewählter Methoden der PRISMA 2020 Richtlinien, dessen aktualisierte Leitlinien Transparenz und Vollständigkeit bei der Darstellung von Vorgehen und Ergebnissen systematischer Reviews fördern. Ziel ist es, den Erkenntnisstand zu einem spezifischen Forschungsproblem durch eine strukturierte Identifikation, Selektion, Bewertung und Synthese einschlägiger Studien methodisch nachvollziehbar zu dokumentieren, wodurch eine belastbare Grundlage für die Analyse der Forschungslücke und die Ableitung von Forschungsfragen geschaffen wird \parencite[S. 1--3]{page_PRISMA2020Statementupdatedguidelinereportingsystematicreviews_2021}. Die PRISMA 2020 Richtlinien fordern u.a. die offene Darlegung der Suchstrategien, der Auswahlkriterien sowie der Bewertungs- und Syntheseverfahren, um die Nachvollziehbarkeit und Replizierbarkeit des Forschungsprozesses sicherzustellen \parencite[S. 1--6]{page_PRISMA2020Explanationelaborationupdatedguidanceexemplarsreportingsystematicreviews_2021}.

Die systematische Literaturrecherche wurde in zwei zeitlich getrennten Iterationen durchgeführt, um den evolving Charakter des Forschungsfeldes zu adressieren und die Aktualität der Wissensbasis sicherzustellen. Beide Iterationen sind ausführlich dokumentiert in \ref{sec:Anhang_Dokumentation_der_systematischen_Literaturrecherche} und dienen ausschließlich der initialen Problemidentifikation, der Ergründung des Forschungsstands sowie der Ableitung der Forschungslücke und initialen Forschungsfragen.

\fixme{während DSR ==> unsystematische Literaturrecherche nach Bedarf...}



\textbf{Konsolidierte Ergebnisse beider Iterationen}

Die erste Iteration erfolgte im Rahmen des Exposés am 30. Mai 2025 (\ref{sec:Anhang_Dokumentation_der_ersten_Iteration_Expose}). Sie identifizierte 61 Quellen, die anhand von Titel und Abstract nach Relevanz klassifiziert wurden (\autoref{tab:A-1}). Diese Klassifikation wurde für die Masterarbeit beibehalten, da sie dem methodischen Standard eines Exposés entspricht und zur Identifikation der Forschungslücke sowie zur Formulierung der Forschungsfragen ausreichend war.

Die zweite Iteration im Rahmen der Masterarbeit (\ref{sec:Anhang_Dokumentation_der_ersten_Iteration}) ergänzt diese Wissensbasis um [W] neue Quellen, die einem vollständigen zweistufigen Screening-Prozess nach PRISMA 2020 unterzogen wurden. Dies gewährleistet, dass alle für die Masterarbeit neu hinzugefügten Quellen den höheren wissenschaftlichen Standards systematischer Reviews genügen.

Über beide Iterationen hinweg stehen somit insgesamt [61 + W] Quellen für die Analyse des Forschungsstands, die Ableitung der theoretischen Grundlagen und die Entwicklung des SSI-Prototyps zur Verfügung. Die detaillierte Bewertung aller Quellen ist in \autoref{tab:A-1} (Iteration 1) und \autoref{tab:bewertung_iteration2} (Iteration 2) dokumentiert.




---







\autoref{tab:quellenuebersicht} stellt eine Übersicht der Bewertung der 61 \fixme{72} identifizierten Quellen dar, welche vollständig in \ref{sec:Anhang_Systematische Literaturrecherche} aufzufinden ist.

Die Einstufung basiert auf Titel und Abstract in Bezug auf den thematischen Fokus der Arbeit. Hohe Relevanz erhalten Quellen mit klaren Beiträgen zu \ac{SSI}, \ac{PQC}, \ac{KRITIS} oder dezentralen Identitätsarchitekturen. Mittlere Relevanz wird Arbeiten zugeordnet, die angrenzende Technologien wie Blockchain-Sicherheit im \ac{IoT} oder digitale Forensik behandeln. Niedrige Relevanz erhalten Quellen zu allgemeinen Technologietrends ohne direkten Bezug zum Thema.

\begin{longtable}{L{1.5cm}L{11cm}L{1cm}}
    \caption{Übersicht der Bewertung der identifizierten Quellen hinsichtlich ihrer Relevanz}
    \label{tab:quellenuebersicht} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endfirsthead
    \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf \autoref{tab:quellenbewertung} und Titel und Abstracts von \textcite{szymanski_QuantumSafeSoftwareDefinedDeterministicInternetThingsIoTHardwareEnforcedCyberSecurityCriticalInfrastructures_2024,nouma_TrustworthyEfficientDigitalTwinsPostQuantumEraHybridHardwareAssistedSignatures_2024,sharif_EIDASRegulationSurveyTechnologicalTrendsEuropeanElectronicIdentitySchemes_2022,alam_PrivatelyGeneratedKeyPairsPostQuantumCryptographyDistributedNetwork_2024,radanliev_ReviewComparisonUSEUUKRegulationsCyberRiskSecurityCurrentBlockchainTechnologies_2023}.} \\
    \endlastfoot
    1 & Szymanski, T. H. (2024). A Quantum-Safe Software-Defined Deterministic Internet of Things (IoT) with Hardware-Enforced Cyber-Security for Critical Infrastructures. Information (2078-2489), 15(4), 173. \url{https://doi.org/10.3390/info15040173} & Hoch \\
    \midrule
    2 & Nouma, S. E., \& Yavuz, A. A. (2024). Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid Hardware-Assisted Signatures. ACM Transactions on Multimedia Computing, Communications \& Applications, 20(6), 1–30. \url{https://doi.org/10.1145/3638250} & Hoch \\
    \midrule
    3 & Sharif, A., Ranzi, M., Carbone, R., Sciarretta, G., Marino, F. A., \& Ranise, S. (2022). The eIDAS Regulation: A Survey of Technological Trends for European Electronic Identity Schemes. Applied Sciences (2076-3417), 12(24), 12679. \url{https://doi.org/10.3390/app122412679} & Hoch \\
    \midrule
    4 & Alam, M., Hoffstein, J., \& Cambou, B. (2024). Privately Generated Key Pairs for Post Quantum Cryptography in a Distributed Network. Applied Sciences (2076-3417), 14(19), 8863. \url{https://doi.org/10.3390/app14198863} & Hoch \\
    \midrule
    5 & Radanliev, P. (2023). Review and Comparison of US, EU, and UK Regulations on Cyber Risk/Security of the Current Blockchain Technologies: Viewpoint from 2023. Review of Socionetwork Strategies, 17(2), 105–129. \url{https://doi.org/10.1007/s12626-023-00139-x} & Hoch \\
    \midrule
    6--38 & Diverse & Mittel  \\
    \midrule
    39--61 & Diverse & Niedrig \\
\end{longtable}

\subsection{Design Science Research} \label{Design Science Research}

Die systematische Entwicklung und Evaluation eines blockchain-basierten Self-Sovereign-Identity-Prototyps mit Post-Quantum-Kryptografie für kritische Infrastrukturen erfordert einen Forschungsansatz, der sowohl die wissenschaftliche Fundierung als auch die praktische Anwendbarkeit der entwickelten Lösung gewährleistet.
\ac{DSR} erweist sich als besonders geeigneter methodischer Rahmen für dieses Vorhaben, da dieser Ansatz explizit darauf abzielt, innovative IT-Artefakte zu schaffen, die zur Erweiterung menschlicher und organisationaler Fähigkeiten beitragen \parencite[S. 75]{hevner_DesignScienceInformationsystemsresearch_2004}. 

Im Kern verfolgt \ac{DSR} einen problemlösungsorientierten Ansatz, der in den Ingenieurwissenschaften und den Wissenschaften des \fixme{Künstlichen (SIMON 1996)} verwurzelt ist. Das zentrale Ziel besteht darin, Innovationen zu schaffen, die Ideen, Praktiken, technische Fähigkeiten und Produkte definieren, durch welche die Analyse, das Design, die Implementierung, das Management und die Nutzung von Informationssystemen effektiv und effizient erreicht werden können \parencite[S. 76]{hevner_DesignScienceInformationsystemsresearch_2004}.

Die besondere Eignung von DSR für das vorliegende Forschungsvorhaben begründet sich durch mehrere zentrale Aspekte:

Erstens adressiert diese Arbeit ein hochrelevantes, praxisorientiertes Problem: die drohende Verwundbarkeit bestehender SSI-Systeme durch leistungsfähige Quantencomputer sowie die spezifischen Sicherheits- und Compliance-Anforderungen kritischer Infrastrukturen. Das primäre Forschungsziel liegt in der Entwicklung technologiebasierter Lösungen für bedeutende und relevante Geschäftsprobleme \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Zweitens erfordert die Integration von Post-Quantum-Kryptografie in blockchain-basierte SSI-Systeme die Schaffung eines neuartigen IT-Artefakts. DSR fordert explizit, dass Forschungsprojekte ein praktikables Artefakt in Form eines Konstrukts, Modells, einer Methode oder einer Instanziierung hervorbringen müssen \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Drittens ermöglicht DSR die systematische Evaluation des entwickelten Artefakts. Der Nutzen, die Qualität und die Wirksamkeit eines Design-Artefakts müssen durch gut durchgeführte Evaluationsmethoden rigoros nachgewiesen werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Viertens unterstützt DSR den wissenschaftlichen Erkenntnisgewinn durch die systematische Dokumentation von Forschungsbeiträgen. Effektive Design-Science-Forschung muss klare und verifizierbare Beiträge in den Bereichen Design-Artefakt, Design-Grundlagen und/oder Design-Methodologien liefern \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Fünftens gewährleistet DSR wissenschaftliche Strenge durch die Anwendung rigoroser Methoden sowohl bei der Konstruktion als auch bei der Evaluation des Design-Artefakts \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Die Methodenwahl DSR ist somit nicht nur durch die Passung zur Forschungsfrage begründet, sondern auch durch die inhärenten Anforderungen des Forschungsvorhabens: die Notwendigkeit, ein funktionsfähiges Artefakt zu entwickeln, dessen Nutzen für die Praxis nachzuweisen und gleichzeitig einen wissenschaftlichen Beitrag zur Wissensbasis zu leisten. DSR schafft hierfür den methodischen Rahmen, der Relevanz und Rigorosität gleichermaßen adressiert und die Brücke zwischen theoretischer Fundierung und praktischer Anwendbarkeit bildet.

\subsubsection{Drei-Zyklen-Modell} \label{Drei-Zyklen-Modell}

Das Drei-Zyklen-Modell von \textcite[S. 87]{hevner_ThreeCycleViewDesignScienceResearch_2007}, dargestellt in \autoref{fig:Design Science Research Cycles}, stellt die enge Verbindung der drei Aktivitätszyklen, dem Relevance Cycle, dem Rigor Cycle und dem Design Cycle, in der Design-Science-Forschung dar.
 
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{3-cycle.png}
    \caption{Design Science Research Cycles}
    \begin{flushleft}
    \textit{Anmerkung.} Aus \textcite[S. 88]{hevner_ThreeCycleViewDesignScienceResearch_2007}.
    \end{flushleft}
    \label{fig:Design Science Research Cycles}
\end{figure}

Diese drei Zyklen stellen sicher, dass Design-Science-Forschung sowohl relevante Probleme aus der Anwendungsdomäne adressiert als auch auf einer soliden wissenschaftlichen Grundlage aufbaut, während gleichzeitig ein iterativer Entwicklungs- und Evaluationsprozess ermöglicht wird.

\paragraph*{Relevance Cycle: Integration von KRITIS-Anforderungen und Praxisanforderungen} \label{Relevance Cycle: Integration von KRITIS-Anforderungen und Praxisanforderungen}

Der Relevance Cycle initiiert die Design-Science-Forschung mit einem Anwendungskontext, der nicht nur die Anforderungen an die Forschung liefert, sondern auch die Akzeptanzkriterien für die letztendliche Evaluation der Forschungsergebnisse definiert \parencite[S. 89]{hevner_ThreeCycleViewDesignScienceResearch_2007}.

Im Kontext dieser Arbeit manifestiert sich der Relevance Cycle durch die Integration mehrschichtiger Anforderungen aus dem KRITIS-Umfeld:

\textbf{KRITIS-spezifische Sicherheitsanforderungen:} Die Zuverlässigkeit, Leistungsfähigkeit und Sicherheit kritischer Infrastrukturen stellen elementare nationale Prioritäten dar, da sie für die moderne Gesellschaft essenziell sind \parencite[S. 1]{alcaraz_CriticalInfrastructureProtectionRequirementschallenges21stcentury_2015}. Der Relevance Cycle adressiert konkret die besonderen Schutzbedarfe von Sektoren wie Energie, Gesundheit, Wasser oder Telekommunikation, deren Ausfall oder Beeinträchtigung erhebliche Versorgungsengpässe, Gefährdungen der öffentlichen Sicherheit oder andere dramatische Folgen nach sich ziehen würde.

\textbf{Regulatorische Vorgaben:} Die Anforderungen aus dem Relevance Cycle umfassen insbesondere die Einhaltung des IT-Sicherheitsgesetzes 2.0, der NIS2-Richtlinie sowie der Vorgaben des Bundesamts für Sicherheit in der Informationstechnik (BSI). Diese regulatorischen Rahmenbedingungen definieren Mindeststandards für Informationssicherheit, Meldepflichten bei Sicherheitsvorfällen und organisatorische Maßnahmen, die in das Design des Artefakts einfließen müssen.

\textbf{Datenschutz- und Compliance-Anforderungen:} Der Relevance Cycle integriert die Anforderungen der Datenschutz-Grundverordnung (DSGVO), insbesondere die Prinzipien Privacy by Design und Privacy by Default. Diese Anforderungen stellen sicher, dass der entwickelte Prototyp nicht nur technisch funktionsfähig ist, sondern auch die Grundrechte auf informationelle Selbstbestimmung und Datenschutz wahrt \fixme{QUELLE}.

\textbf{Quantenbedrohung und kryptografische Agilität:} Der Relevance Cycle adressiert die praktische Herausforderung, dass bestehende kryptografische Verfahren durch die Entwicklung leistungsfähiger Quantencomputer gefährdet sind. Dies erfordert die Integration von Post-Quantum-Algorithmen sowie die Implementierung kryptoagiler Mechanismen, die zukünftige Algorithmus-Updates ohne Systemunterbrechung ermöglichen.

\textbf{Feldtest und Evaluation:} Der Relevance Cycle schließt sich durch die Evaluation des entwickelten Prototyps anhand definierter Use Cases aus dem KRITIS-Umfeld. Die Bewertung erfolgt anhand von Akzeptanzkriterien, die sich aus den identifizierten Anforderungen ableiten: Funktionalität, Sicherheit, Performance, Compliance-Konformität und Zukunftsfähigkeit durch kryptografische Agilität.

Die kontinuierliche Rückkopplung zwischen Anwendungsumfeld und Forschungsaktivitäten stellt sicher, dass das entwickelte Artefakt nicht nur theoretisch fundiert, sondern auch praktisch relevant und anwendbar ist.

\paragraph*{Rigor Cycle: Einbindung wissenschaftlicher Erkenntnisse und systematischer Literaturrecherche} \label{Rigor Cycle: Einbindung wissenschaftlicher Erkenntnisse und systematischer Literaturrecherche}

Der Rigor Cycle liefert vergangenes Wissen an das Forschungsprojekt, um dessen Innovation sicherzustellen. Es obliegt den Forschenden, die Wissensbasis gründlich zu recherchieren und zu referenzieren, um zu gewährleisten, dass die entwickelten Designs Forschungsbeiträge darstellen und nicht Routinedesigns auf Basis wohlbekannter Prozesse sind \parencite[S. 90]{hevner_ThreeCycleViewDesignScienceResearch_2007}.

Im Rahmen dieser Arbeit manifestiert sich der Rigor Cycle durch mehrere zentrale Komponenten:

\textbf{Systematische Literaturrecherche nach PRISMA 2020 und unsystematische Recherche im DSR-Kontext:} 
Die Arbeit folgt einem hybriden Review-Ansatz, der zeitlich und funktional klar differenziert ist.

Die initiale, systematische Literaturrecherche nach ausgewählten Methoden der PRISMA 2020-Richtlinien (Kapitel 3.1) dient ausschließlich der \textbf{initialen Problemidentifikation, Ergrundung des Forschungsstands und Ableitung der Forschungslücke}. Diese systematische Vorgehensweise gewährleistet einen transparenten reproduzierbaren Prozess und verbessert die Berichtqualität \fixme{PRISMA-Quelle}.

\textbf{Davon abzugrenzen} ist die \textbf{bedarfsorientierte, unsystematische Literaturrecherche} während der iterativen Artefaktentwicklung im Design Cycle (Kapitel 4). Diese orientiert sich am iterativen Review-Ansatz nach vom \textcite[S. 208--209]{brocke_StandingShouldersGiantsChallengesRecommendationsLiteratureSearchInformationSystemsResearch_2015} und erfolgt \textbf{ad-hoc und problemspezifisch} zur Lösung konkreter technischer und methodischer Fragestellungen, die sich iterativ aus den Entwicklungs- und Evaluationszyklen ergeben. Diese unsystematische Recherche umfasst:

\begin{itemize}
  \item Technische Dokumentationen zu PQC-Algorithmen (NIST FIPS 203, 204, 205)
  \item Framework-spezifische Ressourcen (Hyperledger Aries/ACA-Py, liboqs, OpenSSL)
  \item BSI-Standards und Compliance-Vorgaben
  \item Ad-hoc-Literatur zu identifizierten Implementierungsherausforderungen
\end{itemize}

Im Gegensatz zur initialen systematischen Recherche ist diese bedarfsgesteuerte Suche \textbf{nicht vollständig dokumentiert nach PRISMA-Standards}, sondern erfolgt iterativ und problemspezifisch zur Lösung konkreter Designherausforderungen während der Prototypentwicklung. Dies entspricht dem explorativen Charakter des Design Science Research, bei dem das Artefakt iterativ entwickelt und evaluiert wird \fixme{DSR-Quelle}.



\textbf{Wissenschaftliche Grundlagen aus Referenzdisziplinen:} Der Rigor Cycle integriert theoretische und methodische Grundlagen aus mehreren Disziplinen:

    Kryptografie: Insbesondere die standardisierten Post-Quantum-Algorithmen des NIST (ML-KEM, ML-DSA, SLH-DSA) sowie Prinzipien der kryptografischen Agilität

    Blockchain-Technologie: Distributed-Ledger-Architekturen, Konsensmechanismen, Smart Contracts und deren Anwendung im Identitätsmanagement

    Self-Sovereign Identity: Konzeptionelle Grundlagen, Standards (DID, Verifiable Credentials) und bestehende Framework-Implementierungen

    Informationssicherheit für KRITIS: BSI-Vorgaben, Sicherheitsstandards, Compliance-Anforderungen und Best Practices

\textbf{Evaluation bestehender Artefakte und Frameworks:} Der Rigor Cycle umfasst die systematische Analyse und Bewertung existierender SSI-Frameworks (z.B. Hyperledger Indy/Aries, Walt.ID, Veramo, Cheqd) hinsichtlich ihrer PQC-Kompatibilität, Erweiterbarkeit und Eignung für KRITIS-Anwendungen.

\textbf{Methodische Strenge in Konstruktion und Evaluation:} Die Anwendung rigoroser Methoden erfolgt sowohl bei der Entwicklung des Prototyps als auch bei dessen Evaluation. Hierzu zählen formale Sicherheitsanalysen der kryptografischen Verfahren, Performance-Messungen, Funktionalitätstests sowie die Anwendung des Framework for Evaluation in Design Science (FEDS).

\textbf{Beitrag zur Wissensbasis:} Der Rigor Cycle schließt sich durch die Rückführung neuer Erkenntnisse in die wissenschaftliche Wissensbasis. Dies umfasst:

    Gestaltungsprinzipien für quantenresistente SSI-Systeme in KRITIS

    Evaluationsergebnisse zur Performance und Skalierbarkeit PQC-basierter Implementierungen

    Methodische Erkenntnisse zur Integration von Post-Quantum-Kryptografie in bestehende SSI-Frameworks

    Praktische Handlungsempfehlungen für die Umsetzung kryptoagiler Architekturen

Die enge Verzahnung des Rigor Cycle mit dem Design Cycle stellt sicher, dass die Entwicklung des Artefakts auf einer soliden wissenschaftlichen Grundlage erfolgt und gleichzeitig neue, über den Stand der Technik hinausgehende Erkenntnisse generiert werden.

\paragraph*{Design Cycle: Iterativer Entwicklungs- und Evaluationsprozess des Prototyps} \label{Design Cycle: Iterativer Entwicklungs- und Evaluationsprozess des Prototyps}

Der interne Design Cycle bildet das Herzstück jedes Design-Science-Forschungsprojekts. Dieser Zyklus iteriert zwischen dem Aufbau eines Artefakts, dessen Evaluation und dem anschließenden Feedback zur weiteren Verfeinerung des Designs \parencite[S. 90-91]{hevner_ThreeCycleViewDesignScienceResearch_2007}.

Im Kontext dieser Arbeit manifestiert sich der Design Cycle durch einen systematischen, mehrstufigen Entwicklungs- und Evaluationsprozess:

\textbf{Phase 1: Architekturdesign und Technologieauswahl}

Der Design Cycle beginnt mit der konzeptionellen Gestaltung der Systemarchitektur. Basierend auf den Anforderungen aus dem Relevance Cycle und den wissenschaftlichen Grundlagen aus dem Rigor Cycle werden fundamentale Designentscheidungen getroffen:

    Auswahl eines geeigneten SSI-Frameworks (z.B. Hyperledger Indy/Aries)

    Festlegung der Blockchain-Plattform (permissioned Ledger)

    Definition der PQC-Algorithmen für verschiedene Anwendungsfälle (Signaturen, Schlüsselaustausch, Hashing)

    Konzeption der kryptoagilen Architektur zur Ermöglichung zukünftiger Algorithmus-Updates

Evaluation: Bewertung der Architekturentscheidungen anhand definierter Kriterien (PQC-Kompatibilität, Erweiterbarkeit, Compliance-Konformität, Performance-Potenzial)

\textbf{Phase 2: Implementierung der Kernkomponenten}

Die Entwicklung des Prototyps erfolgt modular in einer containerisierten Laborumgebung (Docker, Ubuntu 24.04 LTS). Zentrale Komponenten umfassen:

    Kryptografische Schlüsselverwaltung: Integration von PQC-Algorithmen (ML-DSA, ML-KEM) für Schlüsselgenerierung, -speicherung und -verwaltung

    DID-Management: Implementierung dezentraler Identifikatoren mit PQC-basierten Signaturen

    Verifiable Credentials: Erstellung, Signierung und Verifikation von Credentials mit quantenresistenten Verfahren

    Wallet-Funktionalität: Agenten-Software für Holder, Issuer und Verifier

    Blockchain-Integration: Anbindung an permissioned Ledger mit PQC-gesicherten Transaktionen

Evaluation: Funktionalitätstests der einzelnen Komponenten, Unit-Tests, Integrationstests

\textbf{Phase 3: Systemintegration und Interoperabilität}

Nach erfolgreicher Implementierung der Kernkomponenten erfolgt die Integration zum Gesamtsystem. Dabei werden die Schnittstellen zwischen SSI-Komponenten und Blockchain-Backend sowie die End-to-End-Workflows getestet.

Evaluation: Systemtests, Use-Case-Validierung anhand definierter KRITIS-Szenarien

\textbf{Phase 4: Performance- und Sicherheitsanalyse}

Die Evaluation des integrierten Systems umfasst:

    Performance-Messungen: Durchsatz, Latenz, Speicher- und Rechenaufwand der PQC-Operationen

    Skalierbarkeitsanalyse: Bewertung des Systemverhaltens unter Last

    Sicherheitsbewertung: Kryptografische Stärke, Analyse potenzieller Angriffsvektoren, Resilience-Tests

    Compliance-Validierung: Überprüfung der Einhaltung regulatorischer Anforderungen

\textbf{Phase 5: Refinement und Iteration}

Basierend auf den Evaluationsergebnissen erfolgt eine iterative Verfeinerung des Designs. Identifizierte Schwachstellen, Performance-Engpässe oder Funktionsdefizite führen zu Designanpassungen, die wiederum evaluiert werden. Dieser iterative Prozess wird solange durchlaufen, bis ein zufriedenstellendes Design erreicht ist, das die definierten Anforderungen erfüllt.

Die Such- und Optimierungsstrategie des Design Cycle folgt dem Prinzip, verfügbare Mittel zu nutzen, um gewünschte Ziele zu erreichen, während gleichzeitig die Gesetzmäßigkeiten der Problemumgebung berücksichtigt werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Die enge Verzahnung von Konstruktion und Evaluation im Design Cycle stellt sicher, dass das entwickelte Artefakt nicht nur technisch realisierbar ist, sondern auch die definierten Qualitäts- und Nutzenanforderungen erfüllt. Die Erkenntnisse aus jeder Iteration fließen sowohl in die weitere Verfeinerung des Prototyps als auch in die Wissensbasis (Rigor Cycle) und die Bewertung der praktischen Anwendbarkeit (Relevance Cycle) ein.

\subsubsection{DSR-Leitlinien} \label{DSR-Leitlinien}

Die sieben DSR-Leitlinien nach \textcite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}, dargestellt in \autoref{tab:DSR_Guidelines} bilden einen strukturierten Rahmen zur Durchführung und Bewertung von Design-Science-Forschung. Im Folgenden wird dargelegt, wie diese Arbeit die einzelnen Leitlinien erfüllt.

\begin{longtable}{L{4cm}L{7cm}}
    \caption{Design Science Research Guidelines}
    \label{tab:DSR_Guidelines} \\
    \toprule
    \textbf{Guideline} & \textbf{Description} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Guideline} & \textbf{Description} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} In Anlehnung an \textcite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.} \\
    \endlastfoot
    Guideline 1: Design as an Artifact & Design-science research must produce a viable artifact in the form of a construct, a model, a method, or an instantiation. \\
    \midrule
    Guideline 2: Problem Relevance & The objective of design-science research is to develop technology-based solutions to important and relevant business problems. \\
    \midrule
    Guideline 3: Design Evaluation & The utility, quality, and efficacy of a design artifact must be rigorously demonstrated via well-executed evaluation methods. \\
    \midrule
    Guideline 4: Research Contributions & Effective design-science research must provide clear and verifiable contributions in the areas of the design artifact, design foundations, and/or design methodologies. \\
    \midrule
    Guideline 5: Research Rigor & Design-science research relies upon the application of rigorous methods in both the construction and evaluation of the design artifact. \\
    \midrule
    Guideline 6: Design as a Search Process & The search for an effective artifact requires utilizing available means to reach desired ends while satisfying laws in the problem environment. \\
    \midrule
    Guideline 7: Communication of Research & Design-science research must be presented effectively both to technology-oriented as well as management-oriented audiences. \\
\end{longtable}

\paragraph*{Guideline 1: Design als Artefakt} \label{Guideline 1: Design als Artefakt}

Design-Science-Forschung muss ein praktikables Artefakt in Form eines Konstrukts, Modells, einer Methode oder einer Instanziierung hervorbringen \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Diese Arbeit entwickelt mehrere miteinander verknüpfte Artefakte:

    Instanziierung: Ein funktionsfähiger Prototyp eines blockchain-basierten SSI-Systems mit integrierten PQC-Algorithmen, implementiert in einer Laborumgebung

    Konstrukte: Kryptografische Abstraktionsschichten für kryptoagile Systemarchitekturen, PQC-kompatible DID-Methoden

    Modelle: Systemarchitektur für quantenresistente SSI in KRITIS, Integration von PQC in SSI-Workflows

    Methoden: Vorgehensmodell zur Integration von PQC in bestehende SSI-Frameworks, Evaluationsmethodik für quantenresistente Identitätssysteme

\paragraph*{Guideline 2: Problemrelevanz} \label{Guideline 2: Problemrelevanz}

Das Ziel von Design-Science-Forschung besteht darin, technologiebasierte Lösungen für wichtige und relevante Geschäftsprobleme zu entwickeln \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Die Arbeit adressiert ein hochrelevantes Problem von nationaler und internationaler Bedeutung:

    Quantenbedrohung: Bestehende kryptografische Verfahren in SSI-Systemen sind durch leistungsfähige Quantencomputer bedroht

    KRITIS-Sicherheit: Kritische Infrastrukturen stellen besondere Anforderungen an Verfügbarkeit, Integrität und Vertraulichkeit digitaler Identitäten

    Regulatorische Compliance: IT-Sicherheitsgesetz 2.0, NIS2-Richtlinie und BSI-Vorgaben erfordern nachweislich sichere Lösungen

    Zukunftsfähigkeit: Die Notwendigkeit kryptoagiler Systeme zur Anpassung an zukünftige kryptografische Entwicklungen

Die Problemstellung ist sowohl wissenschaftlich als auch praktisch hochrelevant und tangiert unmittelbar die Sicherheit und Funktionsfähigkeit essenzieller gesellschaftlicher Infrastrukturen.

\paragraph*{Guideline 3: Design-Evaluation} \label{Guideline 3: Design-Evaluation}

Nutzen, Qualität und Wirksamkeit eines Design-Artefakts müssen durch gut durchgeführte Evaluationsmethoden rigoros nachgewiesen werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Die Evaluation erfolgt multidimensional und systematisch:

    Funktionalitätstests: Validierung der Kernfunktionen (DID-Erstellung, Credential-Issuance, Verification) anhand definierter Use Cases aus dem KRITIS-Umfeld

    Performance-Analyse: Quantitative Messungen von Durchsatz, Latenz, Speicher- und Rechenaufwand der PQC-Operationen

    Skalierbarkeitsanalyse: Bewertung des Systemverhaltens unter verschiedenen Lastszenarien

    Sicherheitsbewertung: Analyse der kryptografischen Stärke, Identifikation von Angriffsvektoren, Resilience-Tests

    Compliance-Validierung: Überprüfung der Einhaltung regulatorischer Anforderungen (BSI, DSGVO, IT-SiG 2.0)

    FEDS-Framework: Systematische Anwendung formativer und summativer Evaluationsstrategien

Die Evaluationsmethodik orientiert sich am Framework for Evaluation in Design Science (FEDS) und kombiniert künstliche Evaluationsparadigmen (Labor) mit naturalistischen Aspekten (realitätsnahe Use Cases).

\paragraph*{Guideline 4: Forschungsbeiträge} \label{Guideline 4: Forschungsbeiträge}

Effektive Design-Science-Forschung muss klare und verifizierbare Beiträge in den Bereichen Design-Artefakt, Design-Grundlagen und/oder Design-Methodologien liefern \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Die Arbeit leistet Beiträge auf mehreren Ebenen:

Design-Artefakt:

    Erster funktionsfähiger Prototyp eines SSI-Systems mit vollständiger PQC-Integration für KRITIS

    Nachweis der technischen Machbarkeit quantenresistenter SSI-Architekturen

    Kryptoagile Systemarchitektur zur Unterstützung zukünftiger Algorithmus-Updates

Design-Grundlagen:

    Gestaltungsprinzipien für die Integration von PQC in SSI-Systeme

    Systematische Bewertung der Eignung verschiedener PQC-Algorithmen für unterschiedliche SSI-Komponenten

    Erkenntnisse zu Trade-offs zwischen Sicherheit, Performance und Praktikabilität

Design-Methodologien:

    Vorgehensmodell zur PQC-Migration in bestehenden SSI-Frameworks

    Evaluationsmethodik für quantenresistente Identitätssysteme

    Best Practices für die Implementierung kryptoagiler Architekturen in KRITIS

Diese Beiträge erweitern die Wissensbasis sowohl für die wissenschaftliche Community als auch für Praktiker in KRITIS-Organisationen.


\paragraph*{Guideline 5: Forschungsstrenge} \label{Guideline 5: Forschungsstrenge}

Design-Science-Forschung beruht auf der Anwendung rigoroser Methoden sowohl bei der Konstruktion als auch bei der Evaluation des Design-Artefakts \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Die Forschungsstrenge wird durch mehrere Aspekte gewährleistet:

Konstruktion:

    Systematische Anforderungsanalyse basierend auf wissenschaftlicher Literatur und regulatorischen Vorgaben

    Fundierte Technologieauswahl durch kriterienbasierte Evaluation bestehender Frameworks

    Implementierung nach etablierten Software-Engineering-Prinzipien (Modularität, Testbarkeit, Dokumentation)

    Einsatz standardisierter PQC-Algorithmen (NIST FIPS 203, 204, 205)

Evaluation:

    Systematische Literaturrecherche nach PRISMA 2020-Richtlinien

    Anwendung etablierter Evaluationsframeworks (FEDS)

    Reproduzierbare Performance-Messungen in kontrollierter Laborumgebung

    Formale Sicherheitsanalysen der kryptografischen Verfahren

    Systematische Dokumentation aller Designentscheidungen und Evaluationsergebnisse

Wissenschaftliche Fundierung:

    Rückgriff auf etablierte Theorien und Methoden aus Kryptografie, Distributed Systems und Informationssicherheit

    Bezugnahme auf aktuelle wissenschaftliche Literatur und Standards

    Kritische Reflexion von Limitationen und Annahmen

\paragraph*{Guideline 6: Design als Suchprozess} \label{Guideline 6: Design als Suchprozess}

Die Suche nach einem effektiven Artefakt erfordert die Nutzung verfügbarer Mittel, um gewünschte Ziele zu erreichen, während gleichzeitig die Gesetzmäßigkeiten der Problemumgebung berücksichtigt werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Der Suchprozess manifestiert sich durch:

Verfügbare Mittel:

    Bestehende SSI-Frameworks (Hyperledger Indy/Aries) als Ausgangsbasis

    Standardisierte PQC-Algorithmen (ML-KEM, ML-DSA, SLH-DSA)

    Open-Source-Bibliotheken (liboqs, OpenSSL 3.x)

    Containerisierte Entwicklungsumgebung (Docker)

    Blockchain-Plattformen (Indy-Besu, Hedera)

Gewünschte Ziele:

    Quantenresistente SSI-Architektur

    KRITIS-konforme Sicherheit und Compliance

    Praktikable Performance und Skalierbarkeit

    Kryptografische Agilität

Gesetzmäßigkeiten der Problemumgebung:

    Physikalische Grenzen der PQC-Performance (größere Schlüssel, langsamere Operationen)

    Blockchain-Charakteristika (Unveränderlichkeit, begrenzte Transaktionsgröße)

    Regulatorische Constraints (Datenschutz, Nachweispflichten)

    Bestehende Standards und Interoperabilitätsanforderungen

Der iterative Design Cycle ermöglicht die systematische Exploration des Lösungsraums und die schrittweise Annäherung an ein optimales Design unter Berücksichtigung konkurrierender Anforderungen und Randbedingungen.

\paragraph*{Guideline 7: Kommunikation der Forschung} \label{Guideline 7: Kommunikation der Forschung}

Design-Science-Forschung muss sowohl für technologieorientierte als auch für managementorientierte Zielgruppen effektiv präsentiert werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}.

Erfüllung: Die Kommunikation der Forschungsergebnisse erfolgt zielgruppenspezifisch:

Für technologieorientierte Zielgruppen (Forscher, Entwickler):

    Detaillierte Beschreibung der Systemarchitektur und Implementierung

    Technische Spezifikation der PQC-Integration

    Performance-Metriken und Benchmark-Ergebnisse

    Quellcode-Dokumentation und Deployment-Anleitungen

    Reproduzierbare Evaluationsergebnisse

Für managementorientierte Zielgruppen (KRITIS-Betreiber, Entscheidungsträger):

    Zusammenfassung der Problemstellung und Lösungsansätze

    Business Case für quantenresistente SSI-Systeme

    Compliance-Konformität und regulatorische Erfüllung

    Implementierungsrichtlinien und Migrationsstrategien

    Risiko-Nutzen-Bewertung und Handlungsempfehlungen

    Gestaltungsprinzipien und Best Practices

Publikationsformen:

    Diese Masterarbeit als umfassende wissenschaftliche Dokumentation

    Potenzielle Publikation in wissenschaftlichen Journals und Konferenzen

    Präsentation der Ergebnisse im Rahmen des Kolloquiums

    Technische Dokumentation und Whitepapers für Praktiker

    Mögliche Beiträge zu Open-Source-Communities

Die strukturierte Darstellung entlang der DSR-Leitlinien gewährleistet, dass sowohl der wissenschaftliche Beitrag als auch die praktische Relevanz der Forschung klar kommuniziert werden.

---





% Die besondere Eignung von DSR für das vorliegende Forschungsvorhaben begründet sich durch die systematische Ausrichtung an sieben fundierenden Leitlinien (\autoref{tab:DSR_Guidelines}), die \textcite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004} als konzeptionellen Rahmen für effektive Design-Science-Forschung etabliert haben. Diese Leitlinien stellen sicher, dass das Forschungsprojekt gleichzeitig wissenschaftlich rigoros und praktisch relevant durchgeführt wird.

% Design als Artefakt (Guideline 1). Design-Science-Forschung muss ein praktikables Artefakt in Form eines Konstrukts, Modells, einer Methode oder einer Instanziierung hervorbringen \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Die Integration von Post-Quantum-Kryptografie in blockchain-basierte SSI-Systeme erfordert die Schaffung eines neuartigen, funktionsfähigen IT-Artefakts. Dieser Arbeitsschritt ist elementar für das vorliegende Forschungsvorhaben. Der entwickelte Prototyp verkörpert sowohl die Instanziierung (funktionierendes Laborumfeld mit ACA-Py-Agenten, Hyperledger-Komponenten und PQC-Integration) als auch designierte Konstrukte und Methoden (kryptoagile Architekturmuster, PQC-kompatible DID-Schemata).

% Problemrelevanz (Guideline 2). Das Ziel von Design-Science-Forschung besteht darin, technologiebasierte Lösungen für bedeutende und relevante Geschäftsprobleme zu entwickeln \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Das vorliegende Forschungsvorhaben adressiert das \fixme{in Kapitel XY vorgestellte} hochrelevante, multi-dimensionale Problem: die drohende Verwundbarkeit bestehender SSI-Systeme durch die Entwicklung leistungsfähiger Quantencomputer, die exponentielle Sicherheitsrisiken für kritische Infrastrukturen darstellen. Zudem existiert ein regulatorisches Compliance-Problem, da IT-Sicherheitsgesetz 2.0, NIS2-Richtlinie und BSI-Vorgaben nachweislich sichere, zukunftsfähige Lösungen fordern. Dies ist nicht nur ein technisches, sondern auch ein kritisches Geschäftsproblem für Betreiber kritischer Infrastrukturen. \fixme{QUELLE}

% Design-Evaluation (Guideline 3). Der Nutzen, die Qualität und die Wirksamkeit eines Design-Artefakts müssen durch gut durchgeführte Evaluationsmethoden rigoros nachgewiesen werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Dieses Projekt schreibt eine systematische, mehrstufige Evaluationsstrategie vor: Funktionalitätstests der PQC-Komponenten, Performance-Analysen (Latenz, Durchsatz, Speicheraufwand), Sicherheitsbewertungen der kryptografischen Verfahren, Skalierungsstudien unter Lasten sowie Compliance-Validierung anhand realistischer KRITIS-Use-Cases. Diese rigorose Evaluation stellt sicher, dass die Designentscheidungen nicht nur konzeptionell, sondern auch praktisch-funktional gerechtfertigt sind. \fixme{abspecken}

% Forschungsbeiträge (Guideline 4). Effektive Design-Science-Forschung muss klare und verifizierbare Beiträge in den Bereichen Design-Artefakt, Design-Grundlagen und/oder Design-Methodologien liefern \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Das Projekt generiert Forschungsbeiträge auf drei Ebenen: (a) das innovative Artefakt selbst (erster Prototyp quantenresistenter SSI für KRITIS), (b) Gestaltungsprinzipien und Erkenntnisse zur PQC-Integration in SSI-Systeme (Design-Grundlagen) sowie (c) ein reproduzierbares Vorgehensmodell zur PQC-Migration in bestehenden Frameworks (Design-Methodologie). Damit trägt die Arbeit zur Wissensbasis bei, die Forschenden und Praktikern hilft, zukünftige quantenresistente Identitätssysteme zu entwickeln.

% Forschungsstrenge (Guideline 5). Design-Science-Forschung beruht auf der Anwendung rigoroser Methoden sowohl bei der Konstruktion als auch bei der Evaluation des Design-Artefakts \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Forschungsstrenge wird in diesem Projekt gewährleistet durch: (a) systematische Anforderungsanalyse basierend auf PRISMA-geleiteter Literaturrecherche, (b) Verwendung standardisierter, durch NIST validierter PQC-Algorithmen, (c) Implementierung nach etablierten Software-Engineering-Standards (Modularität, Testbarkeit, Versionskontrolle), (d) Anwendung formaler Sicherheitsanalysemethoden sowie (e) transparente Dokumentation aller Designentscheidungen. Diese methodische Strenge unterscheidet design-orientiertes Forschungsprojekt von bloßer Systemimplementierung.

% Design als Suchprozess (Guideline 6). Die Suche nach einem effektiven Artefakt erfordert die Nutzung verfügbarer Mittel, um gewünschte Ziele zu erreichen, während gleichzeitig die Gesetzmäßigkeiten der Problemumgebung berücksichtigt werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Das Projekt nutzt verfügbare Mittel systematisch: existierende SSI-Frameworks (Hyperledger Indy/Aries), standardisierte PQC-Algorithmen (ML-KEM, ML-DSA, Falcon), Open-Source-Bibliotheken (liboqs, OpenSSL 3.x) sowie Containerisierungstechnologien. Die Suchstrategie berücksichtigt aber auch die Gesetzmäßigkeiten der Problemumgebung: physikalische PQC-Performance-Grenzen, Blockchain-Charakteristiken (begrenzte Transaktionsgröße, Immutabilität), regulatorische Constraints und Interoperabilitätsstandards. Der iterative Design Cycle ermöglicht die systematische Exploration des Lösungsraums und schrittweise Verbesserung des Designs innerhalb dieser Rahmenbedingungen.

% Kommunikation der Forschung (Guideline 7). Design-Science-Forschung muss sowohl für technologieorientierte als auch für managementorientierte Zielgruppen effektiv präsentiert werden \parencite[S. 83]{hevner_DesignScienceInformationsystemsresearch_2004}. Die Arbeit adressiert mehrere Zielgruppen gezielt: Forschende und Entwickler erhalten detaillierte technische Spezifikationen, Implementierungsrichtlinien und reproduzierbare Evaluationsergebnisse; KRITIS-Betreiber und Entscheidungsträger erhalten Management Summaries, Compliance-Analysen, Geschäftsfallbewertungen und praktische Handlungsempfehlungen. Diese mehrschichtige Kommunikationsstrategie stellt sicher, dass die Forschungsergebnisse sowohl in die akademische Wissensbasis als auch in die Praxis diffundieren.

\subsection{DSRM Prozessmodell} \label{DSRM Prozessmodell}

Für die Umsetzung des \ac{DSR} wird das sechsstufige Prozessmodell nach \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007} (\autoref{fig:DSRM Process Model}) adaptiert und sichert eine systematische Strukturierung des Forschungsvorhabens.

Durch iterative Rückkopplung insbesondere zu Zieldefinition und Design/Entwicklung kann das Artefakt laufend anhand von Evaluationen und neuen Anforderungen optimiert werden. Die Praxisnähe ist durch die Orientierung an realen Use Cases und \ac{KRITIS}-Anforderungen gewährleistet.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{DSRM Process Model.png}
    \caption{DSRM Process Model}
    \begin{flushleft}
    \textit{Anmerkung.} Aus \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007}.
    \end{flushleft}
    \label{fig:DSRM Process Model}
\end{figure}

\pagebreak

Die konkrete Umsetzung der sechs \ac{DSR}-Phasen zeigt \autoref{tab:dsr_phasen}.

\begin{longtable}{L{3.5cm}L{11cm}}
    \caption{Konkrete Anwendung des DSRM Process Modells}
    \label{tab:dsr_phasen} \\
    \toprule
    \textbf{Phase} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Phase} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung auf Basis von \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007} mit Inhalten des vorliegenden Exposes und \textcite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.} \\
    \endlastfoot
    Phase 1: \newline Problemidentifikation und Motivation &
    Identifikation der drohenden Quantenbedrohung für bestehende \ac{SSI}-Systeme für \ac{KRITIS} als zentrale Herausforderung. \\
    \midrule
    Phase 2: \newline Zieldefinition &
    Konkretisierung der vier Forschungsfragen zu Systemarchitektur, Algorithmenauswahl, Performance und kryptografischer Agilität als Rahmengebung des Lösungsansatzes. \\
    \midrule
    Phase 3: \newline Design und Entwicklung &
    Festlegung des Technologie-Stacks, Architekturentscheidungen, die Auswahl und Integration der \ac{PQC}-Algorithmen sowie die Definition geeigneter Schnittstellen und Datenflüsse. Darüber hinaus werden die spezifischen Compliance-Anforderungen für \ac{KRITIS} in das Systemdesign integriert. Die Implementierung innerhalb der Laborumgebung erfolgt modular, um kryptografische Agilität zu gewährleisten und zukünftige Algorithmus-Updates ohne Systemunterbrechung zu ermöglichen. Sie umfasst die (Weiter-)Entwicklung zentraler Systemkomponenten und Mechanismen für zentrale Aktivitäten des Identity Management Lifecycles. \\
    \midrule
    Phase 4: \newline Demonstration &
    Verifizierung der grundsätzlichen Funktionsfähigkeit des entwickelten Prototyps innerhalb der Laborumgebung anhand voridentifizierter Use Cases, die auf dem Identity Management Lifecycle aufbauen und ausgewählte Szenarien des \ac{KRITIS}-Bereichs darstellen. \\
    \midrule
    Phase 5: \newline Evaluation &
    Multidimensionale Anwendung des von \textcite{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016} entwickelten \ac{FEDS}-Frameworks, welches eine systematische Bewertungsstrategie für \ac{DSR}-Artefakte bereitstellt. Die Evaluation gliedert sich nach \textcite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016} in formative und summative Komponenten, wobei sowohl künstliche als auch naturalistische Evaluationsparadigmen zum Einsatz kommen. \\
    \midrule
    Phase 6: \newline Kommunikation &
    Weitergabe der Forschungsergebnisse. Im Rahmen dieser Masterarbeit erfolgt dies in einem ausgewählten Rahmen, wobei sowohl wissenschaftliche als auch praxisorientierte Zielgruppen berücksichtigt werden. Die Ergebnisse zur Integration von \ac{PQC} in \ac{SSI}-Systeme werden in Form von Gestaltungsprinzipien zusammengefasst, um Anhaltspunkte für zukünftige Entwicklungen in diesem Bereich zu bieten. \\
\end{longtable}

==>

DSRM-Phase	Kapitel in der Arbeit
Phase 1: Problem Identification	1.1
Phase 2: Objectives	1.4, 4.1.1
Phase 3: Design \& Development	4.1-4.4 (Iterationen 0-3)
Phase 4: Demonstration	4.3.3, 4.4.3 (innerhalb Iterationen) + 5.2
Phase 5: Evaluation	4.2.3, 4.3.3, 4.4.3 (formativ) + 5 (summativ)
Phase 6: Communication	8, 9

\subsection{FEDS-Framework} \label{FEDS-Framework}

Die Evaluation des entwickelten SSI-Prototyps folgt dem Framework for Evaluation in Design Science (FEDS) nach \textcite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}, welches eine systematische Evaluationsstrategie für Design-Science-Artefakte bereitstellt. Evaluation ist eine zentrale und kritische Komponente von Design-Science-Forschung, da sie Feedback für die weitere Entwicklung liefert und bei korrekter Durchführung die Rigorosität der Forschung sicherstellt \parencite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

Evaluation in DSR ist essenziell, da Forschende rigorös die Nützlichkeit, Qualität und Wirksamkeit eines Design-Artefakts mittels gut durchgeführter Evaluationsmethoden nachweisen müssen \parencite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

Das FEDS-Framework adressiert die Forschungsfrage, wie eine geeignete Strategie für die Durchführung verschiedener Evaluationsaktivitäten in einem DSR-Projekt zu gestalten ist \parencite[S. 2]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}. Es charakterisiert DSR-Evaluationsepisoden (spezifische Evaluationen) anhand zweier Dimensionen: der funktionalen Zielsetzung der Evaluation (formativ oder summativ) und dem Paradigma der Evaluation (künstlich oder naturalistisch) \parencite[S. 1]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\subsubsection{Dimension 1: Funktionale Zielsetzung}

Die funktionale Zielsetzung unterscheidet zwischen formativer und summativer Evaluation. Diese Unterscheidung liegt nicht in der inhaltlichen Natur der Evaluation, sondern in deren funktionalem Zweck \parencite[S. 2]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\textbf{Formative Evaluationen} werden verwendet, um empirisch fundierte Interpretationen zu erzeugen, die als Grundlage für erfolgreiches Handeln zur Verbesserung der Eigenschaften oder Performance des Evaluationsobjekts dienen. Formative Evaluationen fokussieren auf Konsequenzen und unterstützen Entscheidungen zur Verbesserung des Evaluationsobjekts \parencite[S. 2]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\textbf{Summative Evaluationen} werden verwendet, um empirisch fundierte Interpretationen zu erzeugen, die als Grundlage für die Schaffung geteilter Bedeutungen über das Evaluationsobjekt in verschiedenen Kontexten dienen. Summative Evaluationen fokussieren auf Bedeutungen und unterstützen Entscheidungen zur Auswahl des Evaluationsobjekts für eine Anwendung \parencite[S. 2-3]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

Im Kontext dieser Arbeit werden beide Evaluationsformen eingesetzt. Formative Evaluationen erfolgen während der iterativen Artefaktentwicklung (Kapitel~\ref{sec:Iterative Artefaktentwicklung}) in Form von Itegrationstests um Designschwächen frühzeitig zu identifizieren und zu beheben. Summative Evaluationen werden nach Abschluss der iterativen Artefaktentwicklung durchgeführt, um die Erfüllung der funktionalen und nicht-funktionalen Anforderungen sowie die konforme Umsetzung KRITIS-spezifischer Compliance-Vorgaben am Gesamtsystem zu bewerten (Kapitel~\ref{sec:Summative Evaluation}).

\subsubsection{Dimension 2: Evaluationsparadigma}

Das FEDS-Framework unterscheidet zwischen künstlicher (artificial) und naturalistischer (naturalistic) Evaluation \parencite[S. 3-4]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\textbf{Künstliche Evaluation} kann empirisch oder nicht-empirisch sein und ist nahezu immer positivistisch und reduktionistisch. Sie wird zur Prüfung von Design-Hypothesen verwendet und umfasst Laborexperimente, Simulationen, kriterienbasierte Analysen, theoretische Argumente und mathematische Beweise. Das dominante wissenschaftlich-rationale Paradigma bringt künstlicher Evaluation den Vorteil stärkerer wissenschaftlicher Reliabilität in Form besserer Wiederholbarkeit und Falsifizierbarkeit \parencite[S. 4]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\textbf{Naturalistische Evaluation} erforscht die Performance einer Lösungstechnologie in ihrer realen Umgebung, typischerweise innerhalb einer Organisation. Durch Evaluation in einer realen Umgebung (reale Menschen, reale Systeme, reale Settings) umfasst naturalistische Evaluation alle Komplexitäten menschlicher Praxis in realen Organisationen. Sie ist immer empirisch und tendiert zum Interpretivismus, kann aber auch positivistisch oder kritisch sein. Das dominante interpretivistische Paradigma bringt naturalistischer DSR-Evaluation den Vorteil stärkerer interner Validität \parencite[S. 4]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

Die beiden Dimensionen sind vollständig orthogonal zueinander: Sowohl naturalistische als auch künstliche Evaluationsmethoden können für formative und/oder summative Evaluationen verwendet werden \parencite[S. 4]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\fixme{Für diese Arbeit werden beide Paradigmen kombiniert: Künstliche Evaluationen erfolgen in einer kontrollierten Laborumgebung (Docker-Container, definierte Test-Use-Cases) zur Messung von Performance-Metriken und kryptografischer Stärke. Naturalistische Elemente werden durch KRITIS-spezifische Szenarien integriert, die reale Anforderungen und Workflows abbilden.

Für diese Arbeit wird ausschließlich ein künstliches Evaluationsparadigma angewendet. Die Evaluation erfolgt in einer kontrollierten Laborumgebung (Docker-Containerisierung, definierte Test-Szenarien) zur systematischen Messung von Performance-Metriken und Validierung kryptografischer Sicherheitseigenschaften. KRITIS-spezifische Compliance-Anforderungen werden ausschließlich in der Anforderungsanalyse (Kapitel 4.1.2.3) berücksichtigt und als Grundlage für die Definition der Testkriterien und -szenarien verwendet, nicht jedoch im Rahmen einer naturalistischen Feldstudie evaluiert.}

\subsubsection{FEDS-Evaluationsstrategien}

Das FEDS-Framework identifiziert vier prototypische Evaluationsstrategien, die jeweils unterschiedliche Trajektorien vom Ursprung (keine Evaluation) zur umfassenden und rigorosen Evaluation (formativ-summativ, künstlich-naturalistisch) verfolgen \parencite[S. 4-5]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}:

\begin{enumerate}
    \item \textbf{Quick \& Simple Strategy:} Wenig formative Evaluation, schneller Übergang zu summativen und naturalistischeren Evaluationen. Geeignet bei kleinen, einfachen Designs mit niedrigem sozialen und technischen Risiko.
    
    \item \textbf{Human Risk \& Effectiveness Strategy:} Betont formative Evaluationen früh im Prozess, progrediert schnell zu naturalistischen formativen Evaluationen. Gegen Ende werden summative Evaluationen durchgeführt, die auf rigorose Evaluation der Effektivität des Artefakts fokussieren.
    
    \item \textbf{Technical Risk \& Efficacy Strategy:} Betont künstliche formative Evaluationen iterativ früh im Prozess, progrediert zu summativen künstlichen Evaluationen zur rigorosen Bestimmung der Wirksamkeit. Gegen Ende werden naturalistische Evaluationen eingebunden.
    
    \item \textbf{Purely Technical Strategy:} Wird verwendet, wenn ein Artefakt rein technisch ist ohne menschliche Nutzer, oder wenn geplante Deployment mit Nutzern so weit in der Zukunft liegt, dass naturalistische Evaluation irrelevant ist.
\end{enumerate}

\fixme{Für diese Arbeit wird eine Kombination aus \textbf{Technical Risk \& Efficacy} und \textbf{Human Risk \& Effectiveness} Strategy gewählt. Die Entwicklung beginnt mit künstlichen formativen Evaluationen (Unit-Tests, Performance-Messungen in Laborumgebung), progrediert zu naturalistischeren Evaluationen (KRITIS-Use-Cases) und schließt mit summativen Evaluationen ab (Gesamtbewertung gegen definierte Erfolgskriterien).

Für diese Arbeit wird die \textbf{Purely Technical Strategy} gewählt. Das entwickelte Artefakt ist ein technischer Prototyp, dessen Evaluation ausschließlich in einer kontrollierten Laborumgebung (Docker-Containerisierung, definierte Test-Szenarien) durch künstliche formative und summative Evaluationen erfolgt. Naturalistische Evaluationen werden nicht durchgeführt, da das Deployment in realen KRITIS-Umgebungen über den Scope einer Masterarbeit hinausgeht und kein Zugang zu entsprechenden Produktivsystemen besteht. Die KRITIS-spezifischen Compliance-Anforderungen dienen ausschließlich als Grundlage für die Definition der technischen Testkriterien und -szenarien.}

\subsubsection{FEDS-Evaluationsdesign-Prozess}

Das FEDS-Framework bietet einen vierstufigen Prozess zur Gestaltung der Evaluationsstrategie \parencite[S. 6]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}:

\textbf{Schritt 1: Explizierung der Evaluationsziele}  
Es existieren mindestens vier möglicherweise konkurrierende Ziele bei der Gestaltung der Evaluationskomponente von DSR: Rigorosität (Wirksamkeit und Effektivität), Unsicherheits- und Risikoreduktion, Ethik und Effizienz \parencite[S. 6-7]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

Für diese Arbeit sind die Evaluationsziele:
\begin{itemize}
    \item \textbf{Rigorosität:} Nachweis, dass der Prototyp funktionsfähig ist (Wirksamkeit) und in realistischen KRITIS-Szenarien einsetzbar ist (Effektivität)
    \item \textbf{Risikoreduktion:} Frühzeitige Identifikation technischer Risiken (PQC-Integrationsprobleme, Performance-Engpässe) durch formative Evaluationen
    \item \textbf{Effizienz:} Begrenzung des Evaluationsaufwands auf für eine Masterarbeit realisierbare Laborumgebungen
\end{itemize}

\textbf{Schritt 2: Auswahl der Evaluationsstrategie(n)}  
Basierend auf den Zielen wird eine oder mehrere Strategien gewählt. Tabelle \ref{tab:feds_strategy} fasst die Auswahlkriterien zusammen \parencite[S. 6]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}.

\begin{table}[h]
\centering
\caption{Auswahlkriterien für FEDS-Evaluationsstrategien}
\label{tab:feds_strategy}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Strategie} & \textbf{Auswahlkriterien} \\
\hline
Quick \& Simple & Kleines und einfaches Design mit niedrigem sozialen und technischen Risiko \\
\hline
Human Risk \& Effectiveness & Hauptrisiko sozial oder nutzerorientiert; günstiger Zugang zu echten Nutzern; kritisches Ziel ist rigorose Etablierung langfristiger Nützlichkeit in realen Situationen \\
\hline
Technical Risk \& Efficacy & Hauptrisiko technisch orientiert; Evaluation mit echten Nutzern prohibitiv teuer; kritisches Ziel ist rigorose Etablierung, dass Nutzen dem Artefakt zuzuschreiben ist \\
\hline
Purely Technical & Artefakt rein technisch ohne soziale Aspekte oder Deployment liegt weit in der Zukunft \\
\hline
\end{tabular}
\end{table}

Für diese Arbeit wird \textbf{Technical Risk \& Efficacy} gewählt, da:
\begin{itemize}
    \item Das Hauptrisiko technischer Natur ist (PQC-Integration, Performance)
    \item Die Evaluation in realen KRITIS-Umgebungen nicht realisierbar ist (Zugang, Sicherheitsrisiken, regulatorische Hürden)
    \item Nachweis erforderlich ist, dass PQC-Algorithmen die erwartete Sicherheit bieten
\end{itemize}

\textbf{Schritt 3: Bestimmung der zu evaluierenden Eigenschaften}  
Die Auswahl der Artefakteigenschaften für die Evaluation ist notwendigerweise einzigartig für das Artefakt, seinen Zweck und seine Situation \parencite[S. 7]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}. Für diese Arbeit werden folgende Eigenschaftskategorien evaluiert:

\begin{itemize}
    \item \textbf{Funktionalität:} Vollständigkeit der Implementierung (UC1--UC7), Korrektheit der kryptografischen Operationen
    \item \textbf{Performance:} Latenz (Credential-Verification < 1s), Durchsatz, Speicheraufwand (Schlüssel-, Signaturgröße)
    \item \textbf{Sicherheit:} Kryptografische Stärke (NIST Security Level), Resilience gegen Angriffe
    \item \textbf{Compliance:} Erfüllung von BSI-Anforderungen (IDM, KRY), DSGVO, NIS2
    \item \textbf{Kryptoagilität:} Fähigkeit zum Algorithmus-Update ohne Systemunterbrechung
\end{itemize}

\textbf{Schritt 4: Gestaltung der individuellen Evaluationsepisoden}  
Unter Berücksichtigung von Umgebungsfaktoren (verfügbare Zeit, Budget, Ressourcen) werden die konkreten Evaluationsepisoden geplant \parencite[S. 8]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016}. Für diese Arbeit umfasst dies:

\begin{enumerate}
    \item \textbf{Formative Evaluationen (Iteration 1--3):} Integrationstests
    \item \textbf{Summative Evaluation (nach Iteration 3):}
    \begin{itemize}
        \item Funktionalitätstests anhand KRITIS-Use-Cases (Kapitel 7.2)
        \item Sicherheitsbewertung (kryptografische Analyse, Threat-Modeling) (Kapitel 7.4)
        \item Compliance-Validierung (Abgleich mit BSI-Anforderungen) (Kapitel 7.4)
    \end{itemize}
\end{enumerate}

Die Evaluation erfolgt in einer kontrollierten Laborumgebung (Ubuntu 24.04 LTS, Docker, Hyper-V), um Wiederholbarkeit und wissenschaftliche Rigorosität zu gewährleisten. KRITIS-Realitätsnähe wird durch realistische Use Cases und Szenarien sichergestellt (Kapitel 6).

Das FEDS-Framework sichert somit eine systematische, rigorose und effiziente Evaluation des entwickelten PQC-basierten SSI-Prototyps, die sowohl wissenschaftlichen Standards als auch praktischen Anforderungen gerecht wird.

\newpage
\section{Iterative Artefaktentwicklung} \label{sec:Iterative Artefaktentwicklung}
Überblick über den iterativen Entwicklungsprozess: \\
       - Zuordnung zu DSRM-Phasen und Hevner-Zyklen \\
       - Iterationsplanung und Forschungsfragen-Triangulation


\subsection{Iteration 1: Basisarchitektur mit Transport-Layer PQC-Integration}
\label{sec:Iteration 1: Basisarchitektur mit Transport-Layer PQC-Integration}

\subsubsection{Designziele dieser Iteration}
%           ALLE FF, mit SCHWERPUNKT auf FF1 (Architektur), FF2 (Algorithmen) \\
%           → FF1 (Architektur): Modulare Layer-Architektur implementieren \\
%           → FF2 (Algorithmen): Erste PQC-Algorithmen auswählen (ML-DSA-65) \\
%           → FF3 (Performance): Baseline-Performance ohne PQC messen \\
%           → FF4 (Kryptoagilität): Crypto Abstraction Layer konzipieren

%Initiales Design und Implementierung der Kernkomponenten

% - DZ1.1: Evaluation von SSI-Frameworks und Technologiestack \\
%   → adressiert FF1 (Systemarchitektur - initial) \\
% - DZ1.2: Design der vier-schichtigen Architektur mit PQC-Integration \\
%   → adressiert FF1 (Systemarchitektur) \\
% - DZ1.3: Implementierung und Baseline-Test der Kernkomponenten (DID, VC, Signaturen) \\
%   → adressiert FF2 (Algorithmenauswahl - initial) \\
% ------------------------------------------------ \\
% - DZ1.4: Erste Performance-Baseline ohne PQC \\
%   → adressiert FF3 (Performance - Baseline)

Die erste Iteration der Artefaktentwicklung korrespondiert mit der DSRM-Phase 2 \textit{Objectives} nach \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007} und fokussiert auf die Etablierung einer funktionsfähigen Basisarchitektur für das blockchain-basierte SSI-System mit initialer PQC-Integration auf Transport-Layer-Ebene. Im Kontext des Drei-Zyklen-Modells nach \textcite[S. 88]{hevner_ThreeCycleViewDesignScienceResearch_2007} adressiert diese Iteration primär den \textit{Design Cycle}, wobei die Anforderungen aus dem \textit{Relevance Cycle} systematisch in konkrete Designentscheidungen überführt werden.


Die erste Iteration der Artefaktentwicklung korrespondiert mit der DSRM-Phase 2 \textit{Objectives} nach \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007} und adressiert die systematische Ableitung von Designzielen aus den in Kapitel~\ref{sec:Zielsetzung und Forschungsfragen} definierten Forschungsfragen. Im Kontext des Drei-Zyklen-Modells nach \textcite[S. 88]{hevner_ThreeCycleViewDesignScienceResearch_2007} erfolgt eine primäre Aktivierung des \textit{Design Cycle}, in welchem die aus dem \textit{Relevance Cycle} stammenden Anforderungen zu operativen Designprinzipien und Evaluationskriterien systematisiert werden. Diese Systematisierung bildet das Fundament für die in den Kapitel~\ref{sec:Anforderungsanalyse} bis \ref{sec:Implementierung} durchgeführte Anforderungsanalyse, Technologieauswahl, Architekturentwurf und Implementierung.

Die Designziele dieser Iteration leiten sich aus den in Kapitel~\ref{sec:Zielsetzung und Forschungsfragen} definierten Forschungsfragen ab, wobei der Schwerpunkt auf FF1 (Systemarchitektur \& Compliance) liegt. Bezüglich FF1 wird das Ziel verfolgt, eine modulare Schichtenarchitektur zu etablieren, die eine klare Separation zwischen SSI-Agenten, DLT-Infrastruktur und kryptografischer Transportschicht realisiert. Das Design soll dabei das Potenzial demonstrieren, Post-Quantum-Kryptografie systemisch zu integrieren, ohne dabei klassische blockchainbasierte SSI-Architekturen grundsätzlich umgestalten zu müssen.

Bezüglich FF2 (Algorithmenauswahl und Sicherheitsbewertung) liegt das Designziel auf der systematischen Identifikation und Erprobung quantenresistenter kryptografischer Primitive für die Sicherung von Zertifikatssignaturen auf der Transport-Layer-Ebene. Die empirische Evaluierung soll dabei Erkenntnisse zur praktischen Eignung und Interoperabilität dieser Algorithmen in einer integrierten Systemumgebung generieren, um eine evidenzbasierte Grundlage für die Algorithmenauswahl in SSI-Architekturen zu schaffen.

Für FF3 (Kryptografische Agilität) zielt die Iteration auf die architektonische Vorbereitung für Austauschbarkeit kryptografischer Algorithmen ab. Das Design soll dabei Mechanismen vorsehen, die zukunftige Algorithmenupdates ermöglichen, ohne das System grundlegend umgestalten zu müssen.

\subsubsection{Anforderungsanalyse} \label{sec:Anforderungsanalyse}

\paragraph{Funktionale Anforderungen} \label{sec:Funktionale Anforderungen}

Basierend auf der Analyse von \textcite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021} lassen sich für das SSI-System sechs zentrale funktionale Anforderungen identifizieren, die den den vollständigen Lebenszyklus digitaler Identitätsnachweise ab decken (\autoref{tab:functional_requirements}).

\begin{longtable}{L{1cm}L{4cm}L{9cm}}
    \caption{Funktionale Anforderungen an SSI-Systeme}
    \label{tab:functional_requirements} \\
    \toprule
    \textbf{Nr.} & \textbf{Funktionale Anforderung} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Funktionale Anforderung} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung auf Basis der Auflistungen und des Sequenzdiagramms in Anlehnung an \textcite[S. 130-132]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}.} \\
    \endlastfoot
    1 & Issuer Discovery &
    Das System muss die Auffindbarkeit von publizierten Credential Schemata des Issuers digitaler Identitätsnachweise ermöglichen. \\
    \midrule
    2 & Connection Creation &
    Das System muss Verbindungen zwischen den Akteuren des SSI-Ökosystems etablieren können. \\
    \midrule
    3 & Credential Creation &
    Das System muss Funktionalität zur Erstellung und Ausstellung digitaler Credentials bereitstellen. \\
    \midrule
    4 & Verification with Credentials &
    Das System muss einen Verifikationsprozess zwischen Identity Holder, Verifier und Blockchain-basierter \ac{VDR} durch Validierung eines Identitätsnachweises ermöglichen. \\
    \midrule
    5 & Credential Revocation &
    Das System muss die Funktionalität zum Widerruf von Credentials unterstützen. \\
    \midrule
    6 & Credential Deletion &
    Das System muss die Funktionalität zur Löschung von Credentials unterstützen. \\
\end{longtable}

% \paragraph{Nicht-funktionale Anforderungen} \label{sec:Nicht-Funktionale Anforderungen}

% Auf Grundlage des begrenzten Rahmens der Masterarbeit wurden folgende  

% Die nicht-funktionalen Anforderungen (Non-Functional Requirements, NFR) definieren die Qualitätsattribute und systemübergreifenden Eigenschaften des zu entwickelnden SSI-Prototyps. \textcite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021} identifizieren in ihrer umfassenden Vergleichsstudie 22 NFR für blockchain-basierte Self-Sovereign-Identity-Systeme, die in zwei Kategorien unterteilt werden können: acht prominente Kern-NFR (NFR 1--8), die in der akademischen Literatur sowie in kommerziellen und Open-Source-Lösungen weitverbreitet sind, sowie 14 erweiterte NFR (NFR 9--22), die spezifischere Qualitätsaspekte adressieren.

% Die folgenden acht Kern-NFR bilden das qualitative Fundament für blockchain-basierte SSI-Systeme und sind in der Fachliteratur sowie in Standardisierungsinitiativen (W3C, DIF, RWoT) prominent vertreten \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}:

% \textbf{NFR1: Provability (Nachweisbarkeit)}

% Das System muss Identity Holder die Fähigkeit bieten, ihre Identität kryptografisch nachzuweisen \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Im KRITIS-Kontext ist dies besonders relevant für Zugriffsentscheidungen auf kritische Infrastruktursysteme, bei denen Non-Repudiation (Nicht-Abstreitbarkeit) und forensische Nachvollziehbarkeit essenziell sind.

% \textbf{NFR2: Interoperability (Interoperabilität)}

% Das System muss plattform-, blockchain- und jurisdiktionsübergreifende Kompatibilität gewährleisten und über Programmiersprachen, Anbieter, Netzwerke, rechtliche Rahmen, Kryptografie-Algorithmen sowie Hardware hinweg funktionieren \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Dies schließt zeitliche Interoperabilität ein (langfristige Lesbarkeit von Credentials trotz technologischer Evolution). Für KRITIS ist die Interoperabilität mit bestehenden IAM-Systemen (z.B. Active Directory, LDAP) und Compliance-Frameworks (BSI-Grundschutz, ISO 27001) unabdingbar.

% \textbf{NFR3: Portability (Portabilität)}

% Identity Holder müssen ihre digitalen Identitätsnachweise und Credentials geräte-, plattform- und anbieterübergreifend mitnehmen können \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Dies impliziert standardisierte Export-/Import-Mechanismen (z.B. W3C Verifiable Credentials) sowie Vendor-Lock-In-Vermeidung. Im KRITIS-Kontext unterstützt dies die Anforderung der Unabhängigkeit von einzelnen Dienstleistern gemäß NIS-2-Richtlinie.

% \textbf{NFR4: Pseudonymity (Pseudonymität)}

% Das System muss Identity Holder die Möglichkeit bieten, zu interagieren, ohne ihre reale Identität offenlegen zu müssen \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Dies wird typischerweise durch paarweise pseudonyme DIDs (Decentralized Identifiers) erreicht, bei denen für jede Beziehung ein eindeutiger Identifier generiert wird. Pseudonymität ist komplementär zu selektiver Offenlegung und Zero-Knowledge-Proofs.

% \textbf{NFR5: Recovery (Wiederherstellbarkeit)}

% Das System muss sichere und benutzerfreundliche Mechanismen zur Wiederherstellung von kryptografischen Schlüsseln und Credentials bereitstellen \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. \textcite[S. 133]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021} identifizieren Recovery als eine der am wenigsten unterstützten NFR in existierenden SSI-Lösungen, obwohl die Absicherung und Wiederherstellung in Wallet-basierten Systemen vital ist. Im KRITIS-Kontext ist eine auditierbare, compliant Recovery-Strategie (z.B. mittels Social Recovery, Hardware-Security-Modules oder Threshold-Kryptografie) unerlässlich.

% \textbf{NFR6: Scalability (Skalierbarkeit)}

% Das System muss für breite Adoption und Replikation geeignet sein \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Dies umfasst sowohl technische Skalierbarkeit (Transaktionsdurchsatz, Latenz bei Credential-Verifikation) als auch organisatorische Skalierbarkeit (Onboarding von Issuern und Verifiern). Für KRITIS-Anwendungen müssen hohe Transaktionsvolumina (z.B. Authentifizierung von Millionen IoT-Geräten) bewältigt werden.

% \textbf{NFR7: Security (Sicherheit)}

% Das System muss umfassenden Schutz von Daten, kryptografischen Schlüsseln und Credentials gewährleisten \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Im Kontext der Post-Quantum-Kryptografie bedeutet dies konkret die Resistenz gegen Angriffe durch Quantencomputer. Die Implementierung muss NIST-FIPS-203/204/205-konforme PQC-Algorithmen (ML-DSA, ML-KEM, SLH-DSA) integrieren und dabei gängige Bedrohungsmodelle (Impersonation, Replay-Attacken, Man-in-the-Middle) adressieren.

% \textbf{NFR8: Usability (Benutzerfreundlichkeit)}

% Das System muss eine menschenzentrierte, verständliche User Experience bieten \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Dies schließt intuitive Wallet-Interfaces, verständliche Consent-Dialoge und niedrige Eintrittsbarrieren ein. Für KRITIS-Umgebungen ist die Balance zwischen Sicherheit (z.B. Multi-Faktor-Authentifizierung) und Usability (z.B. Single-Sign-On-Erlebnis) kritisch, um sowohl IT-Security-Experten als auch End-User zu unterstützen.



% ---------------------------------------------------------------

% \paragraph*{Erweiterte NFR für KRITIS-spezifische Compliance}

% Zusätzlich zu den Kern-NFR definieren \textcite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021} 14 erweiterte Anforderungen, von denen folgende im KRITIS-Kontext besonders relevant sind:

% \textbf{NFR9--NFR16: Privacy- und Datenhoheits-Anforderungen}

% Das System muss \textit{Protection} (sichere Datenspeicherung), \textit{Persistence} (Verfügbarkeit bis zur nutzerseitigen Löschung), \textit{Minimization} (Datenminimierung gemäß DSGVO Art. 5 Abs. 1 lit. c), \textit{Existence} (Dateneinsichtnahme), \textit{Control} (Zugriffskontrolle durch Identity Holder), \textit{Consent} (explizite Einwilligung für jeden Zugriff), \textit{Transparency} (Nachvollziehbarkeit der Datenverarbeitung) und \textit{Access} (jederzeitiger Datenzugriff) gewährleisten \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Diese Anforderungen adressieren direkt die DSGVO-Compliance sowie die Prinzipien der Selbstbestimmung und Datenhoheit.

% \textbf{NFR17--NFR19: Systemvertrauen und Inklusion}

% Das System muss \textit{Convenience} (einfacher Datenzugriff), \textit{Inclusion} (Unterstützung verschiedener Nutzergruppen unabhängig von Nationalität, technischer Affinität etc.) und \textit{Trust} (Vertrauenswürdigkeit der Plattform) bieten \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Für KRITIS ist insbesondere das Vertrauen in die Governance-Struktur der SSI-Infrastruktur relevant (z.B. Transparenz des Ledger-Betriebs, Auditierbarkeit von Revocation-Entscheidungen).

% \textbf{NFR20--NFR22: Technologie-Integration und Wirtschaftlichkeit}

% Das System sollte \textit{Biometrics support} (biometrische Authentifizierung), \textit{Support for IoT} (Identity Management für IoT-Geräte in Industrial Control Systems) sowie vertretbare \textit{Cost}-Strukturen für Identity Holder, Issuer und Verifier aufweisen \parencite[S. 130]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Die IoT-Unterstützung ist für KRITIS-Umgebungen mit massiven Sensor- und Aktor-Netzwerken besonders kritisch.

% Für die iterative Artefaktentwicklung werden die NFR nach Kritikalität priorisiert. \textbf{Iteration 1} fokussiert auf NFR1, NFR7 und NFR13 (Provability, Security, Control). \textbf{Iteration 2} adressiert NFR2, NFR5 und NFR11 (Interoperability, Recovery, Minimization). \textbf{Iteration 3} implementiert NFR4, NFR6 und NFR15 (Pseudonymity, Scalability, Transparency). Die verbleibenden NFR werden im Rahmen der summativen Evaluation bewertet und als Grundlage für zukünftige Iterationen dokumentiert.

\paragraph{KRITIS-spezifische Compliance-Anforderungen} \label{sec:KRITIS-spezifische Compliance-Anforderungen}

Die in Tabelle \ref{tab:compliance_requirements} konsolidierten Anforderungen definieren den normativen Rahmen für die Gestaltung und Evaluierung post-quanten-sicherer SSI-Systeme im Kontext kritischer Infrastrukturen.

Im Bereich der \textbf{kryptografischen Verfahren} (Nr. 1-4) basieren die Vorgaben primär auf den Technischen Richtlinien des BSI. Für die Migration auf Post-Quantum-Kryptografie ist insbesondere die Wahl spezifischer Parameter-Sets für ML-DSA (NIST Level 3/5) und ML-KEM (Level 3/5) sowie die zwingende Implementierung hybrider Schlüsseleinigung vorgeschrieben, um sowohl Integrität als auch langfristige Vertraulichkeit gegen Quantencomputer-Angriffe zu gewährleisten \parencite[Kap. 2.2, 2.4, 5.3.4.2]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025}. Ergänzend fordert die TR-02102-2 den Einsatz moderner Transportverschlüsselung via TLS 1.3, um durch Perfect Forward Secrecy (PFS) die Kommunikationskanäle abzusichern \parencite[Kap. 3.2]{bsi_TechnischeRichtlinieTR021022KryptographischeVerfahrenEmpfehlungenundSchlussellangenTeil2VerwendungTransport_2025}.

Hinsichtlich der \textbf{Betriebssicherheit} (Nr. 5-6) leiten sich die Anforderungen direkt aus dem IT-Sicherheitsgesetz 2.0 (BSIG) und internationalen Standards ab. Essenziell für KRITIS-Betreiber ist hierbei die Implementierung effektiver \gls{SzA} durch umfassende Protokollierung sicherheitsrelevanter Ereignisse gemäß § 8a BSIG \parencite[Nr. 101, 103]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024}. Flankierend schreibt die ISO/IEC 27001 eine strikte logische Netzsegmentierung vor, um die Ausbreitung potenzieller Sicherheitsvorfälle innerhalb der Infrastruktur zu begrenzen \parencite[Control A.8.22]{iso/iec_ISOIEC270012022InformationsecuritycybersecurityprivacyprotectionInformationsecuritymanagement_2022}.

Die dritte Säule bildet der \textbf{Datenschutz} (Nr. 7-9) auf Basis der DSGVO. Hierbei stehen Prinzipien wie \textit{Privacy by Design} gemäß Art. 25 und Datenminimierung nach Art. 5 im Fokus. Zudem muss das Recht auf Löschung nach Art. 17 durch geeignete Architekturmuster, etwa die Trennung von Identifikatoren und Inhaltsdaten, technisch gewährleistet werden \parencite[Art. 5, 17, 25]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}.

\begin{longtable}{L{1cm}L{4cm}L{9cm}}
    \caption{Compliance Anforderungen an SSI-Systeme im KRITIS-Kontext}
    \label{tab:compliance_requirements} \\
    \toprule
    \textbf{Nr.} & \textbf{Compliance Anforderung} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Compliance Anforderung} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung auf Basis von \textcite{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025,bsi_TechnischeRichtlinieTR021022KryptographischeVerfahrenEmpfehlungenundSchlussellangenTeil2VerwendungTransport_2025,bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024,iso/iec_ISOIEC270012022InformationsecuritycybersecurityprivacyprotectionInformationsecuritymanagement_2022,daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}.} \\
    \endlastfoot

    % KATEGORIE 1: KRYPTOGRAFISCHE VERFAHREN
    \multicolumn{3}{l}{\textbf{Kryptografische Verfahren und Schlüssellängen}} \\
    \midrule
    1 & Einhaltung spezifischer Parameter-Sets für ML-DSA &
    Zur Gewährleistung der vom BSI geforderten Sicherheitsniveaus dürfen für das Verfahren ML-DSA ausschließlich die Parameter-Sets verwendet werden, die den NIST Security Strength Categories 3 oder 5 entsprechen. Konkret sind dies ML-DSA-65 oder ML-DSA-87 \parencite[Kap. 5.3.4.2]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025} \\
    \midrule
    2 & Einhaltung spezifischer Parameter-Sets für ML-KEM &
    Für den langfristigen Schutz vertraulicher Informationen mittels des gitterbasierten Schlüsselkapselungsverfahrens ML-KEM dürfen gemäß BSI-Einschätzung ausschließlich Parametersätze verwendet werden, die den NIST Security Strength Categories 3 oder 5 entsprechen. Zulässig sind demnach ML-KEM-768 sowie ML-KEM-1024 \parencite[Kap. 2.4.3]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025}. \\
    \midrule
    3 & Implementierung hybrider Schlüsseleinigung &
    Um langfristige Vertraulichkeit (Schutz vor \textit{Store Now, Decrypt Later}) zu gewährleisten, muss für die Schlüsseleinigung zwingend ein hybrides Verfahren implementiert werden, das ein anerkanntes klassisches Verfahren mit einem empfohlenen PQC-KEM kombiniert \parencite[Kap. 2.2, 2.4]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025}. \\
    \midrule
    4 & Bevorzugte Verwendung von TLS 1.3 &
    Für die Absicherung der Transportebene wird gemäß \textcite[Kap. 3.2]{bsi_TechnischeRichtlinieTR021022KryptographischeVerfahrenEmpfehlungenundSchlussellangenTeil2VerwendungTransport_2025} vorrangig das Protokoll TLS 1.3 empfohlen, da es PFS erzwingt und auf unsichere Cipher-Suites verzichtet. \\

    % KATEGORIE 2: BETRIEBSSICHERHEIT UND KRITIS-ORGANISATION
    \midrule
    \multicolumn{3}{l}{\textbf{Betriebssicherheit und KRITIS-Organisation}} \\
    \midrule
    5 & Protokollierung sicherheitsrelevanter Ereignisse &
    Sicherheitsrelevante Ereignisse müssen auf System- und Netzebene zentral protokolliert werden, um eine zeitnahe Erkennung von Angriffen zu ermöglichen \parencite[Nr. 101, 103]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024}. \\
    \midrule
    6 & Logische Netzsegmentierung &
    Gruppen von Informationsdiensten, Benutzern und Informationssystemen sollten in den Netzwerken der Organisation getrennt werden \parencite[Control A.8.22]{iso/iec_ISOIEC270012022InformationsecuritycybersecurityprivacyprotectionInformationsecuritymanagement_2022}. \\

    % KATEGORIE 3: DATENSCHUTZ UND PRIVACY
    \midrule
    \multicolumn{3}{l}{\textbf{Datenschutz und Privacy by Design}} \\
    \midrule
    7 & Datenschutz durch Technikgestaltung (Privacy by Design) &
    Gemäß \textcite[Art. 25]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016} sind bereits bei der Entwicklung des Systems geeignete technische Maßnahmen zu treffen, die die Datenschutzgrundsätze addressieren. \\
    \midrule
    8 & Grundsatz der Datenminimierung &
    Personenbezogene Daten müssen dem Zweck angemessen und auf das notwendige Maß beschränkt sein. \parencite[Art. 5]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}. \\
    \midrule
    9 & Recht auf Löschung &
    Die betroffene Person hat das Recht, von dem Verantwortlichen die unverzügliche Löschung sie betreffender personenbezogener Daten zu verlangen. Der Verantwortliche ist verpflichtet, personenbezogene Daten unverzüglich zu löschen \parencite[Art. 17]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}. \\
\end{longtable}

% 1. \textbf{Technische Sicherheit} (BSI TR-02102-1): Kryptographische Algorithmen, Schlüssellängen, Verschlüsselungsprotokolle.

% \begin{longtable}{L{1cm}L{4cm}L{9cm}}
%     \caption{Ausgewählte Compliance Anforderungen für KRITIS}
%     \label{tab:technical_compliance} \\
%     \toprule
%     \textbf{Nr.} & \textbf{Compliance Anforderung} & \textbf{Beschreibung} \\
%     \midrule
%     \endfirsthead
%     \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
%     \toprule
%     \textbf{Nr.} & \textbf{Compliance Anforderung} & \textbf{Beschreibung} \\
%     \midrule
%     \endhead
%     \midrule
%     \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
%     \endfoot
%     \bottomrule
%     \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} \fixme{Eigene Darstellung auf Basis der Auflistungen und des Sequenzdiagramms in Anlehnung an \textcite[S. 130-132]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}.}} \\
%     \endlastfoot
%     1 & Einhaltung spezifischer Parameter-Sets für ML-DSA &
%     Zur Gewährleistung der vom BSI geforderten Sicherheitsniveaus dürfen für das Verfahren ML-DSA ausschließlich die Parameter-Sets verwendet werden, die den NIST Security Strength Categories 3 oder 5 entsprechen. Konkret sind dies ML-DSA-65 oder ML-DSA-87 \parencite[Kap. 5.3.4.2]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025} \\
%     \midrule
%     2 & Einhaltung spezifischer Parameter-Sets für ML-KEM &
%     Für den langfristigen Schutz vertraulicher Informationen mittels des gitterbasierten Schlüsselkapselungsverfahrens ML-KEM dürfen gemäß BSI-Einschätzung ausschließlich Parametersätze verwendet werden, die den NIST Security Strength Categories 3 oder 5 entsprechen. Zulässig sind demnach ML-KEM-768 sowie ML-KEM-1024 \parencite[Kap. 2.4.3]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025}. \\
%     \midrule
%     3 & Implementierung hybrider Schlüsseleinigung &
%     Um langfristige Vertraulichkeit (Schutz vor \textit{Store Now, Decrypt Later}) zu gewährleisten, muss für die Schlüsseleinigung zwingend ein hybrides Verfahren implementiert werden, das ein anerkanntes klassisches Verfahren mit einem empfohlenen PQC-KEM kombiniert \parencite[Kap. 2.2, 2.4]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025}. \\
%     \midrule
%     4 & Bevorzugte Verwendung von TLS 1.3 &
%     Für die Absicherung der Transportebene wird gemäß \textcite[Kap. 3.2]{bsi_TechnischeRichtlinieTR021022KryptographischeVerfahrenEmpfehlungenundSchlussellangenTeil2VerwendungTransport_2025} vorrangig das Protokoll TLS 1.3 empfohlen. \\
%     \midrule
%     5 & Protokollierung sicherheitsrelevanter Ereignisse (SzA) &
%     Zur Erfüllung der Pflicht zum Einsatz von Systemen zur Angriffserkennung (SzA) müssen sicherheitsrelevante Ereignisse auf System- und Netzebene zentral protokolliert werden \parencite[Nr. 101, 103]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024}. \\
%     \midrule
%     6 & Datenschutz durch Technikgestaltung (Privacy by Design) &
%     Gemäß \textcite[Art. 25 Abs. 1]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016} sind bereits bei der Entwicklung geeignete technische Maßnahmen zu treffen, um Datenschutzgrundsätze wirksam umzusetzen. Die Architektur erfüllt dies durch den konsequenten Verzicht auf die Speicherung personenbezogener Daten (PII) auf dem unveränderlichen Ledger (Off-Chain-Architektur) und die Nutzung von Zero-Knowledge Proofs. \\
%     \midrule
%     7 & Grundsatz der Datenminimierung &
%     Personenbezogene Daten müssen dem Zweck angemessen und auf das notwendige Maß beschränkt sein (Art. 5 Abs. 1 lit. c DSGVO). Durch den Einsatz von \textit{Pairwise DIDs} (did:peer) für jede Interaktion statt einer globalen ID wird die Korrelierbarkeit von Daten minimiert und Profilbildung technisch unterbunden \parencite[Art. 5]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}. \\
%     \midrule
%     8 & Technische Umsetzung des Rechts auf Löschung &
%     Das Recht auf Vergessenwerden (Art. 17 DSGVO) ist in Blockchain-Systemen oft problematisch. Durch die strikte Trennung von öffentlichen Identifikatoren (Ledger) und privaten Daten (lokale Wallet) ist eine Löschung technisch vollständig realisierbar, indem die lokale Wallet und die zugehörigen kryptografischen Schlüssel vernichtet werden \parencite[Art. 17]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016}. \\
% \end{longtable}


% % Definierter Scope: "Technical Compliance Readiness"

% % Fokus rein auf auf die Security by Design und Privacy by Design Aspekte des Software-Artefakts selbst ==>  "Beschaffenheit des Prototypen"



% % Kategorie 1: Krypto-Compliance \& Schlüsselmanagement (Code-Level)


% % Kategorie 2: Datenschutz-Architektur (Data-Level)


% % Kategorie 3: Interoperabilität \& Standards (Format-Level)


% ----

% % 2.5 Technische Informationssicherheit:
% % IDM-07 + IDM-08 bis IDM-13 + KRY-01 bis KRY-04

% % 2.6 Personelle und organisatorische Sicherheit
% % IDM-01 bis IDM-06 + IDM-09


% % Tabelle auf Basis von \textcite[S. 12-19]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024}


% Die KRITIS-spezifische Compliance für SSI-Systeme erfordert eine Verschneidung von vier Compliance-Ebenen:



% 2. \textbf{Organisatorische Sicherheit} (ORP.4, KRITIS-Anforderungskatalog): IAM, Funktionstrennung, Audit-Trails.

% 3. \textbf{Datenschutz} (DSGVO): Pseudonymität, Datensparsamkeit, Betroffenenrechte.

% Die Implementierung dieser Anforderungen in der SSI-Prototyp-Entwicklung wird in Kapitel 4.2 adressiert, wobei Compliance-Checks in den Design Science Research-Zyklus integriert werden.


% Die Umsetzung der KRITIS-Anforderungen in SSI-Systemen erfordert eine ganzheitliche Integration von Compliance-Anforderungen, die aus der deutschen und europäischen Regulierung abgeleitet sind. \textcite[S. 4]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024} präzisieren die Anforderungen des § 8a Absatz 1 und Absatz 1a BSIG für Betreiber Kritischer Infrastrukturen und definieren damit den verbindlichen Rahmen für die Informationssicherheit bei KRITIS-Systemen.

% % \paragraph*{Identitäts- und Berechtigungsmanagement (IAM)}

% % Das Identitäts- und Berechtigungsmanagement bildet das Fundament für sichere SSI-Systeme im KRITIS-Kontext. \textcite[S. 1--2]{bsi_ITGrundschutzKompendiumORP4IdentitaetsundBerechtigungsmanagementEdition2023_2023} definieren, dass der Zugang zu schtzenswerten Ressourcen auf berechtigte Benutzende und IT-Komponenten einzuschränken ist und dass Benutzende sowie IT-Komponenten zweifelsfrei identifiziert und authentisiert werden müssen. Im Kontext von SSI bedeutet dies konkret, dass DIDs (Decentralized Identifiers) eindeutig einer Person oder IoT-Komponente zugeordnet werden müssen und dass eine nicht-abstreitbare Authentifizierung durch kryptografische Verfahren erfolgen muss.

% % \textcite[S. 12]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024} fordern die Implementierung von Multi-Faktor-Authentifizierung für Administratoren: „Automatischer Ablauf Multi-Faktor-Authentifizierung für Administratoren des KRITIS-Betreibers z.B. durch Smart Card oder biometrische Merkmale ist zwingend erforderlich, sofern ein Zugriff über öffentliche Netze erfolgt." Für SSI-Systeme bedeutet dies, dass die Ausweisung von administrativen Credentials und deren Delegation zur Verifizierung neuer Credentials durch hybride Authentifizierungsmechanismen (z.B. Kombination aus DID-basierten Signaturen und Hardware-Token) erfolgen muss.

% % Die Prinzipien \textit{Least Privilege} und \textit{Need-to-Know} müssen in SSI-Umgebungen durch eine strikte Implementierung granularer Zugriffsrechte auf Basis von Verifiable Credentials und selektiven Offenlegungsmechanismen realisiert werden. \textcite[S. 2]{bsi_ITGrundschutzKompendiumORP4IdentitaetsundBerechtigungsmanagementEdition2023_2023} weisen auf die Gefahr des „Wildwuchses" bei der Rechtevergabe hin: „Sind Prozesse beim Identitäts- und Berechtigungsmanagement unzureichend definiert oder implementiert, ist nicht gewährleistet, dass Zugriffe auf das erforderliche Maß eingeschränkt sind." Im SSI-Kontext ist dies besonders kritisch, da fehlerhaft ausgestellte Credentials mit unbegrenzter Gültigkeitsdauer zu permanenten Sicherheitslücken führen können.

% \paragraph*{Authentifizierung und Verschlüsselung}

% Das \textcite[S. 12--14]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024} verlangt für KRITIS-Systeme umfassende Anforderungen zur Transportverschlüsselung und zur Schlüsselverwaltung: „Das Nutzen von starken Verschlüsselungsverfahren z.B. AES und die Verwendung von sicheren Netzwerkprotokollen, die dem Stand der Technik entsprechen z.B. TLS, IPsec, SSH" sind anzuwenden. Im Kontext von SSI-Ledgern (z.B. Hyperledger Indy) bedeutet dies, dass sämtliche Transaktionen, Credentials und DIDs durch starke Verschlüsselung geschützt werden müssen. Die Schlüsselverwaltung muss dabei kritischen Anforderungen genügen: „Anforderungen für das sichere Erzeugen, Speichern, Archivieren, Abrufen, Verteilen, Entziehen und Löschen der Schlüssel" müssen dokumentiert und implementiert sein.

% Die Post-Quantum-Kryptographie-Standards (FIPS 203/204/205) sind gemäß \textcite[S. 30--41]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025} zu integrieren. Die Technische Richtlinie BSI TR-02102-1 empfiehlt konkrete PQC-Algorithmen: ML-KEM für Schlüsselvereinbarung, ML-DSA für Signaturen und SLH-DSA für zustandslose Signaturverfahren. Im SSI-Kontext ist die Hybridisierung dieser Verfahren mit klassischen Algorithmen (z.B. ECDSA) notwendig, um eine „Crop and Paste"-Resistenz zu erreichen: Eine Signatur ist nur gültig, wenn \textit{beide} klassische und quantensichere Signaturen valide sind.

% \paragraph*{Datenschutz und DSGVO-Compliance}

% Die Europäische Datenschutz-Grundverordnung (DSGVO) bildet eine zusätzliche Compliance-Schicht. \textcite[Art. 5, Art. 6]{daseuropaeischeparlamentundderratdereuropaeischenunion_VerordnungEU2016679EuropaeischenParlamentsundRatesvom27April2016_2016} definieren die Grundprinzipien der Datenverarbeitung: Rechtmäßigkeit, Transparenz, Zweckbindung, Datensparsamkeit, Richtigkeit, Speicherbegrenzung, Integrität und Vertraulichkeit. Für SSI-Systeme bedeutet dies, dass:

% \begin{enumerate}
% \item \textbf{DIDs müssen pseudonym sein:} Die Verknüpfung einer Person zu ihrer DID darf nicht ohne explizite Einwilligung erfolgen.

% \item \textbf{Selektive Offenlegung (Selective Disclosure):} Nur minimale Informationen aus Verifiable Credentials dürfen preisgegeben werden (z.B. Bestätigung eines Altersanspruchs ohne Geburtsdatum).

% \item \textbf{Datenlöschung:} Es müssen technische und organisatorische Mechanismen existieren, um Credentials (z.B. durch Revocation auf dem Ledger) zu annullieren und damit das Löschen von Attributen zu ermöglichen.

% \item \textbf{Betroffenenrechte:} Wallets und SSI-Agents müssen Funktionen zur Einsichtnahme (Art. 15 DSGVO), Berichtigung (Art. 16) und Vergessenwerden (Art. 17) unterstützen.
% \end{enumerate}

% \paragraph*{Kryptografische Anforderungen und Schlüssellängen}

% \textcite[S. 20]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025} legen fest, dass ein Sicherheitsniveau von mindestens 120 Bits zu erreichen ist. Konkrete Empfehlungen für SSI-Systeme:

% \begin{itemize}
% \item \textbf{Symmetrische Verschlüsselung:} AES-256 ist verpflichtend für Daten mit KRITIS-Relevanz.

% \item \textbf{Asymmetrische Schlüssellängen:} RSA-Schlüssel müssen mindestens 3000 Bits betragen; %EC-Systeme müssen eine Ordnung q ≥ 2\textsuperscript{250} verwenden.

% \item \textbf{Digitale Signaturen:} ML-DSA-65 oder ML-DSA-87 (quantensicher) sowie klassische %ECDSA (mit Ordnung q ≥ 2\textsuperscript{250}) müssen hybrid kombiniert werden.

% \item \textbf{Hashfunktionen:} SHA-256, SHA-384 und SHA-512 sind akzeptabel; SHA-1 ist obsolet und unzulässig.
% \end{itemize}

% \paragraph*{Versionierung und Deprecation von Verfahren}

% Eine Besonderheit der KRITIS-Compliance ist die Notwendigkeit von Deprecation-Roadmaps. \textcite[S. 56]{bsi_BSITR021021KryptographischeVerfahrenEmpfehlungenundSchluessellaengenVersion202501_2025} empfehlen beispielsweise, die Verwendung von DSA (Digital Signature Algorithm) bis 2029 auslaufen zu lassen. Im SSI-Kontext bedeutet dies, dass:

% \begin{enumerate}
% \item Protokolle zur \textit{Credential Migration} implementiert werden müssen, um alte Credentials mit veralteten Algorithmen durch neue mit aktuellen Algorithmen zu ersetzen.

% \item Die Vertrauenskette (Trust Chain) von Vertrauenswurzeln (Root DIDs, Ledger Validators) muss regelmäßig überprüft und aktualisiert werden.

% \item Audit-Logs müssen dokumentieren, wann und wie kryptographische Verfahren aktualisiert wurden.
% \end{enumerate}

% \paragraph*{Governance und Audit-Trail}

% \textcite[S. 7--27]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024} fordert ein umfassendes Information Security Management System (ISMS) mit definierten Rollen, Verantwortlichkeiten und regelmäßigen Audits. Im SSI-Kontext bedeutet dies: Die Betreiber der Vertrauensinfrastruktur (Ledger Betreiber, Issuer, Verifier) müssen:

% \begin{enumerate}
% \item Rollen und Verantwortlichkeiten dokumentieren (z.B. DID Registry Steward, Credential Issuer Manager).

% \item Eine Revocation- und Recovery-Strategie für Credentials implementieren, die dokumentiert und regelmäßig getestet wird.

% \item Mindestens jährlich Penetrationstests durchführen, insbesondere auf SSI-spezifische Angriffsmuster (z.B. illegitime Credential-Ausstellung, DID-Hijacking).

% \item Meldepflichten erfüllen: \textcite[S. 22--23]{bsi_KonkretisierungKRITISAnforderungen8aAbsatz1undAbsatz1aBSIG_2024} verlangen, dass Strungen der Verfügbarkeit, Integrität, Authentizität und Vertraulichkeit, die zu einem Ausfall oder erheblicher Beeinträchtigung der kritischen Dienstleistung führen, unverzüglich an das BSI gemeldet werden müssen.
% \end{enumerate}







\subsubsection{Framework- und Technologie-Auswahl}

Die in Abschnitt 4.1.1 spezifizierten funktionalen und KRITIS-Compliance-Anforderungen bilden die Grundlage für eine systematische Auswahl der technologischen Komponenten. Die akademische Literatur bietet umfassende Vergleichsstudien zur Bewertung von SSI-Frameworks. Nokhbeh Zaeem et al. führten eine der detailliertesten Analysen durch, in der 31 blockchain-basierte SSI-Lösungen anhand von 22 nicht-funktionalen und sieben funktionalen Anforderungsgruppen untersucht wurden \parencite[S. 128--129]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Die Autoren kommen zu dem Ergebnis, dass Sovrin, das auf Hyperledger Indy basiert, zu den besten derzeit verfügbaren SSI-Lösungen zählt und dass blockchain-basierte SSI-Ansätze ihren nicht-blockchain-basierten Pendants überlegen sind \parencite[S. 133]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}.

\paragraph{DLT-Plattform}

Für die blockchain-basierte Speicherung von DIDs, Schemas und Credential Definitions wird Hyperledger Indy als DLT-Plattform gewählt. Hyperledger Indy wurde als spezialisierte DLT-Plattform für dezentrale Identitätsverwaltung konzipiert und ist ein öffentlich eingesehenes, aber permissioned blockchain-basiertes Ledger, bei dem nur autorisierte Stewards Transaktionen validieren dürfen. Dies erfüllt die KRITIS-Governance-Anforderungen aus 4.1.1.3, insbesondere die Notwendigkeit einer auditier- und kontrollierbaren Validierungsinfrastruktur.

Dixit et al. analysieren ein dezentrales IIoT-Identitätsframework mit identischen SSI-Komponenten sowohl auf Ethereum als auch auf Hyperledger Indy und weisen Hyperledger Indy als public permissioned und Ethereum als permissionless aus \parencite[S. 2]{dixit_DecentralizedIIoTIdentityFrameworkbasedSelfSovereignIdentityusingBlockchain_2022}. In ihren Experimenten zeigen sich signifikante Unterschiede hinsichtlich Transaktionskosten und Performance: Während Identitätsoperationen und Credential-Prozesse auf Indy ressourceneffizient und mit geringeren Gebühren abgewickelt werden können, sind diese bei Ethereum mit höherem Overhead und längeren Ablaufzeiten verbunden \parencite[S. 3]{dixit_DecentralizedIIoTIdentityFrameworkbasedSelfSovereignIdentityusingBlockchain_2022}.

Su und Hsu betonen, dass Hyperledger Indy es Identitätsinhabern ermöglicht, ihre Daten und Beziehungen unabhängig zu kontrollieren, und dass die Plattform auf offenen Standards und sicheren Mechanismen der Public-Key-Kryptographie basiert, die mit anderen distributed ledgers interoperabel ist \parencite[S. 548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}. Ein besonderes Merkmal ist die Integration von Zero-Knowledge-Proof-Verfahren (AnonCreds), die die Privacy-Anforderung NFR4 aus 4.1.1.2 adressieren. Die Produktionsreife ist durch mehrjährigen Betrieb des Sovrin-Netzwerks nachgewiesen, das auf Indy basiert und seit 2020 kontinuierlich verfügbar ist.

Nokhbeh Zaeem et al. identifizieren Connect.me als eine der besten SSI-Lösungen für Einzelpersonen, die von Evernym entwickelt wurde und auf dem Sovrin-Netzwerk basiert, welches wiederum auf Hyperledger Indy aufbaut \parencite[S. 133]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Alternative SSI-Frameworks wie uPort und Jolocom basieren auf der Ethereum-Blockchain. Diese public permissionless Architektur führt zu höheren Transaktionskosten und bietet weniger inhärente Privacy-Preserving-Mechanismen im Vergleich zu Indys permissioned Modell. ShoCard repräsentiert eine kommerzielle Lösung mit geringerer akademischer Dokumentation und eingeschränkter Community-getriebener Entwicklung \parencite[S. 133]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}.

\fixme{VON-Network ergänzen}

\paragraph{SSI-Framework}

Während Indy die Ledger-Ebene bereitstellt, ist eine separate Protokoll- und Implementierungsebene notwendig, um SSI-Workflows (Credential Issuance, Verification, Holder-Management) zu orchestrieren. Hyperledger Aries erfüllt diese Rolle als protokollbasiertes Framework. Ferdous et al. zeigen anhand eines Praxisbeispiels der SSI-Integration für Web-Services, dass Hyperledger Indy aktuell eine der zunehmend genutzten Blockchain-Plattformen im Bereich SSI darstellt \parencite[S. 7]{ferdous_SSI4WebSelfsovereignIdentitySSIFrameworkWeb_2022}. In ihrer Implementierung kombinieren sie Hyperledger Aries und Indy als technische Grundlage und veranschaulichen so die praktische Umsetzbarkeit dieser Framework-Kombination \parencite[S. 6--7]{ferdous_SSI4WebSelfsovereignIdentitySSIFrameworkWeb_2022}.

Aries Cloud Agent Python (ACA-Py) ist die offizielle Referenzimplementierung und eine vollständige Realisierung des Aries Interop Profile (AIP) 2.0. Ferdous et al. erklären, dass Hyperledger Aries Bibliotheken zur Entwicklung von SSI-Applikationen in verschiedenen Programmiersprachen bereitstellt, und beschreiben, dass der SSI-Agent unter Verwendung von NodeJS entwickelt wurde, der auf die ACA-Py-Bibliothek angewiesen ist, um verschiedene SSI-Funktionalitäten bereitzustellen \parencite[S. 5]{ferdous_SSI4WebSelfsovereignIdentitySSIFrameworkWeb_2022}. Die Architektur implementiert DIDComm-Messaging zur sicheren Kommunikation zwischen Identity Holder, Issuer und Verifier und unterstützt mehrere Verifiable Credential Formate, insbesondere W3C VCs und Indy AnonCreds. Diese Standardisierung erfüllt die NFR1 (Nachweisbarkeit) und adressiert die Interoperabilitätsanforderungen aus 4.1.1.2.

ACA-Py implementiert alle sieben funktionalen Anforderungen aus 4.1.1.1: Issuer Discovery (FR1) über DID-Registry-Queries, Connection Creation (FR2) via DIDComm-Protokolle, Credential Creation (FR3) mit W3C VCs und AnonCreds, Verification (FR4) mit ZKP-basierten Presentations, Backup/Recovery (FR5) über Wallet-Export-Mechanismen, Derive/Share (FR6) durch granulare Zugriffskontrolle und Revocation (FR7) via Revocation Registry auf dem Indy-Ledger.

Ein zentrales Merkmal von ACA-Py ist die Separation von Agent und Business Logic: Der Agent kommuniziert über HTTP mit einer externen Controller-Anwendung, die in beliebigen Programmiersprachen implementiert werden kann. Dies ermöglicht Flexibilität und Modularität, adressiert die NFR2 (Interoperabilität) durch Unterstützung von W3C-Standards und ermöglicht pluginbare Integrationspunkte für spezifische Anforderungen wie die Post-Quantum-Kryptografie. Die Architektur trennt den Agent von der Business-Logic-Komponente (Controller), wodurch eine sprachunabhängige Integration über HTTP-APIs möglich wird \parencite[S. 5]{ferdous_SSI4WebSelfsovereignIdentitySSIFrameworkWeb_2022}.

\paragraph{Kryptografiebibliothek}

Für die Integration von Post-Quantum-Kryptografie wird die Open Quantum Safe (OQS) Bibliothek in Verbindung mit OpenSSL 3.x gewählt. Die Anforderungen der Post-Quantum-Kryptografie (Forschungsfrage FF2, Abschnitt 1.4) erfordern eine Kryptografiebibliothek, die NIST-standardisierte PQC-Algorithmen (FIPS 203, 204, 205) implementiert und gleichzeitig klassische Algorithmen hybridisieren kann (Crop-and-Paste-Resistenz aus 4.1.1.3).

Open Quantum Safe (OQS) ist ein Linux-Foundation-Projekt, das die Transition zu quantenresistenter Kryptografie unterstützt. OQS besteht aus zwei Hauptkomponenten: liboqs, eine C-Bibliothek, die quantum-resistente Algorithmen implementiert, sowie Prototyp-Integrationen in verbreitete Protokolle und Anwendungen, einschließlich OpenSSL. Diese Designentscheidung ermöglicht eine kryptoagile Architektur, die zukünftige Standards leicht integrieren kann.

Der oqs-provider für OpenSSL 3.x ermöglicht die nahtlose Integration quantensicherer Algorithmen in bestehende OpenSSL-basierte Systeme durch Bereitstellung einer shared library. Diese Integration ist entscheidend, da sowohl Hyperledger Indy-Nodes als auch ACA-Py-Instanzen OpenSSL für kryptografische Operationen verwenden. Durch die Verwendung von oqs-provider werden alle kryptografischen Operationen auf PQC-Algorithmen erweitert, ohne dass Änderungen an der OpenSSL-API notwendig sind. Dies hat den Vorteil, dass bestehende Systeme mit minimalen Änderungen quantensicher gestaltet werden können.

OQS unterstützt Hybrid-Schemes, bei denen klassische und quantensichere Algorithmen kombiniert werden: Beispielsweise können ML-KEM (NIST FIPS 203, Schlüsselvereinbarung) mit ECDH und ML-DSA (NIST FIPS 204, digitale Signaturen) mit ECDSA hybridisiert werden. Dies erfüllt die Crop-and-Paste-Resistenz-Anforderung aus 4.1.1.3: Eine Signatur ist nur dann gültig, wenn sowohl die klassische als auch die quantum-sichere Variante verifizieren.

Die Wahl von OQS erfüllt die kryptografische Agilitätsanforderung (Forschungsfrage FF3, \autoref{tab:forschungsfragen}): Die modulare Architektur erlaubt den Austausch von Algorithmen über Konfigurationsänderungen ohne Code-Anpassungen. Su und Hsu beschreiben, dass Hyperledger Ursa eine sichere, gemeinsame kryptographische Bibliothek darstellt, die modular und erweiterbar konzipiert ist \parencite[S. 548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}.

\paragraph{Revocation-Infrastruktur}

Für die Revocation-Anforderung FR7 wird das Revocation-Schema von Hyperledger Indy gewählt. Indy implementiert Privacy-Preserving-Revocation durch kryptografische Akkumulatoren, wodurch Holder Nicht-Widerrufung via Zero-Knowledge-Proofs nachweisen können, ohne ihre Credential-IDs oder Indizes preiszugeben \parencite{hardman_0011CredentialRevocationHyperledgerIndyHIPEdocumentation_2018}. Dieses Verfahren erfüllt gleichzeitig NFR4 (Pseudonymität) und adressiert DSGVO-Compliance durch Unlinkability - ein kritisches Merkmal für SSI-Systeme in kritischen Infrastrukturen.

Verglichen mit Alternativen wie Revocation-Listen oder Bitmap-basierten Ansätzen bietet das Akkumulator-Verfahren Auditierbarkeit (Akkumulator-Werte sind auf dem öffentlichen Ledger transparent) bei gleichzeitigem Datenschutz, da keine PII in Revocation-Proofs enthalten ist \parencite{hardman_0011CredentialRevocationHyperledgerIndyHIPEdocumentation_2018}. Die Entkopplung von Issuance und Revocation (Issuer-Autonomie ohne direkten Kontakt zu Holdern) bietet zudem Skalierungsvorteile für KRITIS-Szenarien mit hohem Durchsatz.

Der **Indy Tails Server** wird als Infrastrukturkomponente gewählt, um die erforderlichen Faktoren für das Akkumulator-Verfahren bereitzustellen \parencite{bcgov_GitHubBcgovIndytailsserverThissoftwarestoresmakesavailabletailsfilesuseHyperledger_}. Die BC-Government-Referenzimplementierung bietet native Integration mit ACA-Py und stellt sicher, dass Holder und Verifier die notwendigen kryptografischen Daten abrufen können, ohne auf zentrale Issuer-Services angewiesen zu sein. Diese dezentralisierte Architektur mit öffentlichem Ledger und verteiltem Fileserver erfüllt die Governance- und Auditierbarkeitsanforderungen von KRITIS (4.1.1).

\paragraph{Sidecar Proxy}

Für die transparente Integration von Post-Quantum-Kryptografie in die Agent-zu-Agent-Kommunikation wird NGINX als Sidecar Proxy gewählt. Diese Designentscheidung adressiert eine zentrale Herausforderung: ACA-Py unterstützt nativ keine PQC-Algorithmen, und substantielle Modifikationen der Codebasis würden kryptografische Agilität und Wartbarkeit beeinträchtigen.

Das Sidecar-Proxy-Pattern ermöglicht die automatische Injektion privilegierter Proxy-Container, die sämtliche ein- und ausgehende Kommunikation eines Pods abfangen und kryptografisch schützen, ohne die zugrundeliegende Anwendungslogik zu modifizieren \parencite[S. 1--2]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunications_2024}. \textcite[S. 2, 5]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunications_2024} demonstrieren dieses Pattern für Cryptographic Access Control in Cloud-Native-Anwendungen und identifizieren Transparenz, Automatisierung und Modularität als zentrale Eigenschaften. Diese Arbeit adaptiert das Pattern von CAC auf PQC-Integration für SSI-Systeme.

NGINX wird gewählt, da es mit OpenSSL 3.5+ kompiliert werden kann, das über den OQS-Provider NIST-standardisierte PQC-Algorithmen (ML-KEM-768, ML-DSA-65/87) unterstützt \parencite{nginx_GitHubNginxNginxofficialNGINXOpenSourcerepository_}. Die TLS-Termination erfolgt auf NGINX-Ebene mit hybriden Key Exchange Mechanisms (X25519MLKEM768), bevor die entschlüsselte Kommunikation über HTTP an den Backend-ACA-Py-Agent weitergeleitet wird. Im Vergleich zu vollständigen Service-Mesh-Lösungen (Istio, Linkerd, Consul) bietet NGINX höhere kryptografische Flexibilität: Während Service Meshes primär auf klassisches mTLS setzen und keine native PQC-Unterstützung bieten \parencite[S. 6, Tab. 3]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunications_2024}, erlaubt die NGINX+OQS-Integration beliebige NIST-standardisierte Algorithmen ohne zusätzliche Control-Plane-Komplexität.

Die Wahl erfüllt mehrere nicht-funktionale Anforderungen: **Transparenz (NFR14)** durch vollständige Entkopplung von ACA-Py, sodass keine Anwendungsmodifikationen erforderlich sind \parencite[S. 2]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunications_2024}; **Kryptoagilität (NFR15)** durch konfigurationsbasierte Algorithmus-Austauschbarkeit über OpenSSL-Provider-Architektur; **Security (NFR7)** durch PQC-hybride TLS 1.3-Termination mit Crop-and-Paste-Resistenz. Die Docker-Sidecar-Architektur gewährleistet zudem automatisches Scaling und nahtlose Integration in Orchestrierungsumgebungen.

\paragraph{Anforderungsmapping}

Die gewählte Technologiekombination Hyperledger Indy + ACA-Py + NGINX + OQS + OpenSSL 3.x erfüllt systematisch alle Anforderungskategorien aus 4.1.1:

\textbf{Funktionale Anforderungen (4.1.1.1):} Indy stellt die dezentralisierte Ledger-Infrastruktur für DIDs und Schemas bereit (FR1, FR2, FR7), ACA-Py implementiert die Credential-Lebenszyklusverwaltung und SSI-Workflows (FR3, FR4, FR5, FR6). OQS integriert quantensichere Signaturen und Verschlüsselung über NGINX-Reverseproxy in die Agent-zu-Agent-Kommunikation, ohne ACA-Py zu modifizieren.

\textbf{Nicht-funktionale Anforderungen (4.1.1.2):} Nachweisbarkeit und Interoperabilität werden durch W3C-Standards und standardisierte Aries-Protokolle erreicht (NFR1, NFR2). Portabilität ermöglicht W3C VC-Export/Import (NFR3). Pseudonymität ergibt sich aus der paarweisen DID-Struktur (NFR4). Recovery wird durch Wallet-Backup und Social Recovery-Mechanismen unterstützt (NFR5). Skalierbarkeit ist durch optimierte Konsens-Mechanismen gegeben (NFR6). Security wird durch PQC-Hybridisierung (ML-KEM-768, ML-DSA-65/87) über NGINX-TLS-Termination und klassische Kryptografie erreicht (NFR7). Usability wird durch ACA-Py's HTTP-API und Wallet-Schnittstellen bereitgestellt (NFR8). Transparenz wird durch das Sidecar-Proxy-Pattern gewährleistet, bei dem ACA-Py keine Modifikationen erfordert (NFR14) \parencite[S. 2]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunications_2024}. Kryptoagilität ermöglicht der modulare NGINX+OQS-Aufbau, der Algorithmus-Austausch ohne Anwendungsänderungen erlaubt (NFR15).

\textbf{KRITIS-Compliance (4.1.1.3):} Die permissioned Architektur von Indy gewährleistet Governance (4.1.1.3.1) und Auditierbarkeit (4.1.1.3.2) durch Ledger-Transparenz. Das IAM-Framework wird durch DID-basierte Authentifizierung und granulare Credential-basierte Zugriffskontrolle (Least Privilege) realisiert (4.1.1.3.4). Authentifizierung und Verschlüsselung werden durch PQC-hybride Schemas nach BSI-Richtlinien implementiert: NGINX terminiert TLS 1.3 mit hybriden KEMs (X25519MLKEM768) und ML-DSA-signierten Zertifikaten (4.1.1.3.3). Datenschutz wird durch Zero-Knowledge-Proofs und selektive Offenlegung unterstützt (4.1.1.3.5).

\textbf{Post-Quantum-Kryptografie:} OQS integriert NIST-standardisierte Algorithmen (ML-KEM-768, ML-DSA-65/87, SLH-DSA) über den OpenSSL-3-Provider \parencite{openquantumsafe_TLS_2025}. Die Implementierung als NGINX-Reverseproxy ermöglicht transparente PQC-Integration ohne Modifikation von ACA-Py und gewährleistet Crop-and-Paste-Resistenz durch Hybrid-Schemes. Die kryptografische Agilität wird durch die OpenSSL-Provider-Architektur gewährleistet, die Algorithmus-Aktualisierungen durch Konfigurationsänderungen statt Code-Anpassungen erlaubt.

Die Eignung von Hyperledger Indy und Aries für diese Arbeit lässt sich zusammenfassend anhand folgender Kriterien begründen: Hyperledger Indy ist als spezialisierte DLT-Plattform konzipiert, die nativ DIDs und VCs unterstützt \parencite[S. 548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}. Der production-ready Status beider Projekte mit nachgewiesener Stabilität in Produktivumgebungen erfüllt Anforderungen an technologische Reife. Nokhbeh Zaeem et al. fassen zusammen, dass ihre vergleichende Analyse den allgemeinen Konsens über die besten SSI-Lösungen bestätigt: Sovrin, Connect.me, uPort und ShoCard decken die meisten Anforderungen ab \parencite[S. 133]{nokhbehzaeem_BlockchainBasedSelfSovereignIdentitySurveyRequirementsUseCasesComparativeStudy_2021}. Aries RFCs definieren standardisierte Protokolle, die Interoperabilität zwischen verschiedenen Implementierungen gewährleisten. Das permissioned blockchain-Modell, production-grade Stabilität und dokumentierte Enterprise-Deployments demonstrieren die Eignung für kritische Infrastrukturen. Die PQC-Integration via NGINX-Reverseproxy erweitert diese Eignung um Quantum-Resistenz, ohne die bewährte Architektur zu kompromittieren.


% ----------


acapy ist das beste ==> \parencite{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}


\subsubsection{Architekturentwurf}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Architektur_Iteration1.png}
    \caption{Architekturentwurf Iteration 1}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Architektur_Iteration1}
\end{figure}


Die in \autoref{fig:Architektur_Iteration1} dargestellte Systemarchitektur konkretisiert den in den vorangegangenen Kapiteln definierten Technologie-Stack durch eine Drei-Akteur-Topologie: \textit{Issuer}, \textit{Holder} und \textit{Verifier}. Diese Topologie veranschaulicht das operative Zusammenspiel der Komponenten sowie die Integrationsstrategie für Post-Quantum-Kryptografie.

Die zentrale Designentscheidung ist die vertikale Schichtung jedes Akteurs: Jeder verfügt über einen ACA-Py Agent als Protokoll-Implementierung und einen NGINX PQC Sidecar Proxy als Transportschicht. Diese Architektur realisiert das \emph{Encryption Proxy Pattern}, bei dem die Agenten unverschlüsselt auf HTTP-Ebene mit ihren lokalen Proxies kommunizieren und diese alle Verschlüsselungsoperationen durchführen. Eine Credential-Issuance-Nachricht wird vom Issuer-Agent im Klartext an seinen NGINX Proxy gesendet, dort mit X25519+ML-KEM-768 sowie ML-DSA-65/87 verschlüsselt, über das Netzwerk transportiert und beim Holder-Proxy ankommen, wo sie dechiffriert wird.

Dieses Pattern adressiert die zentrale Herausforderung, dass ACA-Py nativ keine Post-Quantum-Kryptografie unterstützt. Durch die Sidecar-Architektur wird die komplette Netzwerkebene quantenresistent, ohne den Agent-Code zu modifizieren. Die horizontalen Pfeile zwischen den Proxy-Schichten zeigen die DIDComm-Protokoll-Kommunikation mit vollständiger PQC-Transportverschlüsselung.

Unterhalb dieser Agent-Ebene ist die Ledger-Schicht dargestellt mit vier Validator-Knoten (Indy Nodes 1-4), die das permissioned Distributed Ledger replizieren. Der \emph{Indy Webserver Node} stellt die HTTP-API für DID- und Credential-Definition-Queries bereit, auch diese durch einen NGINX PQC Sidecar Proxy geschützt. Die \emph{Revocation Registry} mit integriertem \emph{Indy Tails Server} implementiert Privacy-Preserving-Revocation mittels kryptografischer Akkumulatoren, was Holder ermöglicht, Nicht-Widerrufung via Zero-Knowledge-Proofs nachzuweisen, ohne ihre Identität preiszugeben.

Ein kritischer Aspekt ist die \textbf{duale PQC-Protection}: Nicht nur inter-Agenten-Kommunikation, sondern auch Ledger-Zugriffsoperationen (DID-Registry-Queries, Credential-Abrufe, Revocation-Status-Checks) durchlaufen die NGINX-Proxies. Dies gewährleistet, dass selbst Infrastruktur-Abfragen quantenresistent sind.

Die Architektur verkörpert mehrere Designprinzipien: \textbf{Kryptografische Transparenz} ermöglicht Algorithmus-Updates (z.\,B. ML-KEM-768 zu ML-KEM-1024) durch OpenSSL-Provider-Konfiguration, nicht durch Code-Änderungen. \textbf{Separation of Concerns} trennt Geschäftslogik (Agent) von Kryptographie (Proxy), wodurch Zukunftssicherheit entsteht. Die \textbf{modulare Komponentenstruktur} ermöglicht Skalierbarkeit: Mehrere ACA-Py-Instanzen nutzen den gleichen Proxy-Pool; Ledger-Knoten und Tails-Server skalieren unabhängig.

% \paragraph{High Level}

% Der Architekturentwurf der prototypischen PQC-SSI-Integration basiert auf dem vierschichtigen Framework von Naghmouchi und Laurent (2025), das SSI-Systeme in Infrastructure, Identifiers \& Cryptographic Material, Credentials \& Presentations sowie Wallet Application untergliedert. \fixme{KAP 2 ???} Diese Architekturschichten bilden die konzeptionelle Grundlage für die systematische Integration post-quantensicherer Kryptographie in bestehende SSI-Komponenten.

% \textbf{Infrastructure Layer (Layer 1).} Die Infrastrukturschicht wird durch von-network realisiert, eine portable Hyperledger-Indy-Node-Implementierung für Entwicklungs- und Testzwecke, welches ein dezentrales Ledger bereit stellt, das als Verifiable Data Registry (VDR) und Decentralized Public Key Infrastructure (DPKI) fungiert \parencite{bcgov_GitHubBcgovVonnetworkportabledevelopmentlevelIndyNodenetwork_}. Die Implementierung umfasst vier Indy-Nodes in Docker-Containern, einen Ledger Browser mit API-Schnittstelle sowie drei distinkte Ledger für Domain-, Pool- und Config-Transaktionen. Als VDR ermöglicht von-network die Publikation von DIDs, öffentlichen Schlüsseln, Credential Definitions und Revocation Registries.

% \textbf{Identifiers \& Cryptographic Material Layer (Layer 2).} Diese Schicht integriert post-quantensichere kryptographische Primitive durch die Kombination von liboqs und OpenSSL 3.5. Liboqs ist eine C-Bibliothek des Open Quantum Safe-Projekts, die NIST-standardisierte PQC-Algorithmen bereitstellt, insbesondere ML-KEM (FIPS-203) für Schlüsselaustausch sowie ML-DSA (FIPS-204) und SLH-DSA (FIPS-205) für digitale Signaturen \parencite{open-quantum-safe_GitHubOpenquantumsafeLiboqslibraryprototypingexperimentingquantumresistantcryptography_}. OpenSSL 3.5 als erste Long-Term-Support-Version mit integrierter PQC-Unterstützung ermöglicht hybride Kryptosysteme, die klassische und post-quantensichere Verfahren kombinieren. Die Integration erfolgt über den OQS-Provider, der PQC-Algorithmen in die OpenSSL-Provider-Architektur einbindet \parencite{openssl_GitHubOpensslOpensslTLSSSLcryptolibrary_}.

% \textbf{Credentials \& Presentations Layer (Layer 3).} Der Indy Tails Server bildet die Revocation-Infrastruktur für AnonCreds-Credentials \parencite{bcgov_GitHubBcgovIndytailsserverThissoftwarestoresmakesavailabletailsfilesuseHyperledger_}. Tails-Dateien enthalten kryptographische Akkumulator-Werte für Zero-Knowledge-Proofs der Nicht-Widerrufbarkeit. Der Server stellt eine HTTP/HTTPS-API für Upload und Download der Tails-Dateien bereit und integriert sich nahtlos mit ACA-Py. Die URL des Tails-Servers wird in der Revocation Registry Definition auf dem Ledger persistiert, wodurch Holder die Dateien für Non-Revocation-Proofs abrufen können \parencite{bcgov_GitHubBcgovIndytailsserverThissoftwarestoresmakesavailabletailsfilesuseHyperledger_}.

% \textbf{Wallet Application Layer (Layer 4).} Aries Cloud Agent Python (ACA-Py) implementiert die Wallet-Funktionalität als Open-Source-Framework für dezentrale Identitätsagenten. Die Architektur trennt den Agent (Core-Funktionalität für Aries-Protokolle, DIDComm, Secure Storage) vom Controller (Business-Logik, Anwendungsschnittstelle) \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_}. Die Kommunikation erfolgt bidirektional: Der Agent sendet Webhook-Notifications an den Controller, welcher über eine REST-API administrative Anweisungen zurückgibt. ACA-Py unterstützt Multi-Tenancy, pluggable Storage-Backends sowie Transport-Mechanismen und operiert in den Schichten 2 und 3 des Trust-Over-IP-Frameworks \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_}.

% \textbf{Komponenteninteraktionen.} Die Architektur weist ausgeprägte Interdependenzen zwischen den Schichten auf. ACA-Py (Layer 4) schreibt DIDs, Schemas und Credential Definitions auf von-network (Layer 1) und liest Genesis-Transaktionen für die Pool-Initialisierung. Der Tails-Server (Layer 3) wird von ACA-Py für Speicherung und Abruf von Revocation-Tails-Dateien genutzt. Die PQC-Integration über OpenSSL 3.5 und liboqs (Layer 2) ermöglicht zukünftig quantum-resistente TLS-Verbindungen zwischen Indy-Nodes sowie hybride Signaturverfahren in ACA-Py.

% \paragraph{Low Level}

% Die Low-Level-Architektur spezifiziert die internen Komponenten und Datenflüsse innerhalb der vier Schichten.

\subsubsection{Setup der Entwicklungsumgebung}

\fixme{ABBILDUNG?}

Die Entwicklungsumgebung für diese Arbeit folgt einer geschichteten Architektur, die Ressourceneffizienz, Isolierung und Reproduzierbarkeit kombiniert. Das System nutzt Hyper-V als Type-1-Hypervisor unter Windows 10, der direkt auf der Hardware-Ebene arbeitet und eine stabile Grundlage für die virtualisierte Umgebung bietet.

Auf der Hyper-V-Infrastruktur wird eine Ubuntu 24.04 LTS Virtual Machine betrieben, die als Gastbetriebssystem für die Entwicklungsarbeit dient. Ubuntu 24.04 LTS erfüllt die notwendigen Systemanforderungen und bietet als Long-Term-Support-Version langfristige Stabilität und Sicherheitsupdates - entscheidend für reproduzierbare akademische Forschung.

Die eigentliche Entwicklungsumgebung wird durch Docker-Container bereitgestellt, die auf der Ubuntu-VM ausgeführt werden. Docker ermöglicht die Definition isolierter, vorkonfigurierter Entwicklungsumgebungen über Dockerfiles, wodurch Abhängigkeiten und Toolketten konsistent verwaltet werden. Dies minimiert Konfigurationsfehler und gewährleistet, dass alle Iterationen der Forschungsarbeit unter identischen Bedingungen reproduzierbar sind.

Die Quellcodeverwaltung erfolgt über Git und GitHub, wodurch Versionskontrolle, Kollaborationsmöglichkeiten und vollständige Nachverfolgbarkeit aller Entwicklungsschritte realisiert werden. Diese Kombination aus Hyper-V, Ubuntu, Docker und Git/GitHub schafft eine isolierte, versionskontrollierte und hochgradig reproduzierbare Forschungsinfrastruktur, die akademische Standards erfüllt und gleichzeitig moderne DevOps-Praktiken umsetzt.


% SSI Layers \parencite[S. 7]{naghmouchi_SystematicReviewLayeredFrameworkPrivacybyDesignSelfSovereignIdentitySystems_2025}

% Infrastructure ==> indy on besu \\
% Identifier + Cryptographic material ==> Indy DID \\
% Credential \& Presentations ==> AnonCreds W3C VCs \\
% Agent ==> ACA-Py




% Layer Sicht:

% Agent ==> ACA-Py (als Edge Agent aufgesetzt)
% Credential \& Presentations ==> AnonCreds W3C VCs \\
% Identifier + Cryptographic material ==> Indy DID \\
% Infrastructure ==> indy on besu \\



% indy besu ==> \parencite{shcherbakov_HyperledgerIndyBesupermissionedledgerSelfsovereignIdentity_2024}



\subsubsection{Implementierung}

\paragraph{Zertifikatsstruktur}

Die Zertifikatsinfrastruktur für die PQC-Sidecar-Proxies basiert auf einer selbstsignierten Root Certificate Authority (Root CA), die mit dem Post-Quantum-Signaturalgorithmus ML-DSA-87 (NIST FIPS-204) mit maximaler Schlüsselgröße (4880 Bit) erstellt wurde. Die Root CA dient als Trust Anchor für alle in der Architektur verwendeten TLS-Zertifikate und gewährleistet, dass sämtliche Zertifikatssignaturen quantenresistent sind.

Das Zertifikatserstellungsverfahren folgt einem fünfschrittigen Workflow, wie in \autoref{fig:Zertifikatserstellungsworkflow} dargestellt: Zunächst wird der Root-CA-Schlüssel generiert (\textit{Step 1}), gefolgt von der Erstellung des selbstsignierten Root-CA-Zertifikats mit einer Gültigkeit von zehn Jahren (\textit{Step 2}). Anschließend werden für jeden Sidecar-Proxy dedizierte Schlüssel mit ML-DSA-65 generiert (\textit{Step 3}), die ein besseres Verhältnis zwischen Sicherheit und Performance bieten (3044 Bit). Für jeden Proxy wird ein Certificate Signing Request (CSR) erstellt, der die erforderlichen Subject Alternative Names (SANs) enthält (\textit{Step 4}), bevor die Zertifikate abschließend von der Root CA mit ML-DSA-65 und SHA3-256 signiert werden (\textit{Step 5}). Die detaillierte Implementierung dieses Workflows ist in Listing \autoref{lst:Zertifikatserstellungsworkflow} dokumentiert.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Zertifikatserstellungsworkflow.png}
    \caption{Zertifikatserstellungsworkflow für PQC-basierte Sidecar-Proxies}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Zertifikatserstellungsworkflow}
\end{figure}

Das Root-CA-Zertifikat besitzt eine Gültigkeit von zehn Jahren und wird in den System-Trust-Store aller Container importiert, sodass Python-Bibliotheken wie \texttt{requests} und Kommandozeilen-Tools wie \texttt{curl} die ML-DSA-65-signierten Zertifikate automatisch als vertrauenswürdig erkennen.

Für jeden der fünf Sidecar-Proxies wurde ein dediziertes Server-Zertifikat generiert: \texttt{issuer.crt}, \texttt{holder.crt}, \texttt{verifier.crt}, \texttt{von-webserver.crt} und \texttt{tails-server.crt}. Jedes dieser Zertifikate wird mittels Certificate Signing Request (CSR) erstellt und von der Root CA mit ML-DSA-65 signiert. Die Zertifikate enthalten Subject Alternative Names (SAN), die sowohl den DNS-Namen \texttt{localhost} als auch die IP-Adresse \texttt{127.0.0.1} umfassen, um flexible Zugriffsmöglichkeiten während der Entwicklung zu ermöglichen. Die Zertifikatsgröße liegt bei durchschnittlich 9.500 Bytes pro Agent-Zertifikat, während die Root CA mit 10.312 Bytes aufgrund zusätzlicher Metadaten geringfügig größer ausfällt. Diese Größenordnung ist charakteristisch für ML-DSA-65-Signaturen, deren Signaturlänge 3.293 Bytes beträgt und damit signifikant über klassischen ECDSA-Signaturen (ca.\ 64--72 Bytes) liegt.

Die Integration der Zertifikate erfolgt über Docker-Volume-Mounts: Das Verzeichnis \texttt{hopE/pqc\_sidecarproxy\_nginx/certs/} wird als Read-Only-Volume in jeden Nginx-Container unter \texttt{/opt/nginx/certs/} eingebunden. In der Nginx-Konfiguration werden die Zertifikate durch die Direktiven \texttt{ssl\_certificate /opt/nginx/certs/<agent>.crt} und \texttt{ssl\_certificate\_key /opt/nginx/certs/<agent>.key} referenziert. Parallel dazu wird das Root-CA-Zertifikat in allen ACA-Py-Agent-Containern nach \texttt{/usr/local/share/ca-certificates/pqc-root-ca.crt} kopiert und mittels \texttt{update-ca-certificates} in den System-Trust-Store importiert. Zusätzlich werden die Umgebungsvariablen \texttt{SSL\_CERT\_FILE}, \texttt{REQUESTS\_CA\_BUNDLE} und \texttt{CURL\_CA\_BUNDLE} auf \texttt{/etc/ssl/certs/ca-certificates.crt} gesetzt, um sicherzustellen, dass Python-Anwendungen und HTTP-Clients die importierte Root CA verwenden und TLS-Verbindungen zu den PQC-Proxies erfolgreich validieren können.

\paragraph{Sidecar Proxy nginx}

Die Implementierung der post-quanten-kryptographischen Absicherung auf Transport-Layer-Ebene basiert zum einen auf einer modifizierten Version des NGINX-Dockerfiles von \textcite{open-quantum-safe_OpenquantumsafeOqsdemosNginxDockerfile_2025}, zum anderen auf spezifischen NGINX-Konfigurationsdateien für jeden Sidecar-Proxy.

\textbf{Dockerfile}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{2 Stage Sidecarproxy Dockerfile.png}
    \caption{Sidecar Proxy NGINX Dockerfile Multi-Stage Build}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Sidecar_Proxy_nginx_Dockerfile}
\end{figure}

\fixme{CHANGES IN ROT MARKIEREN INNERHALB DER GRAFIK}

Die \autoref{fig:Sidecar_Proxy_nginx_Dockerfile} visualisiert anschaulich den zweistufigen Aufbau des modifizierten Dockerfiles (Listing \ref{lst:Dockerfile-Sidecar-Proxy-nginx}) für eine post-quantenfähige NGINX-Sidecar-Proxy-Lösung, die auf die Integration von OpenSSL 3.5.4 und dem OQS Provider ausgerichtet ist. Das modifizierte Dockerfile folgt dabei dem Multi-Stage-Build-Prinzip, einer Optimierungsstrategie nach \textcite[S. 1]{rosa_MiningMeasuringImpactchangepatternsimprovingsizebuildtimedockerimages_2025}, und gliedert sich in die zwei zentrale Phasen \textbf{Build-Stage (Stage 1)} und \textbf{Runtime-Stage (Stage 2)}.

In der Build-Phase findet die gesamte Kompilierung und Zusammenführung der für den Betrieb erforderlichen Abhängigkeiten statt. Zunächst werden Versionsangaben für Bibliotheken, Algorithmen, TLS-Gruppen und Installationsverzeichnisse als Build-Parameter definiert. Anschließend werden die projektrelevanten Quellen wie \texttt{liboqs}, \texttt{oqs-provider}, \texttt{openssl}, \texttt{nginx} und \texttt{curl} gezielt und versioniert aus ihren jeweiligen Repositories geladen. Die Kompilierung erfolgt dabei in einer spezifischen Reihenfolge: OpenSSL und liboqs werden mit definierten Einstellungen gebaut, NGINX wird unter Einbindung des OQS Providers und einer hybrid gelinkten OpenSSL-Version kompiliert, und curl wird dynamisch mit OpenSSL gebaut, um HTTPS-Unterstützung sicherzustellen. Abschließend werden die generierten Binaries optimiert und nicht benötigte Dateien entfernt, um das finale Image so schlank wie möglich zu gestalten.

Die Runtime-Stage bildet daraufhin das minimalistische Endprodukt, das zum produktiven Einsatz bestimmt ist. Die in der Build-Phase erzeugten Binaries und Konfigurationen werden hier in das neue Runtime-Image übernommen. Die notwendigen Einstellungen für Post-Quantum-TLS-Gruppen und Logging werden über Umgebungsvariablen und angepasste Konfigurationen gesetzt. Das Image läuft nach dem Least-Privilege-Prinzip \parencite[S. 1]{khattar_DockerProEssentialPracticesSecureScalableContainers_2025} mit einem dedizierten Non-Root User (\texttt{oqs}), und der Container-Entrypoint ist klar über den Aufruf von nginx definiert.

Gegenüber dem Original-Dockerfile \parencite{open-quantum-safe_OpenquantumsafeOqsdemosNginxDockerfile_2025} wurden mehrere zielgerichtete Modifikationen vorgenommen. Die OpenSSL-Version wurde von variablen Tags auf die explizite Versionsnummer \texttt{3.5.4} festgelegt, da dieses Release als LTS mit aktuellen Sicherheitsfixes gilt. Die für den Key-Encapsulation-Mechanism zentralen TLS-Gruppen wurden auf \texttt{X25519MLKEM768}, \texttt{mlkem768}, \texttt{x25519} und \texttt{mlkem1024} reduziert, um gezielt ML-KEM-768 zu forcieren. Im Originalskript werden Zertifikate direkt während des Builds generiert; die modifizierte Version hingegen verzichtet darauf und sieht vor, dass Zertifikate via Volume von außen eingebracht werden, was dem Produktionsszenario entspricht. Die Build-Reihenfolge wurde neu strukturiert: OpenSSL wird zuerst als Shared Library kompiliert, um eine dynamische Verlinkung von curl und nginx zu ermöglichen. Dies umgeht auch Inkompatibilitäten zwischen Alpine Linux und statisch gelinkten OpenSSL-Versionen. Die Optimierung und Entfernung nicht benötigter Artefakte erfolgt granularer, wobei dynamische Binaries und Modul-Dateien wie \texttt{oqsprovider.so} gezielt gestreift werden. Das finale Runtime-Image wurde zusätzlich verschlankt: Es werden \texttt{ca-certificates} als Runtime-Abhängigkeit hinzugefügt, Ports werden explizit nicht mehr im Dockerfile exponiert (dies erfolgt stattdessen via Docker Compose oder Kubernetes), und der ENTRYPOINT ist strikt auf den Sidecar-Proxy-Einsatz ausgerichtet.


% Die primäre Modifikation betrifft den Wechsel von OpenSSL 3.4.0 auf OpenSSL 3.5.4 LTS, welche zum Zeitpunkt der Implementierung die aktuelle Long-Term-Support-Version mit essenziellen Sicherheitspatches darstellte und somit eine produktionsnahe Testumgebung ermöglichte. Diese Version gewährleistet Kompatibilität mit dem OQS-Provider 0.9.0 und liboqs 0.13.0, während gleichzeitig kritische CVE-Fixes berücksichtigt werden, die in früheren Versionen nicht verfügbar waren.

% Eine wesentliche Anpassung erfolgte in der Konfiguration der unterstützten Schlüsselaustausch-Mechanismen über die \texttt{DEFAULT\_GROUPS}-Variable. Während die OQS-Referenzimplementierung eine breite Palette klassischer und post-quanten-Algorithmen vorsieht (\texttt{x25519:x448:prime256v1:secp384r1:secp521r1:mlkem512:mlkem768:mlkem1024}), fokussiert die angepasste Implementierung auf die hybriden NIST-standardisierten Varianten mit der Priorisierung \texttt{X25519MLKEM768:mlkem768:x25519:mlkem1024}. Diese Auswahl basiert auf der Empfehlung des BSI für den Einsatz hybrider Verfahren im Übergangszeitraum zur post-quanten-Kryptographie sowie der NIST-Standardisierung von ML-KEM (ehemals CRYSTALS-Kyber) als primären Key Encapsulation Mechanism. Die hybride Variante X25519MLKEM768 kombiniert dabei die bewährte Elliptic-Curve-Diffie-Hellman-Variante X25519 mit ML-KEM-768 und bietet somit Schutz sowohl gegen klassische als auch gegen quantencomputerbasierte Angriffe.

% Die dritte substantielle Modifikation betrifft das Certificate-Management. Während die OQS-Referenzimplementierung zur Build-Zeit selbstsignierte Zertifikate mit ML-DSA-65-Signaturen generiert und diese im Image einbettet, wurde in der angepassten Version die Zertifikatsgenerierung vollständig entfernt und durch ein Volume-basiertes Mount-Verfahren ersetzt. Diese architektonische Entscheidung ermöglicht die Verwendung einer zentralen Public-Key-Infrastruktur (PKI) mit dedizierten Certificate Authorities sowie die Rotation von Zertifikaten ohne Neubildung des Container-Images. Die Zertifikate werden zur Laufzeit über Docker-Volumes unter dem Pfad \texttt{/opt/nginx/certs} bereitgestellt und durch die NGINX-Konfiguration referenziert. Dies gewährleistet Flexibilität in Multi-Agent-Deployments, bei denen unterschiedliche Agents individuelle Zertifikate mit spezifischen Subject Alternative Names (SANs) benötigen.

% Zusätzlich wurde die Build-Pipeline um cURL 8.11.1 erweitert, welches ebenfalls gegen die kompilierte OpenSSL-3.5.4-Instanz gelinkt wird. Diese Integration ermöglicht administrative Operationen und Debugging-Prozesse mit post-quanten-kryptographischen TLS-Verbindungen direkt aus dem Container heraus, ohne auf externe Tools mit potentiell inkompatiblen OpenSSL-Versionen angewiesen zu sein. Die Shared-Library-Verlinkung über RPATH-Einträge stellt sicher, dass sowohl NGINX als auch cURL zur Laufzeit die korrekte OpenSSL-Version mit aktiviertem OQS-Provider referenzieren, unabhängig von systemweiten OpenSSL-Installationen auf dem Host-System.


\textbf{nginx.conf}

Die NGINX-Konfigurationsdateien implementieren ein standardisiertes Sidecar-Proxy-Pattern für die post-quantensichere Verschlüsselung von Datenverkehr mittels TLS~1.3 mit Hybrid-Key-Exchange (\textit{X25519MLKEM768}). Eine Beispielkonfiguration für den Holder-Agenten-Sidecarproxy ist in Listing \ref{lst:nginx_holder.conf} dargestellt.

Strukturell folgen alle Konfigurationen einem konsistenten Schichtenmodell. Die globale Konfigurationsebene definiert Worker-Prozesse, Logging-Parameter sowie HTTP-Grundeinstellungen. Die mittlere Ebene besteht aus \textbf{Upstream-Blöcken}, die interne Services abstrahieren (\texttt{holder:8030}, \texttt{holder:8031}). Die oberste Ebene enthält \textbf{Server-Blöcke}, die für jeden öffentlich erreichbaren Endpoint eine HTTPS-Schnittstelle exponieren.

Das zentrale Sicherheitsmerkmal aller Konfigurationen ist die Direktive \texttt{ssl\_ecdh\_curve X25519MLKEM768}, die den Hybrid-Key-Exchange zwischen klassischer Elliptischen-Kurven-Kryptografie und dem post-quantensicheren ML-KEM-768-Algorithmus aktiviert. Die konkrete Gruppenauswahl wird extern über die Umgebungsvariable \texttt{DEFAULT\_GROUPS} gesteuert, was eine Trennung von Konfiguration und Deploymentslogik ermöglicht. Komplementär hierzu werden SSL-Zertifikate mit \textit{ML-DSA-65}-Signaturalgorithmus verwendet, die als Volume-Mounts unter \texttt{/opt/nginx/certs/} eingebunden werden. Dieser Ansatz gewährleistet, dass sowohl der Schlüsselaustausch als auch die Server-Authentifikation post-quantensicher erfolgen, während die Zertifikate unabhängig von der Container-Runtime verwaltet und rotiert werden können.

Die Reverse-Proxy-Funktionalität wird durch \texttt{location /}-Blöcke realisiert, die Anfragen an Upstream-Services weiterleiten. Alle Konfigurationen exponieren einen \texttt{/health}-Endpoint für Orchestrierungs-Health-Checks.

\paragraph{DLT-Infrastruktur}

Die Implementierung der Distributed-Ledger-Infrastruktur basiert auf dem \textit{von-network}-Repository \parencite{bcgov_GitHubBcgovVonnetworkportabledevelopmentlevelIndyNodenetwork_}, das eine portable, docker-basierte Hyperledger-Indy-Entwicklungsumgebung bereitstellt. Das von-network-Projekt wurde ursprünglich entwickelt, um Entwicklern und Organisationen eine sofort einsatzbereite Indy-Blockchain-Umgebung zur Verfügung zu stellen, ohne die Komplexität eines produktiven Sovrin-Netzwerks bewältigen zu müssen. Die Entscheidung für von-network als Basis-Infrastruktur begründet sich durch die vollständige Kapselung aller erforderlichen Komponenten -- Genesis-File-Generierung, Validator-Node-Orchestrierung und Ledger-Browser-Interface -- in einer konsistenten Docker-Compose-Konfiguration.

Die unmodifizierte von-network-Implementierung stellt vier Indy-Validator-Nodes bereit, die über das Practical Byzantine Fault Tolerance (PBFT) Konsensprotokoll synchronisiert werden. Jeder Node exponiert zwei Ports für die Indy-Protokollkommunikation (9701--9708) und verfügt über ein dediziertes Docker-Volume zur Persistierung des Ledger-Zustands. Zusätzlich zu den Validator-Nodes wird ein Webserver-Container deployt, der ein Web-Interface zur Ledger-Visualisierung sowie einen HTTP-Endpoint zur Genesis-File-Distribution bereitstellt. Diese Architektur ermöglicht es ACA-Py-Agents, beim Start die Genesis-Transaktionsdatei über eine konfigurierbare URL abzurufen und sich automatisch mit dem Indy-Netzwerk zu verbinden.

Für die Integration der Post-Quantum-Kryptografie auf Transport-Layer-Ebene wurde die von-network-Architektur um einen PQC Nginx Sidecar Proxy erweitert.
Diese Modifikation stellt die zentrale Anpassung gegenüber dem Original-Quellcode dar und betrifft primär die \texttt{docker-compose.yml}-Konfigurationsdatei (Listing \ref{lst:docker-compose.yml-DLT-Infrastruktur}) sowie die Hinzufügung eines neuen Verzeichnisses \texttt{pqc\_sidecarproxy\_nginx/}, das die \fixme{in Kapitel XY vorgestellten} Dockerfile- und Nginx-Konfigurationsdateien für den quantensicheren Reverse Proxy enthält. Der Webserver-Container, der im Original-Setup direkt auf Port 9000 exponiert wurde, verbleibt in der modifizierten Architektur ausschließlich im internen Docker-Netzwerk \texttt{von}. Stattdessen terminiert der neu hinzugefügte \texttt{pqc-sidecarproxy-webserver}-Container alle eingehenden TLS-1.3-Verbindungen auf Port 8000 und leitet die Anfragen nach erfolgreicher ML-KEM-768-basierter Schlüsselvereinbarung und ML-DSA-65-Zertifikatsverifikation als unverschlüsseltes HTTP an den internen Webserver-Container weiter.

Die Integration des Sidecar Proxies erforderte die Definition eines zusätzlichen, extern zugänglichen Docker-Netzwerks \texttt{von\_sidecarproxy}, das als gemeinsame Kommunikationsebene für alle PQC-Proxies der Gesamtarchitektur dient. Dieses Netzwerk wird in der \texttt{docker-compose.yml} als \texttt{external: true} deklariert. Der \texttt{pqc-sidecarproxy-webserver}-Container ist sowohl mit dem internen \texttt{von}-Netzwerk (für Backend-Kommunikation) als auch mit dem externen \texttt{von\_sidecarproxy}-Netzwerk (für Client-Zugriffe) verbunden, wodurch eine strikte Netzwerksegmentierung zwischen interner und externer Kommunikation gewährleistet wird.

Neben der Proxy-Integration wurden die Indy-Validator-Nodes und der Webserver-Container vollständig unverändert aus dem Original-Repository übernommen. Dies umfasst das Base-Image \texttt{ghcr.io/bcgov/von-image:node-1.12-6}, die Genesis-Generierungslogik in \texttt{scripts/init\_genesis.sh} sowie die Node-Startup-Scripts \texttt{scripts/start\_node.sh}. Die Beibehaltung des ursprünglichen Indy-Node-Codes gewährleistet, dass die blockchain-interne Validierung, Konsens-Mechanismen und Ledger-Operationen identisch mit etablierten Indy-Deployments funktionieren und ausschließlich die externe Kommunikationsschicht durch Post-Quantum-Kryptografie abgesichert wird.

Die Ledger-Struktur von Hyperledger Indy bleibt ebenfalls unverändert und umfasst drei logische Ledger-Typen: Der \textit{Domain Ledger} speichert DIDs, Schemas und Credential Definitions; der \textit{Pool Ledger} verwaltet die Validator-Node-Registry; der \textit{Config Ledger} enthält Netzwerk-Konfigurationen wie Transaction Author Agreements (TAA) und Acceptable Mechanism Lists (AML). Die Persistierung dieser Ledger erfolgt über Docker-Volumes (\texttt{node1-data} bis \texttt{node4-data}), die bei einem Neustart der Container den Ledger-Zustand wiederherstellen und somit eine konsistente Datenbasis über mehrere Entwicklungs- und Testzyklen hinweg gewährleisten.

% Zusammenfassend beschränkt sich die Modifikation des von-network-Projekts auf die Hinzufügung einer PQC-Transport-Layer-Sicherheitsebene, während die Core-Funktionalität der Indy-Blockchain-Infrastruktur vollständig erhalten bleibt. Diese minimalinvasive Anpassungsstrategie entspricht dem Sidecar-Pattern-Prinzip, das eine klare Separation zwischen kryptografischen Sicherheitsmechanismen (Proxy-Ebene) und Geschäftslogik (Blockchain-Ebene) etabliert. Die Wiederverwendbarkeit des etablierten von-network-Codes reduziert die Implementierungskomplexität und gewährleistet, dass die DLT-Infrastruktur auf bewährten, produktionserprobten Komponenten basiert, während die quantensichere Absicherung als modulare Erweiterungsschicht implementiert wird.

\paragraph{Revocation Registry}

Die Implementierung der Revocation-Registry-Infrastruktur basiert auf dem offiziellen \textit{indy-tails-server}-Repository \parencite{bcgov_GitHubBcgovIndytailsserverThissoftwarestoresmakesavailabletailsfilesuseHyperledger_}, das einen dedizierten File-Server für die Speicherung und Distribution von Revocation-Registry-Tails-Dateien bereitstellt. Im Kontext von AnonCreds -- dem Privacy-Preserving-Credential-Format von Hyperledger Indy -- werden Tails-Dateien benötigt, um Non-Revocation-Proofs zu generieren und zu verifizieren, ohne dabei die gesamte Revocation-Registry-Datenstruktur im Ledger zu speichern. Der Tails-Server fungiert als zentraler Storage-Service, auf den sowohl Credential-Issuer (zum Upload der Tails-Dateien) als auch Verifier (zum Download für Proof-Verifikation) zugreifen.

Die unmodifizierte indy-tails-server-Implementierung stellt eine Python-basierte REST-API bereit, die zwei primäre Operationen unterstützt: Das Hochladen von Tails-Dateien über HTTP-PUT-Requests auf die Endpoints \texttt{/hash/\{tails-hash\}} oder \texttt{/\{revoc\_reg\_id\}} sowie das Abrufen von Tails-Dateien über HTTP-GET-Requests. Die Server-Architektur implementiert eine Hash-basierte Integritätsvalidierung, bei der der SHA-256-Hash der hochgeladenen Datei mit dem in der URL spezifizierten Hash verglichen wird, um Dateikorruption oder Manipulation zu detektieren. Die Tails-Dateien werden im Dateisystem des Containers persistiert, wobei der Speicherpfad über die Umgebungsvariable \texttt{STORAGE\_PATH} konfigurierbar ist.

Für die Integration in die Post-Quantum-gesicherte Gesamtarchitektur wurde der indy-tails-server analog zur DLT-Infrastruktur um einen PQC Nginx Sidecar Proxy erweitert. Diese Modifikation betrifft primär die Docker-Compose-Konfigurationsdatei (Listing \ref{lst:docker-compose.yml-Revocation-Registry}), in der ein zusätzlicher Service \texttt{pqc-sidecarproxy-tails-server} definiert wurde, sowie die Hin-
zufügung eines neuen Verzeichnisses pqc\_sidecarproxy\_nginx/, das die \fixme{in Kapitel XY vorgestellten} Dockerfile- und Nginx-Konfigurationsdateien für den
quantensicheren Reverse Proxy enthält. Der ursprüngliche Tails-Server-Container verbleibt im internen Docker-Netzwerk \texttt{tails-server} und exponiert Port 6543 ausschließlich innerhalb dieses Netzwerks. Der neu hinzugefügte PQC-Proxy-Container terminiert alle externen TLS-1.3-Verbindungen auf Port 6543 und leitet die Anfragen nach erfolgreicher quantensicherer Authentifizierung (ML-DSA-65-Zertifikatsverifikation) und Schlüsselvereinbarung (ML-KEM-768) als unverschlüsseltes HTTP an den internen Tails-Server weiter.

Die Netzwerk-Integration folgt dem etablierten Sidecar-Pattern: Der \texttt{pqc-sidecarproxy-tails-server}-Container ist sowohl mit dem internen \texttt{tails-server}-Netzwerk (für Backend-Kommunikation) als auch mit dem externen, manuell erstellten \texttt{von\_sidecarproxy}-Netzwerk (für Client-Zugriffe) verbunden. Diese Dual-Network-Architektur erzwingt, dass alle externen Zugriffe auf den Tails-Server -- sowohl von ACA-Py-Issuer-Agents beim Upload als auch von Verifier-Agents beim Download -- über den quantensicheren Reverse Proxy geleitet werden.

Die funktionale Logik des Tails-Servers -- einschließlich File-Upload-Validierung, Hash-Verifikation und Storage-Management -- bleibt vollständig unverändert. Dies umfasst die Python-basierte REST-API-Implementierung, die Multi-Part-File-Upload-Handler sowie die Fehlerbehandlung für invalide Hashes oder fehlende Dateien. Die Beibehaltung des Original-Codes gewährleistet, dass die Revocation-Mechanismen kompatibel mit Standard-AnonCreds-Implementierungen bleiben und ausschließlich die Transport-Layer-Sicherheit durch Post-Quantum-Kryptografie erweitert wird.

% Ein wesentlicher Aspekt der Tails-Server-Integration ist die Performance-Charakteristik in Abhängigkeit von der Revocation-Registry-Größe. Bei der Erstellung einer Credential Definition mit aktivierter Revocation wird ein Parameter \texttt{revocation\_registry\_size} spezifiziert, der die maximale Anzahl widerrufbarer Credentials festlegt. Die Größe der resultierenden Tails-Datei korreliert linear mit der Registry-Größe: Eine Registry mit 3\,000 Einträgen erzeugt eine 768\,KB-Datei, während 32\,768 Einträge eine 8,4\,MB-Datei generieren. Diese Dateien müssen von Verifier-Agents während der Proof-Verifikation heruntergeladen werden, wobei der PQC-Proxy-Overhead durch ML-KEM-768-Schlüsselaustausch die Transfer-Latenz geringfügig erhöht. Empirische Tests zeigten, dass die zusätzliche TLS-1.3-Handshake-Latenz mit ML-KEM-768 im Vergleich zu klassischem X25519 durchschnittlich 15--25\,ms beträgt, was bei typischen Tails-File-Transfer-Zeiten von mehreren Sekunden vernachlässigbar ist.

Die Docker-Compose-Orchestrierung des Tails-Servers definiert ein persistentes Volume \texttt{tails-files}, das den \texttt{STORAGE\_PATH}-Verzeichnis-Mount bereitstellt. Dieses Volume gewährleistet, dass hochgeladene Tails-Dateien über Container-Neustarts hinweg erhalten bleiben, was insbesondere in Entwicklungsumgebungen relevant ist, in denen Credential Definitions mit Revocation Registry wiederverwendet werden sollen. Die Health-Check-Konfiguration des Proxy-Containers überwacht die Verfügbarkeit des HTTPS-Endpoints und stellt sicher, dass nachgelagerte Services (ACA-Py-Agents) erst starten, wenn die Tails-Server-Infrastruktur vollständig einsatzbereit ist.

% Zusammenfassend beschränkt sich die Modifikation des indy-tails-server-Projekts auf die Integration einer quantensicheren Transport-Layer-Sicherheitsebene, während die Core-Funktionalität des Revocation-Registry-File-Servers vollständig erhalten bleibt. Diese Anpassungsstrategie entspricht dem übergeordneten Architekturprinzip, bestehende, produktionserprobte Hyperledger-Komponenten mit minimalen Eingriffen zu erweitern und die Post-Quantum-Kryptografie als modulare, nicht-invasive Sicherheitsschicht zu implementieren. Die resultierende Architektur gewährleistet, dass alle Tails-File-Transfers -- sowohl Upload durch Issuer als auch Download durch Verifier -- über ML-KEM-768-gesicherte TLS-1.3-Verbindungen erfolgen und somit resistent gegen zukünftige Angriffe durch Quantencomputer sind.

\paragraph{SSI-Agenten} \label{SSI-Agenten}

Die Implementierung der Self-Sovereign-Identity-Agenten basiert auf Hyperledger Aries Cloud Agent Python (ACA-Py) \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_}, der offiziellen Referenzimplementierung des Aries Interop Profile (AIP) 2.0. ACA-Py stellt eine vollständige SSI-Agent-Infrastruktur bereit, die alle erforderlichen Protokolle für DID-basierte Verbindungen, Credential Issuance, Presentation Exchange und Revocation Management implementiert. Die Architektur folgt dem Controller-Pattern, bei dem der Agent als eigenständiger Service agiert und über eine REST-API (Admin API) von externen Controller-Anwendungen gesteuert wird.

In der vorliegenden Iteration-1-Implementierung wurden drei ACA-Py-Agent-Instanzen deployt, die die klassischen SSI-Rollen abbilden: Ein \textit{Issuer Agent}, der Credentials ausstellt; ein \textit{Holder Agent}, der Credentials empfängt und speichert; sowie ein \textit{Verifier Agent}, der Proof-Requests erstellt und Presentations verifiziert \fixme{folgt theorie kapitel XY}. Alle drei Agents verwenden das unmodifizierte ACA-Py-Base-Image (Listing \ref{lst:Dockerfile-acapy-base}) aus dem offiziellen Hyperledger-Repository und werden ausschließlich über Kommandozeilen-Parameter konfiguriert, ohne Änderungen am ACA-Py-Quellcode vorzunehmen.

Listing \ref{lst:docker-compose.yml-SSI-Agenten} zeigt die Docker-Compose-Konfiguration der drei ACA-Py-Agenten innerhalb der Gesamtarchitektur. Die Agent-Konfiguration erfolgt hierbei vollständig deklarativ über Docker-Compose-Service-Definitionen, die jeweils den \texttt{start}-Befehl von ACA-Py mit rollenspezifischen Parametern aufrufen. Zentrale Konfigurationsparameter umfassen \texttt{--label} (zur Identifikation des Agents), \texttt{--inbound-transport http 0.0.0.0 <port>} (zur Definition des DIDComm-Message-Endpoints), \texttt{--outbound-transport http} (für ausgehende Verbindungen), \texttt{--admin 0.0.0.0 <port>} (zur Aktivierung der Admin-API) sowie \texttt{--wallet-type askar} (zur Spezifikation des Wallet-Backends). Alle Agents verwenden Aries Askar als Wallet-Implementierung, das eine moderne, in Rust implementierte Wallet-Architektur mit ChaCha20-Poly1305-Verschlüsselung und Argon2id-Key-Derivation bereitstellt.

Ein kritischer Konfigurationsparameter ist \texttt{--endpoint https://host.docker.internal:<port>}, der spezifiziert, unter welcher URL der Agent für eingehende DIDComm-Verbindungen erreichbar ist. Dieser Parameter wird in Out-of-Band-Invitations und DID-Exchange-Nachrichten eingebettet und muss auf den externen PQC-Proxy-Endpoint zeigen, nicht auf den internen Agent-Container. Beispielsweise konfiguriert der Issuer-Agent \texttt{--endpoint https://host.docker.internal:8020}, wodurch andere Agents ihre DIDComm-Nachrichten an den PQC-Proxy auf Port 8020 senden, der diese nach TLS-Terminierung an den internen Issuer-Container weiterleitet. Diese Indirektion ist essentiell, um sicherzustellen, dass alle Agent-zu-Agent-Verbindungen die Post-Quantum-gesicherte Transport-Layer-Sicherheit nutzen.

Die Wallet-Konfiguration erfolgt über die Parameter \texttt{--wallet-name <name>}, \texttt{--wallet-key <key>} und \texttt{--auto-provision}, wobei letzterer sicherstellt, dass das Wallet automatisch beim ersten Start initialisiert wird. Jedes Wallet wird in einem dedizierten Docker-Volume persistiert (\texttt{issuer-data}, \texttt{holder-data}, \texttt{verifier-data}), sodass DID-Keypairs, Credentials und Connections über Container-Neustarts hinweg erhalten bleiben. Die Wallet-Verschlüsselung mittels ChaCha20-Poly1305 gewährleistet, dass gespeicherte kryptografische Schlüssel selbst bei Kompromittierung des Host-Filesystems nicht ohne den Wallet-Key extrahiert werden können.

Ein wesentlicher Aspekt der Agent-Konfiguration ist die Anbindung an die DLT-Infrastruktur über den Parameter \texttt{--genesis-url https://host.docker.internal:8000/genesis}. Diese URL referenziert den PQC-gesicherten Webserver der DLT-Infrastruktur und ermöglicht es den Agents, beim Start die Genesis-Transaktionsdatei über eine quantensichere TLS-1.3-Verbindung abzurufen. Analog dazu wird die Verbindung zur Revocation Registry über \texttt{--tails-server-base-url https://host.docker.internal:6543} konfiguriert, wobei ebenfalls der PQC Nginx Sidecar Proxy als Endpoint fungiert. Die Verwendung von \texttt{host.docker.internal} ermöglicht dabei die Adressierung des Docker-Hosts aus Container-Perspektive und abstrahiert plattformspezifische Netzwerk-Unterschiede zwischen Linux, macOS und Windows.

Zusätzlich zu den grundlegenden Konfigurations-Parametern aktivieren die Agents mehrere Auto-Response-Features, die für Entwicklungs- und Testzwecke die manuelle Interaktion reduzieren: \texttt{--auto-accept-invites} (automatisches Akzeptieren von Connection-Invitations), \texttt{--auto-respond-credential-proposal}, \texttt{--auto-respond-credential-offer} und \texttt{--auto-respond-credential-request} (automatische Credential-Exchange-Antworten beim Issuer), \texttt{--auto-store-credential} (automatisches Speichern empfangener Credentials beim Holder) sowie \texttt{--auto-verify-presentation} (automatische Presentation-Verifikation beim Verifier). Diese Automatisierungen ermöglichen vollständig scriptgesteuerte SSI-Workflows über die Admin-API, wie sie in Jupyter-Notebook-basierten Demonstrationen implementiert sind.

Für die Post-Quantum-Absicherung der Agent-Kommunikation wurde analog zur DLT-Infrastruktur und Revocation Registry ein dedizierter PQC Nginx Sidecar Proxy pro Agent implementiert. Die Docker-Compose-Konfiguration definiert drei zusätzliche Services -- \texttt{pqc-sidecarproxy-issuer}, \texttt{pqc-sidecarproxy-holder} und \texttt{pqc-sidecarproxy-verifier} --, die jeweils als Reverse Proxy vor dem zugehörigen Agent platziert werden. Jeder Agent-Container exponiert zwei interne HTTP-Ports: einen für Inbound-Transport (8020, 8030, 8040) und einen für die Admin-API (8021, 8031, 8041). Diese Ports sind ausschließlich im agent-spezifischen internen Docker-Netzwerk (\texttt{hope-issuer}, \texttt{hope-holder}, \texttt{hope-verifier}) zugänglich.

Die Sidecar Proxies terminieren alle eingehenden TLS-1.3-Verbindungen und leiten die Anfragen nach erfolgreicher quantensicherer Authentifizierung und Schlüsselvereinbarung als unverschlüsseltes HTTP an die internen Agent-Ports weiter. Jeder Proxy ist mit zwei Docker-Netzwerken verbunden: dem agent-spezifischen internen Netzwerk für Backend-Kommunikation sowie dem gemeinsamen externen \texttt{von\_sidecarproxy}-Netzwerk für Client-Zugriffe. Diese Dual-Network-Architektur erzwingt, dass sämtliche externe Kommunikation -- einschließlich DIDComm-Nachrichten zwischen Agents, Admin-API-Zugriffe durch Controller-Anwendungen sowie Ledger- und Tails-Server-Requests -- über quantensichere TLS-Verbindungen erfolgt.

Die Netzwerk-Architektur folgt einem strikten Isolation-Prinzip: Jeder Agent residiert in einem dedizierten internen Docker-Netzwerk, das ausschließlich den Agent-Container und den zugehörigen PQC-Proxy umfasst. Die Agents können untereinander nicht direkt kommunizieren, sondern ausschließlich über das externe \texttt{von\_sidecarproxy}-Netzwerk, das die PQC-Proxies verbindet. Diese Segmentierung erzeugt eine Defense-in-Depth-Architektur, bei der selbst bei einer Kompromittierung eines Agent-Containers der Zugriff auf andere Agents durch Netzwerk-Isolation verhindert wird.

Die Health-Check-Konfiguration der Agent-Container überwacht die Verfügbarkeit der Admin-API mittels periodischer HTTP-Requests an \texttt{http://localhost:<admin-port>/status/ready}. Zusätzlich definieren die Service-Dependencies (\texttt{depends\_on}) eine Startup-Reihenfolge, die sicherstellt, dass die PQC-Proxies vor den Agents starten und dass die Infrastruktur-Services (von-network, Tails-Server) vollständig initialisiert sind, bevor die Agents ihre Genesis-Datei abrufen. Diese Orchestrierung eliminiert Race-Conditions während des Deployment-Prozesses und gewährleistet eine deterministische Startup-Sequenz.

%Zusammenfassend basiert die SSI-Agent-Implementierung auf vollständig unmodifizierten ACA-Py-Komponenten, wobei die Post-Quantum-Kryptografie ausschließlich über externe Nginx-Sidecar-Proxies integriert wird. Diese nicht-invasive Architektur gewährleistet, dass die Agents mit Standard-Aries-Protokollen und -Workflows kompatibel bleiben, während alle externen Kommunikationskanäle durch ML-KEM-768-basierte Schlüsselvereinbarung und ML-DSA-65-Zertifikatsauthentifizierung quantensicher abgesichert sind. Die strikte Netzwerk-Segmentierung und das Sidecar-Pattern etablieren eine klare Separation zwischen kryptografischer Transport-Sicherheit und SSI-Geschäftslogik, was die Grundlage für nachfolgende Iterationen bildet, in denen PQC schrittweise auf die Application-Layer-Ebene erweitert wird.

\paragraph{Docker Orchestrierung der Gesamtarchitektur} \label{Docker Orchestrierung der Gesamtarchitektur}

\fixme{manage scripts im Anhang hinterlegen?}

Die in den vorangegangenen Abschnitten beschriebenen Einzelkomponenten -- Zertifikatsinfrastruktur, PQC-Sidecar-Proxies, DLT-Infrastruktur, Revocation Registry und SSI-Agenten -- werden mittels einer mehrstufigen Docker-Compose-Orchestrierung zu einem funktionsfähigen Gesamtsystem integriert. Der Startprozess der Gesamtarchitektur folgt einer deterministischen Sequenz, die in Listing~\ref{lst:Docker-Compose-Start-der-Gesamtarchitektur} dokumentiert ist und die korrekten Abhängigkeiten zwischen den Infrastrukturschichten gewährleistet.

Die Orchestrierung gliedert sich in drei sequenzielle Phasen, die jeweils durch separate Docker-Compose-Konfigurationen gesteuert werden. In der ersten Phase wird die DLT-Infrastruktur über das \texttt{von-network}-Management-Skript initialisiert, wodurch die vier Indy-Validator-Nodes, der Genesis-Webserver sowie der zugehörige PQC-Sidecar-Proxy gestartet werden. Diese Phase erzeugt das gemeinsame externe Docker-Netzwerk \texttt{von\_sidecarproxy}, das als zentrale Kommunikationsschicht für alle quantensicheren Verbindungen dient. Die zweite Phase umfasst die Initialisierung der Revocation Registry mittels des \texttt{indy-tails-server}-Management-Skripts, wodurch der Tails-Server-Container sowie dessen PQC-Proxy-Frontend bereitgestellt werden. In der dritten Phase werden schließlich die SSI-Agenten -- Issuer, Holder und Verifier -- gemeinsam mit ihren jeweiligen PQC-Sidecar-Proxies über die projektspezifische Docker-Compose-Konfiguration gestartet.

\refstepcounter{manualListingCounter}
\label{lst:Docker-Compose-Start-der-Gesamtarchitektur}
\begin{lstlisting}[language=bash, caption={Docker Compose Start der Gesamtarchitektur}, numbers=left, frame=single]
(.venv) ferris@blockchain-ssi-pqc:~/github/MSc-blockchain-ssi-pqc$ ./von-network/manage start && ./indy-tails-server/docker/manage start && docker compose -f ./hopE/docker-compose.yml up -d
[+] Running 15/15
Network von_von                                 Created     0.0s 
Network von_sidecarproxy                        Created     0.0s 
Volume "von_webserver-ledger"                   Created     0.0s 
Volume "von_node1-data"                         Created     0.0s 
Volume "von_node3-data"                         Created     0.0s 
Volume "von_node2-data"                         Created     0.0s 
Volume "von_node4-data"                         Created     0.0s 
Volume "von_nginx-logs"                         Created     0.0s 
Volume "von_webserver-cli"                      Created     0.0s 
Container von-node4                             Started     0.6s 
Container von-webserver                         Started     0.4s 
Container von-node3                             Started     0.4s 
Container von-node2                             Started     0.5s 
Container von-node1                             Started     0.5s 
Container von-pqc-sidecarproxy-webserver        Started     0.6s 
Want to see the scrolling container logs? Run "./manage logs"
[+] Running 4/4
Network docker_tails-server                     Created     0.0s 
Volume "docker_nginx-logs"                      Created     0.0s 
Container docker-tails-server-1                 Started     0.3s 
Container pqc-sidecarproxy-tails-server         Started     0.5s 
Run './manage logs' for logs
WARN[0000] /home/ferris/github/MSc-blockchain-ssi-pqc/hopE/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 13/13
Network hope_hope-issuer                        Created     0.0s 
Network hope_hope-holder                        Created     0.0s 
Network hope_hope-verifier                      Created     0.0s 
Volume "hope_issuer-data"                       Created     0.0s 
Volume "hope_holder-data"                       Created     0.0s 
Volume "hope_verifier-data"                     Created     0.0s 
Volume "hope_nginx-logs"                        Created     0.0s 
Container issuer-agent                          Started     0.6s 
Container holder-agent                          Started     0.5s 
Container verifier-agent                        Started     0.5s 
Container pqc-sidecarproxy-holder               Started     1.3s 
Container pqc-sidecarproxy-issuer               Started     1.0s 
Container pqc-sidecarproxy-verifier             Started     1.2s
\end{lstlisting}


Die in \autoref{fig:Docker-Compose-Übersicht-Iteration-1} visualisierte Containerarchitektur verdeutlicht die resultierende Systemtopologie. Die Gesamtarchitektur umfasst insgesamt 14 Container, die sich auf die drei funktionalen Schichten verteilen: Die DLT-Schicht besteht aus sechs Containern -- vier Validator-Nodes, einem Webserver und einem PQC-Proxy. Die Revocation-Schicht umfasst zwei Container für den Tails-Server und dessen PQC-Proxy. Die Agent-Schicht besteht aus sechs Containern für die drei SSI-Agenten und ihre jeweiligen PQC-Proxies.

% Die Netzwerksegmentierung folgt einem strikten Isolation-Prinzip. Jede funktionale Einheit -- DLT-Infrastruktur, Revocation Registry sowie jeder einzelne SSI-Agent -- residiert in einem dedizierten internen Docker-Netzwerk, das ausschließlich den jeweiligen Service-Container und den zugehörigen PQC-Proxy umfasst. Die einzige Verbindung zwischen diesen isolierten Netzwerken erfolgt über das externe \texttt{von\_sidecarproxy}-Netzwerk, an das alle PQC-Proxies angebunden sind. Diese Architektur erzwingt, dass sämtliche inter-service Kommunikation -- einschließlich DIDComm-Nachrichten zwischen Agenten, Admin-API-Zugriffe, Genesis-File-Abrufe und Tails-Server-Requests -- ausschließlich über die quantensicheren TLS-1.3-Verbindungen mit ML-KEM-768-Schlüsselvereinbarung und ML-DSA-65-Zertifikatsauthentifizierung erfolgt.

Die \texttt{depends\_on}-Direktiven in den Docker-Compose-Konfigurationen definieren explizite Startup-Abhängigkeiten, die Race-Conditions während des Deployment-Prozesses eliminieren. Die PQC-Proxies werden vor den ihnen zugeordneten Backend-Services gestartet, und die SSI-Agenten warten auf die vollständige Initialisierung der Infrastruktur-Services, bevor sie ihre Genesis-Transaktionsdatei abrufen. Diese Orchestrierung gewährleistet eine deterministische Startup-Sequenz und stellt sicher, dass alle Komponenten beim Erreichen ihres operativen Zustands auf vollständig verfügbare Abhängigkeiten zugreifen können.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{docker_compose_übersicht.png}
    \caption{Docker-Compose-Übersicht der Iteration 1 Architektur}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Docker-Compose-Übersicht-Iteration-1}
\end{figure}

\subsubsection{Formative Evaluation}

Die formative Evaluation dieser Iteration orientiert sich an der \textit{Technical Risk \& Efficacy Strategy} des FEDS-Frameworks nach \textcite[S. 4--5]{venable_FEDSFrameworkEvaluationDesignScienceResearch_2016} und erfolgt unter einem künstlichen Evaluationsparadigma in der Laborumgebung. Ziel ist die Validierung der technischen Funktionsfähigkeit der implementierten Komponenten als Voraussetzung für die nachfolgende Iteration, nicht jedoch die vollständige Prüfung gegen die in Kapitel~\ref{sec:Anforderungsanalyse} definierten Anforderungen -- diese erfolgt im Rahmen der summativen Evaluation in Kapitel~\ref{sec:Summative Evaluation}.

Für die formative Evaluation während der ersten Iteration war die Bereitstellung eines PQC-fähigen Browsers zwingend erforderlich, da aktuelle Produktivbrowser keine Post-Quanten-Kryptographie in ihren TLS-Implementierungen unterstützen. Diese Limitation führt bei Verbindungsversuchen zu PQC-fähigen Servern zu einem Cipher-Mismatch, wie in \autoref{fig:Cipher-Mismatch-Blockchain-Webserver} veranschaulicht. Um diese Inkompatibilität zu überwinden, wurde ein Chromium-basierter Browser mit integrierter PQC-Unterstützung kompiliert (\ref{Eigenkompilation eines Chromium-Browsers mit PQC-Unterstützung}). Dieser selbstkompilierte Browser ermöglicht die Durchführung von TLS-Handshakes mit hybriden und rein PQ-basierten Algorithmen und dient als fundamentale Testplattform für die experimentelle Analyse von PQC-Verfahren im Kontext realer Webanwendungen.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_blockchain_webserver_CIPHER_MISMATCH.png}
    \caption{Cipher Mismatch bei Validierung der TLS-1.3-Verbindung des Blockchain-Webservers}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Cipher-Mismatch-Blockchain-Webserver}
\end{figure}

\paragraph{Validierung der Zertifikatskette und ML-DSA-Signaturen}

Zur Verifikation der kryptographischen Integrität der implementierten Public-Key-Infrastruktur wurde die Zertifikatskette der Sidecar-Proxies mittels \texttt{openssl}-Diagnosewerkzeugen analysiert. Ziel war der Nachweis, dass die ausgelieferten X.509-Zertifikate korrekt auf den spezifizierten Post-Quanten-Signaturalgorithmen basieren. Die Inspektion des vom Issuer-Agenten-Proxy bereitgestellten Zertifikats (\autoref{fig:Successful-Validation-Issuer-MLDSA-Cert}) bestätigt, dass der öffentliche Schlüssel des Leaf-Zertifikats (\textit{pqc reverseproxy issuer agent}) den Algorithmus \texttt{ML-DSA-65} verwendet. Des Weiteren belegt der Signaturalgorithmus \texttt{ML-DSA-87}, dass die Zertifikatskette valide durch die \textit{Master Thesis PQC Root CA} signiert wurde, was die erfolgreiche Generierung und Einbindung der Dilithium-basierten Zertifikate in den TLS-Handshake beweist.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_issuer_mldsa_cert.png}
    \caption{Erfolgreiche Validierung des ML-DSA-Zertifikats des Issuer-Agenten}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Issuer-MLDSA-Cert}
\end{figure}

\paragraph{Validierung der TLS 1.3 Algorithmen-Aushandlung}

Die erfolgreiche Integration der PQC-Algorithmen in das Transportprotokoll wurde durch einen Verbindungsaufbau mittels \texttt{openssl s\_client} verifiziert. Wie in \autoref{fig:Successful-Validation-Issuer-TLS1.3} dargestellt, konnte erfolgreich eine TLS-1.3-Sitzung etabliert werden. Die Analyse der Handshake-Parameter bestätigt die Verwendung der hybrid-post-quanten Schlüsselaustauschgruppe \texttt{X25519MLKEM768}, welche den klassischen elliptischen Kurvenalgorithmus X25519 mit dem KEM-Verfahren ML-KEM-768 kombiniert. Zudem wird für die Authentifizierung des Peer-Zertifikats der Signaturalgorithmus \texttt{mldsa65} (Dilithium) ausgewiesen. Diese Ergebnisse validieren die korrekte Konfiguration der OQS-Provider-Bibliothek innerhalb der Proxy-Komponenten und belegen die praktische Funktionsfähigkeit des hybriden Schlüsselaustauschs im Zusammenspiel mit PQC-Signaturen.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_issuer_TLS1.3.png}
    \caption{Erfolgreiche Validierung der TLS-1.3-Verbindung des Issuer-Agenten}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Issuer-TLS1.3}
\end{figure}

\paragraph{Validierung der Ledger-Initialisierung}

Die operative Funktionsfähigkeit des Hyperledger Indy Netzwerks wurde primär über das Web-Interface des Blockchain-Servers validiert. Wie in \autoref{fig:Successful-Validation-Blockchain-Webserver} dargestellt, zeigen die Statusindikatoren aller vier Validator-Nodes eine aktive Beteiligung am Konsensus-Protokoll (Status \textit{Node1--4}), womit der Distributed Ledger erfolgreich initialisiert ist. Simultan belegt diese Abbildung die korrekte PQC-Absicherung der Webserver-Komponente: Der Zugriff erfolgt über den eigens kompilierten PQC-Chromium-Browser, dessen Security-Panel explizit eine authentifizierte TLS-1.3-Verbindung unter Verwendung der hybriden Schlüsselaustauschgruppe \texttt{X25519MLKEM768} ausweist.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_blockchain_webserver.png}
    \caption{Erfolgreiche Validierung der TLS-1.3-Verbindung des Blockchain-Webservers}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Blockchain-Webserver}
\end{figure}

Als zweite notwendige Bedingung für die spätere Anbindung der SSI-Agenten (ACA-Py) wurde die Verfügbarkeit der Genesis-Datei verifiziert. \autoref{fig:Successful-Validation-Genesis-File-Blockchain-Webserver} dokumentiert den Abruf des \texttt{/genesis}-Endpunkts mittels \texttt{curl}. Die erfolgreiche Rückgabe der JSON-formatierten Genesis-Transaktionen bestätigt, dass die für das Bootstrapping externer Clients erforderlichen Netzwerkinformationen korrekt publiziert werden.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_genesis.png}
    \caption{Erfolgreiche Validierung der Genesis-Datei des Blockchain-Webservers}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Genesis-File-Blockchain-Webserver}
\end{figure}

\paragraph{Validierung der ACA-Py API-Verfügbarkeit}

Die funktionale Erreichbarkeit der SSI-Agenten wurde durch eine systematische Analyse der Initialisierungsphase und der anschließenden API-Verfügbarkeit validiert. Die Logging-Ausgabe des Issuer-Agenten (Listing~\ref{lst:Issuer-Agent-Boot-Logs}) dokumentiert die erfolgreiche Genesis-Datei-Abrufung über \texttt{https://host.docker.internal:8000/genesis} (Zeile 3) und die vollständige Ledger-Konfiguration (Zeile 8). Die Erstellung eines neuen Wallet-Profils mit Askar-Backend (Zeile 5) und die erfolgreiche Initialisierung der Inbound- und Outbound-Transports (Zeile 10--26) demonstrieren die korrekte Konfiguration des ACA-Py-Agents. Die durchgeführten Health-Checks über den \texttt{/status/ready}-Endpunkt (Zeile 51--52) bestätigen die vollständige Initialisierung und Bereitschaft des Agenten.

Die Visualisierung der Swagger-basierten Admin-Oberfläche (\autoref{fig:Successful-Validation-Issuer-Agent-ACA-Py-Swagger-API}) ergänzt diese technischen Log-Daten durch den Nachweis, dass die Admin-API über den PQC-Reverse-Proxy fehlerfrei erreichbar ist und alle administrativen Endpunkte zur Steuerung der Agenten-Komponente bereitstellt. Die Tatsache, dass die Swagger-Oberfläche unter dem PQC-gesicherten HTTPS-Endpoint vollständig funktionsfähig ist, belegt die korrekte TLS-Terminierung am Proxy sowie die fehlerfreie Weiterleitung der HTTP-Anfragen an den ACA-Py-Container.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_Issuer_Agent_Swagger_API.png}
    \caption{Erfolgreiche Validierung der Issuer Agent ACA-Py Swagger Admin API}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Issuer-Agent-ACA-Py-Swagger-API}
\end{figure}

\paragraph{Validierung der Netzwerkisolation}

Die integrale Sicherheitseigenschaft der Netzwerksegmentierung wurde durch eine Inspektion der Docker-Netzwerktopologie und systematische Erreichbarkeitstests validiert. Die Architektur implementiert ein striktes Micro-Segmentation-Konzept, bei dem jeder SSI-Akteur in einem dedizierten, isolierten Subnetz operiert. \autoref{fig:Darstellung-Network-Isolation} belegt diese Topologie anhand des \texttt{docker network inspect}-Outputs: Der Issuer-Agent befindet sich exklusiv im Netzwerksegment \texttt{hope\_hope-issuer}, während der Holder-Agent im Segment \texttt{hope\_hope-holder} isoliert ist. In jedem dieser Segmente fungiert der zugehörige PQC-Sidecar-Proxy als einziger Ingress-Punkt.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_network_isolation.png}
    \caption{Darstellung der Netzwerkisolation innerhalb der Gesamtarchitektur}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Darstellung-Network-Isolation}
\end{figure}

\autoref{fig:Successful-Validation-Network-Isolation-Through-Tests} demonstriert die Wirksamkeit dieser Isolation auf zwei Ebenen. Erstens zeigt die Prozessliste (\texttt{docker ps}), dass lediglich die Sidecar-Proxies externe Ports (z.\,B. 8020, 8030, 8040) an das Host-System binden, während die Ports der ACA-Py-Container (z.\,B. \texttt{issuer-agent}) nicht exponiert sind. Zweitens beweisen die Inter-Container-Verbindungstests die logische Trennung: Ein direkter Zugriffsversuch aus dem \texttt{issuer-agent}-Container auf den \texttt{holder-agent} schlägt mit einem DNS-Auflösungsfehler (\textit{Could not resolve host}) fehl, da keine Routing-Route zwischen den isolierten Netzwerkbrücken existiert. Im Gegensatz dazu ist der lokale Zugriff des \texttt{pqc-sidecarproxy-holder} auf seinen zugehörigen Agenten erfolgreich möglich. Diese Konfiguration erzwingt, dass jegliche Kommunikation zwischen den Akteuren das Host-Netzwerk passieren muss, wo sie durch die in den Proxies terminierte Post-Quanten-Kryptographie (TLS 1.3 mit ML-KEM) abgesichert wird.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{validate_network_isolation_through_tests.png}
    \caption{Erfolgreiche Validierung der Netzwerkisolation innerhalb der Gesamtarchitektur}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Successful-Validation-Network-Isolation-Through-Tests}
\end{figure}

\subsubsection{Erkenntnisse und Anpassungsbedarfe}

Die erste Iteration bildet das fundamentale technologische Fundament der Forschungsarbeit. Die formative Evaluation (Kapitel \ref{sec:Formative-Evaluation-I1}) validierte die operative Integrität der entwickelten Architektur: Die verteilten Micro-Services, der Hyperledger Indy Ledger und die PQC-Sidecar-Proxies interagieren funktional korrekt. Diese Initialphase generierte jedoch spezifische Erkenntnisse, die eine gezielte Weiterentwicklung in der zweiten Iteration motivieren. Diese werden nachfolgend in Bezug auf die Designziele und die Funktionsmerkmale (FF) analysiert.

\paragraph{Abgleich mit den Designzielen der Iteration 1}

Das primäre Designziel, die Absicherung der Transportebene in einem SSI-Ökosystem mittels Post-Quanten-Kryptographie, wurde vollständig erreicht. Die erfolgreiche Validierung des Sidecar-Musters belegt die Machbarkeit einer transparenten PQC-Migration für Legacy-Systeme (ACA-Py, Indy Node) ohne Eingriffe in deren Kerncode. Die implementierte Micro-Segmentation erfüllt zudem die architektonischen Anforderungen an Netzwerkisolation und Angriffsflächenminimierung in KRITIS-Umgebungen.

\paragraph{FF1: Funktionale SSI-Kernprozesse \& Systemarchitektur}

Die implementierte Sidecar-Proxy-Architektur erwies sich als effektives Design-Pattern zur Realisierung einer \emph{Separation of Concerns} im KRITIS-Kontext. Durch die strikte Entkopplung der kryptographischen Terminierung (Proxy) von der Business-Logik (SSI-Agent) konnte eine PQC-Integration realisiert werden, die die Kernprozesse der Identitätsverwaltung funktional nicht beeinträchtigt. Diese architektonische Entscheidung ermöglicht es, sicherheitskritische Updates an der Krypto-Komponente vorzunehmen, ohne die Integrität der komplexen SSI-Logik zu gefährden.

\paragraph{FF2: Sicherheit, Compliance \& Algorithmenwahl}

Die Implementierung von ML-KEM-768 und ML-DSA-65 bestätigte die technische Reife dieser Algorithmen für den produktiven Einsatz. Die durchgeführten Tests validierten die Interoperabilität von hybriden Zertifikatsketten. Eine zentrale Erkenntnis ist jedoch, dass Sicherheit im KRITIS-Kontext über die Transportebene hinausgehen muss. Die aktuelle Transportverschlüsselung schützt zwar Daten "in Transit", bietet aber keine Ende-zu-Ende-Integrität auf Anwendungsebene (z.B. für VCs im Ruhezustand). Eine Erweiterung des Sicherheitsmodells auf die Applikationsschicht ist daher für Defense-in-Depth unerlässlich.

\paragraph{FF3: Kryptografische Agilität}

Entgegen der Annahme, dass statische Proxy-Konfigurationen starr sind, offenbarten die Tests, dass die gewählte Container-Architektur in Kombination mit TLS 1.3 ein hohes Maß an \textit{operativer Krypto-Agilität} bietet.
Erstens ermöglicht die Containerisierung (Docker) den Austausch kryptografischer Bibliotheken durch einfache Image-Updates. Ein Algorithmenwechsel erfordert lediglich den Austausch des Proxy-Containers ("Rolling Update"), ohne dass der SSI-Agent (Business-Logik) gestoppt oder rekompiliert werden muss \textcite{docker2017rolling}. Dies realisiert eine Agilität auf Infrastrukturebene.
Zweitens erlaubt das TLS 1.3-Protokoll eine dynamische Aushandlung (Negotiation) der Cipher Suites \textcite{keyfactor2023tls}. Die Sidecar-Proxies können so konfiguriert werden, dass sie mehrere PQC-Algorithmen parallel unterstützen und je nach Client-Fähigkeit den stärksten gemeinsamen Standard wählen. Dies stellt eine Protokoll-basierte Agilität dar.
Drittens begünstigt die Micro-Segmentation granulare Migrationspfade: Einzelne Netzwerksegmente (z.B. nur der Issuer) können isoliert auf neuere Algorithmen umgestellt werden, ohne das Gesamtsystem zu gefährden \textcite{colortokens2025microsegmentation}.

\paragraph{Design-Refinement für Iteration 2}

Basierend auf diesen Erkenntnissen wird das Design für Iteration 2 gezielt erweitert, um PQC zusätzlich auf der Applikationsebene zu verankern:

\begin{itemize}
\item \textbf{Application-Layer-PQC:} Integration der \emph{liboqs} direkt in die SSI-Agenten. Dies ermöglicht PQC-Signaturen für VCs und DIDComm-Nachrichten, wodurch eine Ende-zu-Ende-Integrität unabhängig vom Transportkanal gewährleistet wird (Adressierung FF2).
\item \textbf{Hybride Sicherheitsarchitektur:} Beibehaltung der Sidecar-Proxies als erste Verteidigungslinie (Transport Security) bei gleichzeitiger Härtung der Datenobjekte selbst (Application Security). Dies schafft Redundanz und Tiefenverteidigung.
\end{itemize}

\subsection{Iteration 2: Application-Layer PQC-Integration}

\subsubsection{Designziele dieser Iteration}
%   ALLE FF, mit SCHWERPUNKT auf FF2 (PQC), FF3 (Performance) \\
%   → FF1 (Architektur): Blockchain-Integration finalisieren \\
%   → FF1 (Architektur): Compliance-Layer finalisieren (BSI, DSGVO) \
%   → FF2 (Algorithmen): Vollständige PQC-Suite (ML-DSA, ML-KEM) \\
%   → FF2 (Algorithmen): Hybride Schemata implementieren \
%   → FF3 (Kryptoagilität): Plugin-System für Algorithmen implementieren
%   → FF3 (Kryptoagilität): Key-Rotation-Mechanismus validieren

Die zweite Iteration der Artefaktentwicklung baut auf der in Iteration 1 erfolgreich validierten Basisarchitektur auf und korrespondiert erneut mit der DSRM-Phase 2 \textit{Objectives} nach \textcite[S. 54]{peffers_DesignScienceResearchmethodologyinformationsystemsresearch_2007}. Der Fokus dieser Iteration liegt auf der Erweiterung des Prototyps um eine tiefgreifende PQC-Integration auf der Anwendungsebene (Application Layer). Im Kontext des Drei-Zyklen-Modells nach \textcite[S. 88]{hevner_ThreeCycleViewDesignScienceResearch_2007} wird der Design Cycle intensiviert, um die kryptografische Sicherheit von der reinen Transportsicherung (TLS) auf die tatsächlichen Nutzdaten (Verifiable Credentials und DID-Dokumente) auszuweiten und somit eine Ende-zu-Ende-Sicherheit zu gewährleisten.

Die Designziele dieser Iteration leiten sich konsistent aus den in Kapitel~\ref{sec:Zielsetzung und Forschungsfragen} definierten Forschungsfragen ab, wobei eine inhaltliche Vertiefung der technischen Anforderungen erfolgt:

Bezüglich \textbf{FF1 (Systemarchitektur \& Compliance)} wird das Ziel verfolgt, die SSI-Kernprozesse - insbesondere Issuance und Verification - so zu modifizieren, dass sie quantenresistente Signaturen und Schlüsselformate nativ unterstützen. Das Design muss sicherstellen, dass die Unveränderlichkeit und Authentizität von Identitätsnachweisen unabhängig vom Transportkanal auch langfristig gegenüber Quantencomputer-Angriffen gewährleistet bleibt, was eine zentrale Anforderung für den Einsatz in KRITIS-Umgebungen darstellt.

Hinsichtlich \textbf{FF2 (Algorithmenauswahl \& Sicherheitsbewertung)} liegt der Fokus auf der Integration und Evaluierung des NIST-standardisierten Signaturalgorithmus ML-DSA (Dilithium) innerhalb der Credential-Strukturen. Das Designziel besteht darin, die praktische Machbarkeit von PQC-Signaturen in Verifiable Credentials (VCs) und Decentralized Identifiers (DIDs) nachzuweisen.

Für \textbf{FF3 (Kryptografische Agilität)} zielt diese Iteration auf die Implementierung von Agilitätsmechanismen direkt in den Datenstrukturen ab. Das System soll so gestaltet werden, dass es hybride Szenarien unterstützt und eine Koexistenz sowie den nahtlosen Wechsel zwischen klassischen (z.\,B. Ed25519) und post-quanten Kryptografieverfahren innerhalb der DID-Methoden und Credential-Definitionen ermöglicht, ohne die Interoperabilität grundlegend zu gefährden.

\subsubsection{Architekturentwurf}
%   - Application-Layer-PQC-Integration (liboqs in ACA-Py) \\
%   - Kryptoagiles Design (Plugin-Architektur) \\

\fixme{\url{https://github.com/decentralized-identity/aries-rfcs} ==> Aries RFCs referenzieren}

\paragraph{Gesamtarchitektur}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Gesamtarchitektur_Iteration2.png}
    \caption{Gesamtarchitekturentwurf Iteration 2}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Gesamtarchitektur_Iteration2}
\end{figure}

\paragraph{ACA-Py Applikationsarchitektur}

\fixme{Überflüssig?}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ACAPY Application Architecture_Iteration 2.png}
    \caption{ACA-Py High Level Applikationsarchitektur}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:ACAPY_Application_Architecture_Iteration2}
\end{figure}

Die Architektur von Hyperledger Aries Cloud Agent Python (ACA-Py) folgt einem fünfschichtigen Design mit klarer Verantwortlichkeitstrennung.

\textit{Layer 1} bildet eine zustandslose HTTP-REST-API (Admin API), die es Controller-Anwendungen ermöglicht, den Agent über standardisierte Endpoints wie \texttt{/out-of-band/create-invitation}, \texttt{/issue-credential-2.0/send} oder \texttt{/present-proof-2.0/send-request} zu steuern, ohne direkt mit der Python-Codebasis zu interagieren.

\textit{Layer 2} implementiert die SSI-Geschäftslogik als Protocol Handler, die Aries RFCs (Request for Comments) in Form zustandsbasierter State Machines umsetzen: Das Out-of-Band Protocol (RFC 0434) generiert Invitation-Nachrichten mit \texttt{did:peer:4}-DIDs, das DID Exchange Protocol (RFC 0023) authentifiziert Agent-Verbindungen mittels ED25519-Signaturen, das Issue Credential Protocol (RFC 0453) erstellt AnonCreds-CL-signierte Credentials, und das Present Proof Protocol (RFC 0454) orchestriert Zero-Knowledge-Proof-basierte Presentations.

\textit{Layer 3} abstrahiert die Schlüsselverwaltung durch das Aries-Askar-Wallet, das ED25519-Schlüsselpaare (32 Bytes, für Signaturen) und X25519-Schlüsselpaare (32 Bytes, für Key Agreement) generiert, diese mit Multicodec-Präfixen (\texttt{0xed}, \texttt{0xec}) kodiert und verschlüsselt in einer SQLite-Datenbank ablegt, wobei ChaCha20-Poly1305-Authenticated-Encryption nach Argon2id-basierter Schlüsselableitung verwendet wird.

\textit{Layer 4} realisiert DIDComm-Messaging über \texttt{pack\_message()} und \texttt{unpack\_message()}: Nachrichten werden mittels X25519-ECDH-Key-Agreement, HKDF-SHA256-Schlüsselableitung und XChaCha20-Poly1305-Verschlüsselung in JWE-Strukturen transformiert, die Vertraulichkeit und Integrität zwischen Agents gewährleisten.

\textit{Layer 5} implementiert HTTP- und WebSocket-basierte Transport-Mechanismen für Inbound- (\texttt{--inbound-transport http 0.0.0.0 8020}) und Outbound-Kommunikation, wobei in der klassischen Architektur kein quantensicheres TLS verwendet wird. Die gesamte kryptografische Basis -- ED25519 für DID-Authentifizierung, X25519 für DIDComm-Encryption und AnonCreds-CL-Signaturen für Credentials -- ist nicht quantenresistent, da elliptische Kurven durch Shor's Algorithmus auf Quantencomputern kompromittiert werden können, was die Notwendigkeit einer systematischen Post-Quantum-Kryptografie-Integration begründet.


\paragraph{ACA-Py Applikationsarchitektur mit PQC-Integration}

Die Integration von Post-Quantum-Kryptografie in Hyperledger Aries Cloud Agent Python erfolgt über eine plugin-basierte Architektur, die sechs zentrale Design-Prinzipien verfolgt: Transparenz, Erweiterbarkeit, Interoperabilität, Defense-in-Depth, Standards-Konformität und Separierung von Belangen.
Abbildung~\ref{fig:ACAPY_Application_Architecture_Iteration2_PQC} visualisiert die erweiterte Architektur und hebt die Unterschiede zur klassischen Implementierung hervor.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ACA-Py Applikationsarchitektur mit PQC-Integration.png}
    \caption{ACA-Py High Level Applikationsarchitektur mit PQC-Integration}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:ACAPY_Application_Architecture_Iteration2_PQC}
\end{figure}

Das fundamentale Design-Prinzip der \textit{Transparenz} manifestiert sich in der vollständigen Beibehaltung der Controller-Application-Schnittstelle und der ACA-Py Admin API. Beide Schichten bleiben unverändert, wodurch bestehende Controller-Anwendungen -- beispielsweise Web-Applikationen, Jupyter-Notebooks oder CLI-Tools -- ohne Modifikationen weiterhin funktionieren. Die HTTP-REST-API exponiert identische Endpoints (\texttt{/out-of-band/create-invitation}, \texttt{/issue-credential-2.0/send}, etc.) mit strukturell unveränderten Request- und Response-Schemata. Diese Architekturentscheidung eliminiert Breaking Changes und ermöglicht eine schrittweise Migration: Controller-Code bleibt kompatibel, während die kryptografischen Primitive unterhalb der API-Ebene ausgetauscht werden. Das Diagramm markiert diese Schichten explizit mit \texttt{NO CHANGE}, um die Nicht-Invasivität der PQC-Integration zu verdeutlichen.

Die \textit{Erweiterbarkeit} der Architektur wird durch das Key-Type-Registry-Pattern gewährleistet. Das PQC-Plugin erweitert die Wallet-Schnittstelle um neue Schlüsseltypen (ML-DSA-65 für digitale Signaturen, ML-KEM-768 für Key Encapsulation), ohne die bestehende Registry zu modifizieren. Diese Erweiterung erfolgt zur Laufzeit durch Registrierung zusätzlicher \texttt{KeyType}-Objekte, die Multicodec-Präfixe (\texttt{0xd065} für ML-DSA-65, \texttt{0xe018} für ML-KEM-768), Multicodec-Namen und JWS-Algorithmus-Identifier definieren. Die Architektur nutzt pluggable Crypto Backends: Das PQC-Plugin bindet \texttt{liboqs-python} als kryptografische Engine ein, während klassische Operationen weiterhin über \texttt{libsodium} abgewickelt werden. Dieses Backend-Abstraktionsmodell erlaubt die Integration zukünftiger Post-Quantum-Algorithmen -- beispielsweise SLH-DSA (NIST FIPS-205) oder zukünftige NIST-Standardisierungsrunden -- durch Addition weiterer Backend-Module ohne Änderungen an der Core-Architektur.

Das Design gewährleistet \textit{Interoperabilität} durch explizite Fallback-Mechanismen zu klassischen Algorithmen. Die Wallet-Operation-Wrapper im Plugin implementieren Key-Type-Erkennung: Wenn eine Operation einen klassischen Schlüssel (ED25519, X25519) detektiert, delegiert sie an die ursprüngliche Implementierung; bei PQC-Schlüsseln erfolgt die Delegation an die \texttt{liboqs}-Integration. Diese Dual-Mode-Architektur ermöglicht hybride Deployments, in denen Agents sowohl mit PQC-fähigen als auch mit Legacy-Agents kommunizieren können. Zusätzlich unterstützt das Design explizit Hybrid-Kryptografie (X25519MLKEM768), bei der klassische und quantensichere Algorithmen kombiniert werden, um Crop-and-Paste-Resistenz gemäß BSI-Richtlinien zu erreichen. Die strukturelle JWE-Kompatibilität bleibt erhalten: DIDComm-Nachrichten verwenden weiterhin die JSON-Web-Encryption-Struktur mit \texttt{protected}, \texttt{recipients}, \texttt{iv}, \texttt{ciphertext} und \texttt{tag}-Feldern, wobei lediglich das \texttt{encrypted\_key}-Feld größere Ciphertexte aufnimmt (1088 Bytes für ML-KEM-768 statt circa 32 Bytes für X25519-ECDH) und der \texttt{alg}-Header den verwendeten Algorithmus spezifiziert.

Das Prinzip \textit{Defense-in-Depth} wird durch zweischichtige Quantenresistenz realisiert. Auf Transport-Layer-Ebene (Iteration 1, bereits implementiert) terminieren Nginx-Reverse-Proxies TLS-1.3-Verbindungen mit ML-KEM-768-Schlüsselaustausch und ML-DSA-65-Zertifikaten, wodurch alle HTTP-basierten Kommunikationskanäle quantensicher werden. Auf Application-Layer-Ebene (Iteration 2, Plugin-Integration) verschlüsselt das DIDComm-Messaging-Protokoll Nachrichten mittels ML-KEM-768-basierter Key Encapsulation, unabhängig vom zugrundeliegenden Transport-Protokoll. Diese doppelte Absicherung gewährleistet, dass selbst bei Kompromittierung der Transport-Layer-Infrastruktur (z.\,B.\ Proxy-Penetration) die End-to-End-verschlüsselten DIDComm-Nachrichten quantenresistent bleiben. Das Architekturdiagramm visualisiert diese Schichtung durch separate Markierungen für \texttt{Iteration 2: PQC within DIDComm} (Transport Layer) und die PQC-spezifischen Wallet-Operationen (\texttt{PQC Pack/Unpack}).

Die \textit{Standards-Konformität} manifestiert sich in der strukturellen Beibehaltung aller Aries RFCs. Das Out-of-Band Protocol (RFC 0434), DID Exchange Protocol (RFC 0023), Issue Credential Protocol (RFC 0453) und Present Proof Protocol (RFC 0454) bleiben als Python-Module unverändert; lediglich die von ihnen aufgerufenen Wallet-Operationen werden durch das Plugin abgefangen und an PQC-Implementierungen delegiert. Diese Interception-Architektur (im Diagramm als \texttt{PQC-Interception} und \texttt{Delegation to PQC-Plugin} markiert) gewährleistet, dass die State-Machine-Logik der Protokoll-Handler intakt bleibt. Die Multicodec-Spezifikation für ML-DSA-65 und ML-KEM-768 ist derzeit provisorisch (W3C-Standardisierung ausstehend), jedoch strukturell kompatibel mit dem bestehenden Multicodec-Framework. Die DIDComm-v1-JWE-Kompatibilität bleibt vollständig erhalten, sodass Messages zwischen PQC-fähigen und klassischen Agents ausgetauscht werden können, sofern beide Seiten die entsprechenden Algorithmen unterstützen.

Das Prinzip der \textit{Separierung von Belangen} reflektiert die schrittweise Migrationsstrategie. AnonCreds-Credentials verwenden weiterhin CL-Signaturen (Camenisch-Lysyanskaya), da der AnonCreds-Standard keine Post-Quantum-Signaturen spezifiziert und eine Änderung Inkompatibilität mit bestehenden Indy-Ledger-Deployments erzeugen würde. Diese Entscheidung isoliert die PQC-Integration auf die DID-Ebene (did:peer:4-Generierung mit ML-DSA-65/ML-KEM-768) und die Transport-Ebene (TLS 1.3 und DIDComm), während die Credential-Signatur-Ebene klassisch bleibt. Zukünftige Migrationen könnten W3C Verifiable Credentials mit PQC-Signaturen nutzen, ohne die bestehende AnonCreds-Infrastruktur zu beeinflussen. Diese Architektur vermeidet einen Big-Bang-Ansatz, bei dem alle kryptografischen Schichten simultan migriert werden müssten, zugunsten einer inkrementellen, risikominimierten Transition.

Die zentrale architektonische Innovation ist das \texttt{PQC Plugin}-Modul, das als Vermittlungsschicht zwischen Protocol Handlers, Wallet Interface und den kryptografischen Backends fungiert. Das Plugin umfasst vier Kernkomponenten: Den \texttt{liboqs Integration Layer}, der Python-Bindings zu den NIST-standardisierten Algorithmen bereitstellt; den \texttt{PQC did:peer:4 Generator}, der Dual-Key-DIDs (Signing Key: ML-DSA-65, Agreement Key: ML-KEM-768) mit korrekter Multicodec-Kodierung erzeugt; die \texttt{Protocol Interceptors}, die Aufrufe von Protocol Handlers abfangen und an PQC-Implementierungen delegieren; sowie die \texttt{Wallet Operation Wrappers}, die \texttt{create\_key()}, \texttt{sign\_message()}, \texttt{verify\_message()}, \texttt{pack\_message()} und \texttt{unpack\_message()} durch PQC-Varianten erweitern. Diese Komponenten werden beim Agent-Start durch die Plugin-Setup-Funktion registriert und installiert, wodurch die Interception-Logik zur Laufzeit aktiviert wird.

Die Wallet-Implementierung (Aries Askar mit SQLite-Backend) erfährt zwei wesentliche Erweiterungen: Die \texttt{Expanded Key-Type-Registry} speichert Metadaten für ML-DSA-65- und ML-KEM-768-Schlüssel, einschließlich Schlüsselgrößen (Public Key: 1952 Bytes bzw.\ 1184 Bytes, Private Key: 4000 Bytes bzw.\ 2400 Bytes) und Algorithmus-Identifiern. Die \texttt{PQC-Key-Storage}-Erweiterung persistiert diese Schlüssel verschlüsselt in der SQLite-Datenbank, wobei die bestehende ChaCha20-Poly1305-AEAD-Verschlüsselung nach Argon2id-Key-Derivation unverändert bleibt. Die Wallet-Schnittstelle exponiert neue Operationen (\texttt{PQC Key Operations}, \texttt{PQC Sign/Verify}, \texttt{PQC Pack/Unpack}), die transparent von den Protocol Handlers aufgerufen werden, ohne dass diese Kenntnis über die zugrundeliegenden Algorithmen besitzen müssen.

Die Architektur gewährleistet, dass alle Schichten oberhalb der Wallet-Schnittstelle -- insbesondere Protocol Handlers und Admin API -- keine Änderungen erfahren. Dies reduziert die Testoberfläche, da ausschließlich die Plugin-Komponenten und Wallet-Erweiterungen validiert werden müssen, während die etablierte Aries-Protokoll-Logik unverändert bleibt. Die resultierende Architektur kombiniert die Produktionsreife von Hyperledger Aries mit der Quantenresistenz von NIST-standardisierten Post-Quantum-Algorithmen, ohne die Interoperabilität mit bestehenden SSI-Deployments zu kompromittieren.


%  - PQC-Algorithmen-Integration (ML-DSA, ML-KEM) \\
%  - Kryptoagiles Design (Plugin-Architektur) \\
\subsubsection{Implementierung}

% Die Implementierung der PQC-Unterstützung auf Application-Ebene erfolgt durch das \texttt{pqc\_didpeer4\_fm}-Plugin, das über den ACA-Py Plugin-Mechanismus transparent in die Agent-Architektur integriert wird. ACA-Py definiert ein Plugin als Python-Paket mit einem \texttt{setup(context)}-Entrypoint, der beim Agent-Start aufgerufen wird und Zugriff auf zentrale Agent-Komponenten (Wallet, DIDComm, Protocol Registry) über einen \texttt{PluginContext} erhält. Plugins können entweder neue Protokolle registrieren oder bestehende Funktionen durch Monkey-Patching erweitern.

Die Implementierung der PQC-Unterstützung auf Application-Ebene folgt einem zweistufigen Ansatz. In der ersten Stufe wird das \texttt{pqc\_didpeer4\_fm}-Plugin entwickelt, das sich in den ACA-Py Plugin-Mechanismus einfügt. Das Plugin ist als Python-Paket strukturiert und definiert einen \texttt{setup(context)}-Entrypoint, der beim Agent-Start aufgerufen wird. Über den \texttt{PluginContext} erhält das Plugin unmittelbaren Zugriff auf zentrale Agent-Komponenten wie die Wallet, das DIDComm-System und die Protocol Registry. Diese Architektur ermöglicht es Plugins, neue Protokolle zu registrieren oder bestehende Funktionen durch Monkey-Patching zu erweitern, ohne den ACA-Py Kern zu modifizieren.

Die zweite Stufe integriert das entwickelte Plugin in die containerisierte Deployment-Infrastruktur. Dabei wird das Plugin als Abhängigkeit in der \texttt{Dockerfile} definiert, sodass es während des Container-Builds installiert wird. Anschließend erfolgt die Konfiguration über \texttt{docker-compose}, indem der ACA-Py Service mit den erforderlichen Umgebungsvariablen und Plugin-Parametern initialisiert wird. Diese Trennung von Plugin-Entwicklung und Deployment-Integration gewährleistet Modularität und Wartbarkeit.

\begin{figure}[h]
  \flushleft
  \caption{Verzeichnisstruktur des Projekts}
  \label{fig:pqc_didpeer4_fm_directory_structure}
\dirtree{%
.1 pqc\_didpeer4\_fm/.
.2 pqc\_didpeer4\_fm/.
.3 v1\_0/.
.4 askar\_pqc\_patch.py.
.4 base\_manager\_patch.py.
.4 connection\_target\_patch.py.
.4 key\_type\_patches.py.
.4 key\_types.py.
.4 liboqs\_wrapper.py.
.4 monkey\_patches.py.
.4 multicodec\_patch.py.
.4 pqc\_didcomm\_v1.py.
.4 pqc\_multicodec.py.
.4 pqc\_multikey.py.
.4 pqc\_peer4\_creator.py.
.4 pqc\_peer4\_resolver.py.
.4 validator\_patch.py.
.4 wallet\_patch.py.
.3 \_\_init\_\_.py.
.2 README.md.
.2 setup.py.
}
\begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
\end{flushleft}
\end{figure}

% Das \texttt{pqc\_didpeer4\_fm}-Plugin nutzt beide Erweiterungsmechanismen: Es registriert neue PQC-Schlüsseltypen über \texttt{key\_types.py} und \texttt{key\_type\_patches.py} in der globalen \texttt{KeyType}-Registry und erweitert durch Monkey-Patching bestehende ACA-Py-Funktionen. Die Plugin-Initialisierung in \texttt{\_\_init\_\_.py} orchestriert die schrittweise Integration aller Module beim Agent-Start.

Die Implementierung des PQC-Plugins gliedert sich in drei funktionale Schichten, in die die in \autoref{fig:pqc_didpeer4_fm_directory_structure} dargestellten 15 Module eingeteilt sind: 

Die \textit{Kryptografie-Abstraktionsschicht} (\texttt{liboqs\_wrapper.py}) kapselt ML-DSA-65- und ML-KEM-768-Operationen, 

die \textit{DID-Verarbeitungsschicht} umfasst \texttt{pqc\_peer4\_creator.py} (did:peer:4-Generierung), \texttt{pqc\_peer4\_resolver.py} (DID-Auflösung), \texttt{pqc\_multicodec.py} und \texttt{pqc\_multikey.py} (Multiformat-Kodierung) sowie \texttt{pqc\_didcomm\_v1.py} (DIDComm-Verschlüsselung), 

und die \textit{Integration-Patching-Schicht} mit \texttt{monkey\_patches.py} (zentrale Funktion-Interceptors), \texttt{askar\_pqc\_patch.py} und \texttt{wallet\_patch.py} (Wallet-Operationen), \texttt{base\_manager\_patch.py} und \texttt{connection\_target\_patch.py} (Connection-Management), \texttt{validator\_patch.py} (Input-Validierung) sowie \texttt{multicodec\_patch.py} (Multicodec-Registry-Erweiterung). Diese modulare Architektur ermöglicht eine vollständig transparente PQC-Integration ohne Modifikation des ACA-Py-Kerncodes.

% \paragraph{Kryptografie-Abstraktionsschicht (liboqs Integration Layer)}
% - Einbindung der liboqs-python-Bibliothek \\
% - ML-DSA-65 Signatur-Operationen \\
% - ML-KEM-768 Key Encapsulation

%  - liboqs-Wrapper-Implementierung (liboqs\_wrapper.py) \\
%   - ML-DSA-65 Signatur-Operationen \\
%   - ML-KEM-768 Key Encapsulation \\
%   - Fehlerbehandlung und Validierung

% \paragraph{PQC did:peer:4 Generator}
% - Multicodec und Multikey-Kodierung \\
% - did:peer:4 Struktur mit Dual-Keys

%   - DID-Generierung mit PQC-Schlüsseln (pqc\_peer4\_creator.py) \\
%   - DID-Auflösung (pqc\_peer4\_resolver.py) \\
%   - Multicodec-Kodierung (pqc\_multicodec.py, multicodec\_patch.py) \\
%   - Multikey-Transformation (pqc\_multikey.py) \\
%   - did:peer:4 Struktur mit Dual-Keys (ML-DSA-65 + ML-KEM-768)

% \paragraph{Protocol Interceptors (Monkey-Patching)}
% - Interception-Mechanismus \\
% - Connection Manager Interception \\
% - DID Exchange Signatur-Interception \\
% - Installation der Patches

%   - Interception-Mechanismus (\_\_init\_\_.py, monkey\_patches.py) \\
%   - Connection Manager Interception (base\_manager\_patch.py) \\
%   - DID Exchange und Connection Target (connection\_target\_patch.py) \\
%   - JWS-Validierung (validator\_patch.py) \\
%   - Installation der Patches (Reihenfolge und Abhängigkeiten)

% \paragraph{Wallet Operation Wrappers}
% - Key-Creation-Wrapper \\
% - Signatur-Wrapper (sign\_message / verify\_message) \\
% - DIDComm Pack/Unpack-Wrapper \\

%   - Askar-Integration (askar\_pqc\_patch.py)
%   - Key-Creation-Wrapper und Verkey-Lookup (wallet\_patch.py)
%   - Signatur-Wrapper (sign\_message / verify\_message)
%   - DIDComm Pack/Unpack-Wrapper (pqc\_didcomm\_v1.py)

% \paragraph{Key-Type-Registry-Erweiterung}
% - KeyType-Datenstruktur \\
% - Registrierung im Wallet \\
% - Fallback-Mechanismus \\

%   - KeyType-Definitionen (key\_types.py)
%   - Registry-Integration (key\_type\_patches.py)
%   - API-Schema-Anpassungen
%   - Fallback-Mechanismus

% \paragraph{Wallet-Speicherung und Persistierung}
% - SQLite-Schema (Aries Askar) \\

% \paragraph{Deployment und Konfiguration}
% - Plugin-Installation (Dockerfile.acapy-base-pqc)\\
% - Agent-Konfiguration \\
% - Umgebungsvariablen \\
% - Verifikation der Installation

%   - Plugin-Installation (Dockerfile.acapy-base-pqc, setup.py) \\
%   - Agent-Konfiguration (--plugin pqc\_didpeer4\_fm) \\
%   - Umgebungsvariablen (OQS\_DISABLE=0) \\
%   - Verifikation der Installation

\paragraph{Pluginentwicklung: Kryptografie-Abstraktionsschicht}

Die Kryptografie-Abstraktionsschicht bildet die unterste Ebene der PQC-Integration und wird durch das Modul \texttt{liboqs\_wrapper.py} (Abbildung~\ref{fig:pqc_didpeer4_fm_directory_structure_Kryptografie-Abstraktionsschicht}) realisiert. Dieses Modul kapselt die nativen Operationen der C-basierten liboqs-Bibliothek und stellt eine Python-API für die NIST-standardisierten PQC-Algorithmen ML-DSA-65 (Dilithium3, FIPS-204) und ML-KEM-768 (Kyber768, FIPS-203) bereit.

\begin{figure}[H]
  \flushleft
  \caption{Verzeichnisstruktur des Projekts}
  \label{fig:pqc_didpeer4_fm_directory_structure_Kryptografie-Abstraktionsschicht}
\dirtree{%
.1 pqc\_didpeer4\_fm/.
.2 pqc\_didpeer4\_fm/.
.3 v1\_0/.
.4 askar\_pqc\_patch.py.
.4 base\_manager\_patch.py.
.4 connection\_target\_patch.py.
.4 key\_type\_patches.py.
.4 key\_types.py.
.4 \textbf{liboqs\_wrapper.py}.
.4 monkey\_patches.py.
.4 multicodec\_patch.py.
.4 pqc\_didcomm\_v1.py.
.4 pqc\_multicodec.py.
.4 pqc\_multikey.py.
.4 pqc\_peer4\_creator.py.
.4 pqc\_peer4\_resolver.py.
.4 validator\_patch.py.
.4 wallet\_patch.py.
.3 \_\_init\_\_.py.
.2 README.md.
.2 setup.py.
}
\begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
\end{flushleft}
\end{figure}

Die Implementierung (Listing~\ref{lst:liboqs_wrapper.py}) definiert eine \texttt{LibOQSWrapper}-Klasse mit sechs Kern\-methoden: \texttt{generate\_ml\_dsa\_65\_keypair()} und \texttt{generate\_ml\_kem\_768\_keypair()} erzeugen kryptografische Schlüsselpaare, \texttt{sign\_ml\_dsa\_65()} und \texttt{verify\_ml\_dsa\_65()} implementieren digitale Signaturen, während \texttt{encapsulate\_ml\_kem\_768()} und \texttt{decapsulate\_ml\_kem\_768()} die Key Encapsulation für sichere Schlüsselvereinbarung realisieren.

Die Wrapper-Architektur abstrahiert die komplexen Foreign-Function-Interface-Aufrufe (FFI) an die liboqs-C-Bibliothek und stellt sicher, dass Schlüsselmaterial ausschließlich als Byte-Arrays (\texttt{bytes}) serialisiert wird - eine Voraussetzung für die Persistierung in der Aries-Askar-Wallet und die Kodierung in Multicodec-Formate. Das Singleton-Pattern (\texttt{get\_liboqs()}) gewährleistet eine einzige globale Instanz zur Vermeidung redundanter Initialisierungen. Diese Abstraktionsschicht ermöglicht es den höheren Modulen (DID-Generierung, DIDComm-Verschlüsselung), PQC-Operationen durchzuführen, ohne direkte Abhängigkeiten zur liboqs-C-API zu haben.

\paragraph{Pluginentwicklung: DID-Verarbeitungsschicht}

Die DID-Verarbeitungsschicht (siehe Abbildung~\ref{fig:pqc_didpeer4_fm_directory_structure_DID-Verarbeitungsschicht}) orchestriert die Erzeugung, Auflösung und Kodierung von PQC-fähigen did:peer:4-Identifikatoren sowie die DIDComm-Nachrichtenverschlüsselung. Diese Schicht umfasst sechs funktional gekoppelte Module, die gemeinsam eine standardkonforme Integration von Post-Quantum-Kryptografie in das did:peer:4-Ökosystem realisieren.

\begin{figure}[H]
  \flushleft
  \caption{Verzeichnisstruktur des Projekts}
  \label{fig:pqc_didpeer4_fm_directory_structure_DID-Verarbeitungsschicht}
\dirtree{%
.1 pqc\_didpeer4\_fm/.
.2 pqc\_didpeer4\_fm/.
.3 v1\_0/.
.4 askar\_pqc\_patch.py.
.4 base\_manager\_patch.py.
.4 connection\_target\_patch.py.
.4 key\_type\_patches.py.
.4 key\_types.py.
.4 liboqs\_wrapper.py.
.4 monkey\_patches.py.
.4 multicodec\_patch.py.
.4 \textbf{pqc\_didcomm\_v1.py}.
.4 \textbf{pqc\_multicodec.py}.
.4 \textbf{pqc\_multikey.py}.
.4 \textbf{pqc\_peer4\_creator.py}.
.4 \textbf{pqc\_peer4\_resolver.py}.
.4 validator\_patch.py.
.4 wallet\_patch.py.
.3 \_\_init\_\_.py.
.2 README.md.
.2 setup.py.
}
\begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
\end{flushleft}
\end{figure}

Das Modul \texttt{pqc\_peer4\_creator.py} (Listing~\ref{lst:pqc_peer4_creator.py}) implementiert die Funktion \texttt{create\_pqc\_peer4\_did()}, die aus zwei PQC-Schlüsselpaaren (ML-DSA-65 für \texttt{authentication}/\texttt{assertionMethod}, ML-KEM-768 für \texttt{keyAgreement}) einen did:peer:4-Long-Form-Identifier generiert. Die Schlüssel werden über die Wallet-API erzeugt, in Multikey-Format transformiert und als \texttt{KeySpec}-Objekte in einem did:peer:4-Input-Dokument strukturiert, wobei die Reihenfolge der Schlüssel deren Fragment-IDs determiniert (\texttt{\#key-0} für Signaturen, \texttt{\#key-1} für Verschlüsselung in \texttt{recipientKeys}). Das Gegenstück \texttt{pqc\_peer4\_resolver.py} (Listing~\ref{lst:pqc_peer4_resolver.py}) registriert einen DID-Resolver für die \texttt{peer}-Methode, der did:peer:4-Long-Form-DIDs in DID-Dokumente
  auflöst und dabei PQC-Multicodec-Präfixe korrekt dekodiert.

Die Multiformat-Kodierung wird durch drei Module realisiert: \texttt{pqc\_multicodec.py} (Listing~\ref{lst:pqc_multicodec.py}) definiert eine Multicodec-Registry mit provisorischen Präfixen gemäß W3C-Draft (ML-DSA-65: \texttt{0xd065}, ML-KEM-768: \texttt{0xe018}) und stellt Wrapper-Funktionen (\texttt{wrap\_pqc()}, \texttt{unwrap\_pqc()}) für Präfix-Operationen bereit. \texttt{pqc\_multikey.py} (Listing~\ref{lst:pqc_multikey.py}) transformiert Schlüsselinformationen in das Multikey-Format durch Verkettung von Multicodec-Präfix und Schlüsselmaterial sowie Base58-Kodierung mit Multibase-Präfix \texttt{z} (Base58btc), wodurch Multikeys wie \texttt{z6MNxxx...} (ML-DSA-65) oder \texttt{z6MK768xxx...} (ML-KEM-768) entstehen.

Das Modul \texttt{pqc\_didcomm\_v1.py} (Listing~\ref{lst:pqc_didcomm_v1.py}) erweitert die DIDComm-v1-Envelope-Verarbeitung um PQC-Unterstützung: \texttt{pack\_message\_pqc()} und \texttt{unpack\_message\_pqc()} detektieren automatisch anhand der Schlüssellänge (ML-KEM-768: 1184~Bytes $\approx$ 1615~Base58-Zeichen vs. X25519: 32~Bytes $\approx$ 44~Zeichen), ob PQC- oder klassische Kryptografie verwendet werden muss, und generieren JWE-Envelopes mit angepassten Algorithmus-Headern (\texttt{alg: "PQC-Authcrypt"} bzw. \texttt{"PQC-Anoncrypt"}). Die Content Encryption erfolgt weiterhin mit XChaCha20-Poly1305 (quantum-sicher für symmetrische Verschlüsselung), während der Content Encryption Key (CEK) mittels ML-KEM-768 Key Encapsulation für jeden Empfänger verschlüsselt wird - im Gegensatz zur klassischen ECDH-ES-basierten CEK-Vereinbarung. Diese Schicht ermöglicht eine hybride Betriebsweise, bei der PQC- und klassische Agenten koexistieren können, solange jeweils homogene Verschlüsselungsmodi verwendet werden.

\paragraph{Pluginentwicklung: Integration-Patching-Schicht}

Die Integration-Patching-Schicht (siehe Abbildung~\ref{fig:pqc_didpeer4_fm_directory_structure_Integration-Patching-Schicht}) implementiert die transparente Einbettung der PQC-Funktionalität in den ACA-Py-Kern durch gezieltes Monkey-Patching kritischer Funktionen und Erweiterung globaler Registries. Diese Schicht umfasst neun Module, die gemeinsam eine vollständig transparente PQC-Integration ohne Modifikation des ACA-Py-Quellcodes ermöglichen - existierende Workflows und API-Aufrufe bleiben unverändert funktionsfähig.

\begin{figure}[H]
  \flushleft
  \caption{Verzeichnisstruktur des Projekts}
  \label{fig:pqc_didpeer4_fm_directory_structure_Integration-Patching-Schicht}
\dirtree{%
.1 pqc\_didpeer4\_fm/.
.2 pqc\_didpeer4\_fm/.
.3 v1\_0/.
.4 \textbf{askar\_pqc\_patch.py}.
.4 \textbf{base\_manager\_patch.py}.
.4 \textbf{connection\_target\_patch.py}.
.4 \textbf{key\_type\_patches.py}.
.4 \textbf{key\_types.py}.
.4 liboqs\_wrapper.py.
.4 \textbf{monkey\_patches.py}.
.4 \textbf{multicodec\_patch.py}.
.4 pqc\_didcomm\_v1.py.
.4 pqc\_multicodec.py.
.4 pqc\_multikey.py.
.4 pqc\_peer4\_creator.py.
.4 pqc\_peer4\_resolver.py.
.4 \textbf{validator\_patch.py}.
.4 \textbf{wallet\_patch.py}.
.3 \_\_init\_\_.py.
.2 README.md.
.2 setup.py.
}
\begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
\end{flushleft}
\end{figure}

Das zentrale Orchestrierungsmodul \texttt{monkey\_patches.py} (Listing~\ref{lst:monkey_patches.py}) koordiniert die Installation aller Patches durch die Funktion \texttt{apply\_all\_patches()}, die beim Plugin-Setup aufgerufen wird. Dieses Modul überschreibt Methoden der Klasse \texttt{BaseConnectionManager} (z.\,B. \texttt{create\_did\_peer\_4()}, \texttt{\_extract\_key\_material\_in\_base58\_format()}, \texttt{long\_did\_peer\_4\_to\_short()}) und delegiert deren Implementierung an spezialisierte Patch-Module, wobei die ursprünglichen Methoden als Fallback-Referenzen gespeichert werden. Das Modul \texttt{base\_manager\_patch.py} (Listing~\ref{lst:base_manager_patch.py}) stellt die PQC-Implementierungen dieser \texttt{BaseConnectionManager}-Methoden bereit: \texttt{create\_did\_peer\_4\_pqc\_complete()} generiert did:peer:4-DIDs mit ML-DSA-65- und ML-KEM-768-Schlüsseln anstelle klassischer ED25519/X25519-Schlüssel, \texttt{\_extract\_key\_material\_in\_base58\_format\_pqc()} extrahiert PQC-Schlüsselmaterial aus DID-Dokumenten unter Berücksichtigung der größeren Schlüssellängen, und \texttt{record\_keys\_for\_resolvable\_did\_pqc()} persistiert beide PQC-Schlüssel (Signatur- und Verschlüsselungsschlüssel) in der Wallet-Datenbank.

Die Wallet-Integration erfolgt durch drei Module: \texttt{askar\_pqc\_patch.py} (Listing~\ref{lst:askar_pqc_patch.py}) patcht die Aries-Askar-Funktionen \texttt{create\_keypair()} zur Unterstützung von PQC-Schlüsselgenerierung mittels liboqs sowie \texttt{pack\_message()} und \texttt{unpack\_message()} zur Integration der PQC-DIDComm-v1-Implementierung aus \texttt{pqc\_didcomm\_v1.py}. \texttt{wallet\_patch.py} (Listing~\ref{lst:wallet_patch.py}) erweitert die Methode \texttt{get\_local\_did\_for\_verkey()} der \texttt{AskarWallet}-Klasse, um ML-KEM-768-Verkeys (1184~Bytes Länge) korrekt in der Datenbank zu lokalisieren – eine kritische Anpassung, da klassische Verkey-Lookups nur für 32-Byte-ED25519-Schlüssel ausgelegt sind. \texttt{connection\_target\_patch.py} (Listing~\ref{lst:connection_target_patch.py}) passt das Marshmallow-Schema der \texttt{ConnectionTarget}-Klasse an, indem die Validierungsregeln für \texttt{recipient\_keys} PQC-konforme Schlüssellängen akzeptieren (bisherige Validierung: exakt 44~Base58-Zeichen für ED25519).

Die Erweiterung der Schlüsseltyp-Infrastruktur erfolgt durch zwei Module: \texttt{key\_types.py} (Listing~\ref{lst:key_types.py}) definiert neue \texttt{KeyType}-Konstanten (\texttt{ML\_DSA\_65}, \texttt{ML\_KEM\_768}) mit Metadaten wie NIST-FIPS-Referenzen, Schlüssellängen und Multicodec-Präfixen. \texttt{key\_type\_patches.py} (Listing~\ref{lst:key_type_patches.py}) registriert diese KeyTypes in der globalen ACA-Py-Registry durch \texttt{register\_pqc\_key\_types()}, erweitert die Admin-API-Schemata (\texttt{patch\_api\_key\_type\_schemas()}) zur Akzeptanz von PQC-KeyType-Strings in JSON-Requests, und patcht Algorithmus-Mappings (\texttt{patch\_alg\_mappings\_for\_pqc()}) für JWS/JWE-Header-Generierung. \texttt{multicodec\_patch.py} (Listing~\ref{lst:multicodec_patch.py}) erweitert die globale \texttt{SupportedCodecs}-Enumeration durch dynamisches Hinzufügen von ML-DSA-65- und ML-KEM-768-Multicodec-Einträgen, sodass Multicodec-Dekodierungsfunktionen aus \texttt{multiformats}-Bibliotheken PQC-Präfixe verarbeiten können.

Das Modul \texttt{validator\_patch.py} (Listing~\ref{lst:validator_patch.py}) patcht die \texttt{JWSHeaderKid}-Validierungsklasse, die standardmäßig nur klassische DID-Formate (did:key, did:sov) in JWS-Header-\texttt{kid}-Feldern akzeptiert, um did:peer:4-Identifier zu unterstützen – eine Voraussetzung für ML-DSA-65-signierte DID-Exchange-AttachDecorators. Diese neun Module bilden gemeinsam eine Patch-Architektur, die durch sequenzielle Installation beim Plugin-Setup (orchestriert in \texttt{\_\_init\_\_.py}) eine vollständige PQC-Funktionalität in ACA-Py injiziert, ohne dass Änderungen an Controllern, Admin-API-Endpunkten oder externen Business-Logic-Schichten erforderlich sind.

% \paragraph{Deployment und Konfiguration}'
% - Plugin-Installation (Dockerfile.acapy-base-pqc) \\
% - Agent-Konfiguration \\
% - Umgebungsvariablen \\
% - Verifikation der Installation

%   - Umgebungsvariablen (OQS\_DISABLE=0) \\
%   - Verifikation der Installation

% Das Deployment des PQC-Plugins erfolgt über die Python-Package-Infrastruktur (siehe Abbildung~\ref{fig:pqc_didpeer4_fm_directory_structure_Deployment}). Das Modul \texttt{setup.py} (Listing~\ref{lst:setup.py}) definiert das Plugin als installierbare Python-Distribution mit Entry-Point-Registration (\texttt{aries\_cloudagent.plugins = pqc\_didpeer4\_fm}), sodass ACA-Py das Plugin beim Start automatisch über setuptools-Discovery erkennt und die \texttt{setup()}-Funktion aus \texttt{\_\_init\_\_.py} aufruft. Die Plugin-Aktivierung erfolgt durch den Kommandozeilenparameter \texttt{--plugin pqc\_didpeer4\_fm} beim Agent-Start oder durch entsprechende Konfiguration in docker-compose.yml-Dateien. Das \texttt{README.md} (Listing~\ref{lst:README.md}) dokumentiert die Installation (\texttt{pip install -e .}), Konfigurationsparameter und den transparenten Integrationsmechanismus, der keine API-Änderungen erfordert - existierende Workflows wie \texttt{POST /out-of-band/create-invitation} mit Parameter \texttt{use\_did\_method: "did:peer:4"} erzeugen automatisch PQC-fähige DIDs anstelle klassischer ED25519-basierter Identifikatoren. Die Verifikation erfolgt über die Admin-API-Endpoint \texttt{GET /wallet/did}, die für PQC-DIDs erweiterte Metadaten (\texttt{pqc\_enabled: true}, \texttt{signature\_algorithm: "ml-dsa-65"}, \texttt{key\_agreement\_algorithm: "ml-kem-768"}) zurückgibt.

% \begin{figure}[H]
%   \flushleft
%   \caption{Verzeichnisstruktur des Projekts}
%   \label{fig:pqc_didpeer4_fm_directory_structure_Deployment}
% \dirtree{%
% .1 pqc\_didpeer4\_fm/.
% .2 pqc\_didpeer4\_fm/.
% .3 v1\_0/.
% .4 askar\_pqc\_patch.py.
% .4 base\_manager\_patch.py.
% .4 connection\_target\_patch.py.
% .4 key\_type\_patches.py.
% .4 key\_types.py.
% .4 liboqs\_wrapper.py.
% .4 monkey\_patches.py.
% .4 multicodec\_patch.py.
% .4 pqc\_didcomm\_v1.py.
% .4 pqc\_multicodec.py.
% .4 pqc\_multikey.py.
% .4 pqc\_peer4\_creator.py.
% .4 pqc\_peer4\_resolver.py.
% .4 validator\_patch.py.
% .4 wallet\_patch.py.
% .3 \_\_init\_\_.py.
% .2 \textbf{README.md}.
% .2 \textbf{setup.py}.
% }
% \begin{flushleft}
%     \textit{Anmerkung.} Eigene Darstellung.
% \end{flushleft}
% \end{figure}

% \textbf{Plugin-Installation}
%   - Weiterentwicklung vom Dockerfile.acapy-base zum Dockerfile.acapy-base-pqc + setup.py \\

\paragraph{Dockerfile-Modifikation}

Für die Integration des Plugins wurde das ACA-Py Docker-Base-Image (Listing~\ref{lst:Dockerfile-acapy-base}) aus Iteration~1 Kapitel~\ref{SSI-Agenten} modifiziert. Die Baseline-Architektur von Iteration~1 verwendet einen 2-stufigen Build-Prozess (Poetry-Wheel-Building + Runtime-Installation) mit System-OpenSSL und unterstützt ausschließlich klassische Kryptografie (ED25519, X25519, RSA).

Iteration~2 (Listing~\ref{lst:Dockerfile-acapy-base-pqc}, siehe Abbildung~\ref{fig:Iteration2_Acapy_Multi_Stage_Build}) erweitert die Architektur auf einen 4-stufigen Multi-Stage-Build: Stage~1 kompiliert OpenSSL~3.5.4 mit FIPS-Modul und nativer ML-KEM/ML-DSA-Unterstützung, Stage~2 baut liboqs~0.14.0 als Shared Library, Stage~3 bleibt identisch zu Iteration~1 (Poetry-basiertes ACA-Py-Wheel), und Stage~4 integriert alle Artefakte durch \texttt{COPY --from}-Direktiven aus den Builder-Stages. Die Runtime-Stage überschreibt System-OpenSSL-Symlinks mittels \texttt{ln -sf}, aktualisiert Shared-Library-Pfade via \texttt{ldconfig}, importiert das PQC-Root-CA-Zertifikat in den System-Trust-Store (\texttt{update-ca-certificates}), und installiert das \texttt{pqc\_didpeer4\_fm}-Plugin mithilfe von pip direkt in das Container-Image.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Iteration2_Acapy_Multi_Stage Build.png}
    \caption{ACA-Py Multi-Stage Build Dockerfile mit PQC-Integration (Iteration 2)}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Iteration2_Acapy_Multi_Stage_Build}
\end{figure}

\paragraph{Deployment in docker-compose.yml}

Das Deployment der PQC-fähigen SSI-Agenten erfolgt innerhalb der in Iteration 1 (Kapitel~\ref{Docker Orchestrierung der Gesamtarchitektur}) entwickelten docker-compose.yml-Orchestrierung, deren Evolution vom klassischen Setup (Iteration~1, Listing~\ref{lst:docker-compose.yml-SSI-Agenten}) zur PQC-Integration (Iteration~2, Listing~\ref{lst:docker-compose.yml-SSI-Agenten-mit-acapy-base-pqc-und-plugin}) zwei zentrale Anpassungen umfasst. Während die Iteration~1-Konfiguration noch das klassische ACA-Py-Base-Image ohne PQC-Unterstützung und Plugin-Aktivierung verwendet, wurde in Iteration~2 im Rahmen der ersten Anpassung die docker-compose.yml so modifiziert, dass alle drei Agent-Services (issuer, holder, verifier) das neue \texttt{acapy-base-pqc}-Image nutzen in welchem das \texttt{pqc\_didpeer4\_fm}-Plugin enthalten ist. Diese Änderung kann durch den Vergleich von \autoref{fig:Docker-Compose-Übersicht-Iteration-2} mit \autoref{fig:Docker-Compose-Übersicht-Iteration-1} nachvollzogen werden.

Die zweite Anpassung erweitert die \texttt{command}-Direktive aller drei Agenten um den Parameter \texttt{--plugin pqc\_didpeer4\_fm}, der beim Agent start das \texttt{pqc\_didpeer4\_fm}-Plugin lädt.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{docker_compose_übersicht_pqc_plugin.png}
    \caption{Docker-Compose-Übersicht der Iteration 2 Architektur}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Docker-Compose-Übersicht-Iteration-2}
\end{figure}

\subsubsection{Formative Evaluation}

\paragraph{Validierung des Plugin-Ladevorgangs bei Agent-Start}
% - Issuer Agent Boot Log Iteration 1 vs Iteration 2 \\

% Listing~\ref{lst:Issuer-Agent-Boot-Logs} vs. Listing~\ref{lst:Issuer-Agent-Boot-Logs-mit-PQC-Plugin}

Die erste formative Evaluationsmaßnahme bestand in der Validierung des korrekten Plugin-Ladevorgangs beim Start eines ACA-Py-Agenten. Dieser Test diente der Sicherstellung, dass die PQC-Integration transparent und ohne Beeinträchtigung der Standard-ACA-Py-Funktionalität erfolgt.

Listing~\ref{lst:Issuer-Agent-Boot-Logs} zeigt den Boot-Prozess eines Standard-ACA-Py-Agenten (Version 1.3.2) ohne PQC-Plugin. Nach der Registrierung der Default- und Askar-Plugins wird direkt mit der Ledger-Konfiguration und Wallet-Initialisierung fortgefahren.

Im Vergleich dazu zeigt Listing~\ref{lst:Issuer-Agent-Boot-Logs-mit-PQC-Plugin} den erweiterten Boot-Prozess mit geladenem \texttt{pqc\_didpeer4\_fm}-Plugin. Zwischen der Askar-Plugin-Registrierung und der Ledger-Konfiguration erfolgt nun die Plugin-Initialisierung mit mehreren charakteristischen Schritten:

\begin{enumerate}
\item \textbf{Askar-Patching:} Die \texttt{\_create\_keypair}-Funktion wird durch eine PQC-fähige Variante ersetzt, die ML-DSA-65 und ML-KEM-768 unterstützt. Zusätzlich werden \texttt{Session}-Methoden (\texttt{insert\_key}, \texttt{fetch\_key}, \texttt{update\_key}) und \texttt{AskarWallet.assign\_kid\_to\_key()} gepatcht.
\item \textbf{KeyType-Registry-Erweiterung:} Die neuen Schlüsseltypen \texttt{ml-dsa-65} und \texttt{ml-kem-768} werden in der ACA-Py KeyTypes-Registry registriert und die API-Schemas zur Laufzeit erweitert.
\item \textbf{did:peer:4-Erweiterung:} Die unterstützten Schlüsseltypen für did:peer:4 werden von \texttt{['ed25519', 'x25519']} auf \texttt{['ed25519', 'x25519', 'ml-dsa-65', 'ml-kem-768']} erweitert.
\item \textbf{Multicodec-Patching:} Die \texttt{SupportedCodecs}-Klasse wird für PQC-Multicodec-Präfixe erweitert (ML-DSA-65: \texttt{0xd065}, ML-KEM-768: \texttt{0xe018}).
\item \textbf{DIDComm-Patching:} \texttt{AskarWallet.pack\_message()} und \texttt{unpack\_message()} werden für ML-KEM-768-basierte Verschlüsselung angepasst. Die \texttt{AttachDecorator}-Klasse wird für ML-DSA-65-JWS-Signaturen erweitert.
\item \textbf{Monkey-Patches:} Die \texttt{BaseConnectionManager}-Methoden (\texttt{create\_did\_peer\_4}, \texttt{record\_keys\_for\_resolvable\_did}, etc.) werden durch PQC-fähige Varianten ersetzt.
\end{enumerate}



\paragraph{Validierung der Pluginfunktionalität: Out-of-Band Invitation mit did:peer:4}

% did:peer:4 ==> /out-of-band/create-invitation
% - Admin-API Aufruf: \texttt{POST /out-of-band/create-invitation} mit Parameter \texttt{use\_did\_method: "did:peer:4"} \\

Die zweite formative Evaluationsmaßnahme validierte die Kernfunktionalität des Plugins: die transparente Erstellung von PQC-fähigen did:peer:4-DIDs während des Out-of-Band-Invitation-Prozesses.

Listing~\ref{lst:Iteration2_Validierung-der-Pluginfunktionalität-Wallet-DID-Abfrage-vor-OOB-Invitation} zeigt die initiale Wallet-Abfrage eines frisch gestarteten Issuer-Agenten. Das leere \texttt{results}-Array bestätigt, dass noch keine DIDs im Wallet vorhanden sind.

\refstepcounter{manualListingCounter}
\label{lst:Iteration2_Validierung-der-Pluginfunktionalität-Wallet-DID-Abfrage-vor-OOB-Invitation}
\begin{lstlisting}[language=bash, caption={Iteration 2 - Validierung der Pluginfunktionalität - Wallet DID Abfrage vor Out-of-Band Invitation}, numbers=left, frame=single]
ferris@blockchain-ssi-pqc:~$ curl -X GET https://host.docker.internal:8021/wallet/did | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    15  100    15    0     0   1083      0 --:--:-- --:--:-- --:--:--  1153
{
  "results": []
}
\end{lstlisting}

Anschließend wurde mittels \texttt{POST /out-of-band/create-invitation} mit dem Parameter \texttt{use\_did\_method: "did:peer:4"} eine Einladung erstellt (Listing~\ref{lst:Issuer-Agent-Boot-Logs-mit-PQC-Plugin}). Die API-Response enthält eine vollständige did:peer:4-Langform-DID im \texttt{services}-Array der Invitation, erkennbar am charakteristischen Format \texttt{did:peer:4zQm...:z25g...}.

\refstepcounter{manualListingCounter}
\label{lst:Issuer-Agent-Boot-Logs-mit-PQC-Plugin}
\begin{lstlisting}[language=bash, caption={Iteration 2 - Validierung der Pluginfunktionalität - Out-of-Band Invitation}, numbers=left, frame=single]
ferris@blockchain-ssi-pqc:~$ curl -X POST https://host.docker.internal:8021/out-of-band/create-invitation     -H "Content-Type: application/json"     -d '{
      "handshake_protocols": ["https://didcomm.org/didexchange/1.1"],
      "use_did_method": "did:peer:4",
      "my_label": "Issuer Test"
    }' | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 16198  100 16051  100   147  91160    834 --:--:-- --:--:-- --:--:-- 91514
{
  "state": "initial",
  "trace": false,
  "invi_msg_id": "89e9cc87-318f-49aa-a61a-fc805706cd8d",
  "oob_id": "70998122-5a5b-4020-8b5f-ae5884af20b3",
  "invitation": {
    "@type": "https://didcomm.org/out-of-band/1.1/invitation",
    "@id": "89e9cc87-318f-49aa-a61a-fc805706cd8d",
    "label": "Issuer Test",
    "handshake_protocols": [
      "https://didcomm.org/didexchange/1.1"
    ],
    "services": [
      "did:peer:4zQmYFdntsqaiZcU9PMf4dVshmxyTu5yk3NnkA28VjHqaySm:z25gYmQoBS9XWQbLxdKXKizWUz5MxCWwLc..."
    ]
  },
  "invitation_url": "https://host.docker.internal:8020?oob=eyJAdHlwZSI6ICJodHR..."
}
\end{lstlisting}

Die entscheidende Validierung erfolgt in Listing~\ref{lst:Iteration2_Validierung-der-Pluginfunktionalität-Wallet-DID-Abfrage-nach-OOB-Invitation} durch eine erneute Wallet-Abfrage nach der Invitation-Erstellung. Die Response zeigt nun die automatisch generierte PQC-DID mit folgenden charakteristischen Merkmalen:

\begin{itemize}
\item \textbf{Dual-Key-Struktur:} Das \texttt{key\_type}-Feld weist den Wert \texttt{ml-dsa-65} auf, während die Metadata zusätzlich \texttt{kem\_verkey} (ML-KEM-768) enthält. Dies bestätigt die erfolgreiche Implementierung der Hybrid-Kryptografie mit getrennten Schlüsseln für digitale Signaturen und Schlüsselvereinbarung.
\item \textbf{PQC-Metadata:} Die Metadaten enthalten explizite Marker (\texttt{pqc\_enabled: true}, \texttt{signature\_algorithm: "ml-dsa-65"}, \texttt{key\_agreement\_algorithm: "ml-kem-768"}), die eine eindeutige Identifikation PQC-fähiger DIDs zur Laufzeit ermöglichen.
\item \textbf{Key Identifier:} Das \texttt{kem\_key\_kid}-Feld referenziert den KEM-Schlüssel über den DID-URL-Fragment-Identifier \texttt{\#key-1}, was der did:peer:4-Spezifikation entspricht, bei der Verification-Methods sequenziell nummeriert werden (\texttt{\#key-0} für Authentication, \texttt{\#key-1} für Key Agreement).
\end{itemize}

\refstepcounter{manualListingCounter}
\label{lst:Iteration2_Validierung-der-Pluginfunktionalität-Wallet-DID-Abfrage-nach-OOB-Invitation}
\begin{lstlisting}[language=bash, caption={Iteration 2 - Validierung der Pluginfunktionalität - Wallet DID Abfrage nach Out-of-Band Invitation}, numbers=left, frame=single]
ferris@blockchain-ssi-pqc:~$ curl -X GET https://host.docker.internal:8021/wallet/did | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 17778  100 17778    0     0  1655k      0 --:--:-- --:--:-- --:--:-- 1736k
{
  "results": [
    {
      "did": "did:peer:4zQmYFdntsqaiZcU9PMf4dVshmxyTu5yk3NnkA28VjHqaySm:z25gYmQoBS9XWQbLxdKXKizWUz5MxCWwLc...",
      "verkey": "2BvJSsMeLjejWKygFBC1qFPLqUvvTzfed7y2Btp...",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "invitation_reuse": "true",
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmYFdntsqaiZcU9PMf4dVshmxyTu5yk3NnkA28VjHqaySm:z25gYmQoBS9XWQbLxdKXKizWUz5MxCWwLc...D6SUGP43VJWg#key-1",
        "kem_verkey": "h6ngVfG9n2qF1SY5gM3DaDhK9iiwhvnW555QtodD1sgvEcg5...",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    }
  ]
}
\end{lstlisting}



% ==> /wallet/did um zu schauen was personalisiertes

% LOGS anzeigen lassen vom Webserver und vom Agenten selbst

    %    - Überprüfung der Plugin-Registrierung in Logs \\
    %    - Testaufruf einer gepatchten Funktion (z.\,B. DID-Erstellung)
%   → FF1: Blockchain-DID-Registry funktioniert? \\
%   → FF2: Alle PQC-Algorithmen korrekt integriert? \\
%   → FF3: Performance-Vergleich klassisch vs. PQC \\
%   → FF4: Algorithmus-Wechsel ohne Code-Änderung möglich? \\
%    \\
%      - Funktionalitätstests (UC1-UC7) \\
%      - Kryptografische Validierung (Signatur-Verifikation)

% https://dspace.bracu.ac.bd/xmlui/bitstream/handle/10361/25158/24141271,%2020101496,%202010360,%2020101053_CSE.pdf?sequence=1&isAllowed=y#cite.0@indyGithub

\subsubsection{Finales Artefakt}

Das finale Artefakt der zweiten Iteration repräsentiert einen funktionsfähigen SSI-Prototypen mit vollständiger Post-Quantum-Kryptografie-Integration auf Application-Layer-Ebene. Die Architektur vereint die in Iteration~1 etablierte Transport-Layer-Sicherung mittels PQC-Sidecar-Proxies mit einer tiefgreifenden Anwendungsschicht-Integration durch das entwickelte \texttt{pqc\_did\_peer4\_fm}-Plugin.

Die Kernkomponente bildet das ACA-Py-Plugin mit dreischichtiger Architektur. Die Kryptografie-Abstraktionsschicht kapselt native \texttt{liboqs}-Operationen und exponiert eine Python-API für ML-DSA-65 und ML-KEM-768. Die DID-Verarbeitungsschicht orchestriert Generierung, Auflösung und Kodierung PQC-fähiger \texttt{did:peer:4}-Identifikatoren. Die Integration-Patching-Schicht realisiert transparentes Monkey-Patching kritischer ACA-Py-Kernfunktionen ohne Modifikation des Framework-Quellcodes.

Hinsichtlich der in Kapitel~4.2.1 definierten Designziele erfüllt das finale Artefakt sämtliche Anforderungen. Das Designziel zu FF1 (Systemarchitektur \& Compliance) wird durch die native Unterstützung quantenresistenter Signaturen in den DID-Dokumenten adressiert, wodurch die Authentizität von Identitätsnachweisen unabhängig vom Transportkanal langfristig gegenüber Quantencomputer-Angriffen gewährleistet bleibt. Das Designziel zu FF2 (Algorithmenauswahl \& Sicherheitsbewertung) manifestiert sich in der erfolgreichen Integration von ML-DSA-65 für digitale Signaturen innerhalb der \texttt{did:peer:4}-Strukturen, was die praktische Machbarkeit von PQC-Signaturen in dezentralen Identifikatoren nachweist. Das Designziel zu FF3 (Kryptografische Agilität) wird durch die Erweiterung der Multicodec-Registry um provisorische Präfixe für ML-DSA-65 (\texttt{0xd065}) und ML-KEM-768 (\texttt{0xe018}) sowie die abstrahierte Kryptografie-Schicht realisiert, welche die Koexistenz klassischer und post-quanten Verfahren innerhalb der DID-Methoden ermöglicht.

Das Multi-Stage-Build-Dockerfile integriert alle Abhängigkeiten in ein kohärentes Container-Image: Stage~1 kompiliert OpenSSL~3.5.4 mit nativer ML-KEM/ML-DSA-Unterstützung, Stage~2 baut \texttt{liboqs}~0.14.0, Stage~3 generiert das ACA-Py-Wheel, und Stage~4 fusioniert alle Artefakte in ein produktionsfähiges Runtime-Image. Die formative Evaluation validierte die funktionale Korrektheit durch erfolgreiche Plugin-Registrierung beim Agent-Start sowie korrekte Generierung von \texttt{did:peer:4}-Long-Form-DIDs mit PQC-Schlüsselmaterial im Out-of-Band-Invitation-Workflow.












\newpage
\section{Summative Evaluation} \label{sec:Summative Evaluation}
        - Evaluationsmethodik (FEDS)


Im Rahmen der summativen Evaluation wurde das finale Artefakt aus \fixme{KAPITEL} mithilfe des in \ref{sec:Anhang_Summative Evaluation} dargestellten Jupyter Notebooks evaluiert.

KRITIS Szenario ...  Ziel der Evaluation ist die Validierung der funktionalen Anforderungen gemäß Kapitel~\ref{sec:Funktionale Anforderungen}.

- Erst Initialisierung des Artefakts durch:

==> \ref{sec:Anhang_Teil1-Setup-Verbindungstests}
    Variablendeklaration und Helper Funktionen ==> Listing~\ref{lst:Jupyter-Notebook-Cell-1}

    Infrastrukturcheck und Zeigen der Ledgerinitialisierung durch Abruf der Ledgertransaktionen (Validator-Node-Registrierung) ==> Listing~\ref{lst:Jupyter-Notebook-Cell-2-output}

==> \ref{sec:Anhang_Teil2-DID-Setup-Ledger-Registration-KRITIS-Identitäten}
    - ANlegen von Issuer DID lokal im acapy agent ==> Listing~\ref{lst:Jupyter-Notebook-Cell-3-output}
    - Registrieren als ENDORSER auf Ledger ==> Listing~\ref{lst:Jupyter-Notebook-Cell-4-output}
    - Walletansicht indy ed25519 posted mit true

\subsection{Validierung der funktionalen Anforderungen}

\subsubsection{Issuer Discovery}

Die funktionale Anforderung FR1 fordert, dass das System die Auffindbarkeit von publizierten Credential-Schemata des Issuers digitaler Identitätsnachweise ermöglichen muss. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines dreiphasigen, Ledger-basierten Discovery-Mechanismus demonstriert (Listing~\ref{lst:Jupyter-Notebook-Cell-8} und Listing~\ref{lst:Jupyter-Notebook-Cell-8-output}). 

Phase 1 extrahiert alle TRUST\_ANCHOR-Identitäten (Role \texttt{'101'}) aus NYM-Transaktionen des Domain Ledgers, wobei im KRITIS-Szenario der Issuer \enquote{Energienetzbetreiber} mit DID \texttt{9pbXiFBZZGwXKp61HQBz3J} identifiziert wird (Listing~\ref{lst:Jupyter-Notebook-Cell-8-output}, Zeilen 7--17). 

Phase 2 verifiziert sechs kryptographische Eigenschaften (DID-Identifier, Ed25519-Verkey, TRUST\_ANCHOR-Role, Endorser, On-Ledger-Aktivitäten, Registrierungszeitpunkt) mittels der Funktion \texttt{verify\_issuer\_identity()}, wobei für den identifizierten Issuer alle Eigenschaften erfolgreich validiert werden (Listing~\ref{lst:Jupyter-Notebook-Cell-8-output}, Zeilen 24--29). 

Phase 3 filtert SCHEMA-Transaktionen nach dem Schema-Namen \texttt{kritis\_emergency\_maintenance\_cert}, extrahiert den Issuer-DID aus dem Schema-Identifier-Format \texttt{<issuer\_did>:2:<schema\_name>:<version>} und führt eine Cross-Referenzierung mit den TRUST\_ANCHOR-Identitäten durch (Listing~\ref{lst:Jupyter-Notebook-Cell-8-output}, Zeilen 39--60).


\subsubsection{Connection Creation}

Die funktionale Anforderung FR2 fordert, dass das System Verbindungen zwischen den Akteuren des SSI-Ökosystems etablieren muss. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines dreiphasigen Out-of-Band-Invitation-Protokolls mit did:peer:4-basierter Post-Quantum-Kryptographie demonstriert (Listing~\ref{lst:Jupyter-Notebook-Cell-9}, Listing~\ref{lst:Jupyter-Notebook-Cell-9-output}, Listing~\ref{lst:Jupyter-Notebook-Cell-10} und Listing~\ref{lst:Jupyter-Notebook-Cell-10-output}).

Phase~1 implementiert einen Pre-Check existierender Connections via \texttt{GET /connections} auf beiden Agenten, um redundante Connection-Erstellungen zu vermeiden, wobei im KRITIS-Szenario keine existierenden Connections gefunden werden und eine neue Etablierung ausgelöst wird (Listing~\ref{lst:Jupyter-Notebook-Cell-9-output}, Zeilen~5--10).

Phase~2 realisiert die Connection-Etablierung mittels Aries RFC~0434 Out-of-Band Protocol: Der Inviter erstellt eine Invitation mit \texttt{POST /out-of-band/create-invitation} unter Verwendung von \texttt{use\_did\_method: "did:peer:4"}, wobei die Response eine \texttt{invitation\_msg\_id} als eindeutigen Identifier enthält (Listing~\ref{lst:Jupyter-Notebook-Cell-9-output}, Zeile~14). Der Invitee akzeptiert die Invitation via \texttt{POST /out-of-band/receive-invitation}, wodurch das DIDComm DIDExchange-Protokoll initiiert und did:peer:4-DIDs mit ML-DSA-65-Schlüsselmaterial generiert werden (Listing~\ref{lst:Jupyter-Notebook-Cell-12-output} zeigt die resultierenden DIDs mit Metadata \texttt{pqc\_enabled: true}, \texttt{signature\_algorithm: ml-dsa-65}, \texttt{key\_agreement\_algorithm: ml-kem-768}). Der Inviter identifiziert seine Connection anhand der \texttt{invitation\_msg\_id} durch Iteration über alle Connections, wobei die erfolgreiche Zuordnung mit übereinstimmender \texttt{invitation\_msg\_id} und State \texttt{active} validiert wird (Listing~\ref{lst:Jupyter-Notebook-Cell-9-output}, Zeilen~23--27).

Phase~3 validiert den Connection-Status durch Abruf detaillierter Connection-Informationen via \texttt{GET /connections/\{conn\_id\}} auf beiden Seiten, wobei die Konsistenz durch Vergleich der \texttt{invitation\_msg\_id}, komplementäre \texttt{their\_role}-Werte (\texttt{inviter}/\texttt{invitee}) und beidseitigen State \texttt{active} verifiziert wird (Listing~\ref{lst:Jupyter-Notebook-Cell-9-output}, Zeilen~31--45). Die Connection-Übersicht (Listing~\ref{lst:Jupyter-Notebook-Cell-11-output}) gruppiert Connections anhand der \texttt{invitation\_msg\_id} und zeigt zwei aktive Connection-Paare: Issuer<-->Holder (Connection~Group~1) und Holder<-->Verifier (Connection~Group~2), wodurch die vollständige Konnektivität des SSI-Dreiecks validiert wird.

\subsubsection{Credential Creation}

Die funktionale Anforderung FR3 fordert, dass das System Funktionalität zur Erstellung und Ausstellung digitaler Credentials bereitstellen muss. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines mehrstufigen Credential-Issuance-Workflows mit Revocation-Registry-Integration demonstriert (Listing~\ref{lst:Jupyter-Notebook-Cell-13}, Listing~\ref{lst:Jupyter-Notebook-Cell-13-output} und Listing~\ref{lst:Jupyter-Notebook-Cell-14-output}).

Der Issuer initiiert die Credential-Ausstellung durch Versenden eines Credential Offers via \texttt{POST /issue-credential-2.0/send-offer} mit einer Credential Preview, die neun KRITIS-spezifische Attribute enthält (Identität: \texttt{first\_name}, \texttt{name}, \texttt{organisation}; Berechtigung: \texttt{cert\_type}, \texttt{facility\_type}, \texttt{security\_clearance\_level}; Zeitgültigkeit: \texttt{epoch\_valid\_from}, \texttt{epoch\_valid\_until}; Rolle: \texttt{role}), wobei die Ausstellung über die in FR2 etablierte Connection (\texttt{connection\_id}) und die in FR1 identifizierte Credential Definition (\texttt{cred\_def\_id}) erfolgt (Listing~\ref{lst:Jupyter-Notebook-Cell-13}, Zeilen~8--31). Die Response enthält eine Exchange ID zur Nachverfolgung des Issuance-Prozesses, wobei der initiale State \texttt{offer-sent} den erfolgreichen Versand bestätigt (Listing~\ref{lst:Jupyter-Notebook-Cell-13-output}, Zeilen~3--5).

Der Holder akzeptiert das Credential Offer automatisch (\texttt{auto-store=true} Konfiguration), wodurch das Aries RFC~0453 Issue Credential v2.0 Protocol den vollständigen State-Machine-Durchlauf (\texttt{offer-sent} → \texttt{request-sent} → \texttt{credential-issued} → \texttt{done}) ausführt und das Credential im Holder Wallet persistiert. Nach einer Wartezeit von 5 Sekunden (Listing~\ref{lst:Jupyter-Notebook-Cell-13}, Zeile~55) zeigt der Status-Check auf Issuer-Seite den finalen State \texttt{done} (Listing~\ref{lst:Jupyter-Notebook-Cell-13-output}, Zeile~11), während der Holder-Wallet-Abruf via \texttt{GET /credentials} das gespeicherte Credential mit Referent \texttt{39ac5fc4-efc2-45eb-9a21-01c589757b65} und allen neun Attributen bestätigt (Listing~\ref{lst:Jupyter-Notebook-Cell-13-output}, Zeilen~14--22).

Die Revocation-Registry-Integration extrahiert zwei kritische Identifier aus der Issuer-Exchange-Response: Die Revocation Registry ID (\texttt{9pbXiFBZZGwXKp61HQBz3J:4:...:CL\_ACCUM:...}) identifiziert die auf dem Indy Ledger publizierte Revocation Registry (Type~113 REVOC\_REG\_DEF Transaction), während die Credential Revocation ID (\texttt{1}) die Position des Credentials im Revocation-Accumulator spezifiziert (Listing~\ref{lst:Jupyter-Notebook-Cell-13-output}, Zeilen~26--28). Diese IDs werden für den Revocation-Workflow (FR5) benötigt und demonstrieren die Integration von Credential Issuance und Revocation Management im Gesamtsystem.

Die Holder-Credentials-Übersicht (Listing~\ref{lst:Jupyter-Notebook-Cell-14-output}) zeigt das vollständig ausgestellte KRITIS-Notfall-Wartungszertifikat mit allen Attributen, dem Schema-Identifier \texttt{...kritis\_emergency\_maintenance\_cert:1.1} aus FR1, der Credential-Definition-ID aus dem Schema-basierten Discovery-Prozess und dem initialen Revoked-Status \texttt{False}, der die Gültigkeit des Credentials bestätigt (Zeilen~6--19). Der Issuer Credential Registry Check via \texttt{GET /issue-credential-2.0/records} validiert die serverseitige Persistierung des Exchange Records (Listing~\ref{lst:Jupyter-Notebook-Cell-13-output}, Zeilen~35--38), wobei die Verfügbarkeit des Records die Aktivierung des \texttt{--preserve-exchange-records}-Flags bestätigt, das für Audit-Zwecke und Revocation-Management in KRITIS-Kontexten erforderlich ist.

\subsubsection{Verification with Credentials}

Die funktionale Anforderung FR4 fordert, dass das System einen Verifikationsprozess zwischen Identity Holder, Verifier und Blockchain-basierter Verifiable Data Registry (VDR) durch Validierung eines Identitätsnachweises ermöglichen muss. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines vierstufigen Privacy-Preserving-Verification-Workflows mit Zero-Knowledge-Proofs, Revocation-Detection und Zeitgültigkeitsprüfung demonstriert (Listing~\ref{lst:Jupyter-Notebook-Cell-15} bis Listing~\ref{lst:Jupyter-Notebook-Cell-18-Output}).

Der Verifier initiiert den Verifikationsprozess durch Versenden eines Proof Requests via \texttt{POST /present-proof-2.0/send-request} mit einer Indy-Proof-Request-Struktur, die fünf offengelegte Attribute (\texttt{requested\_attributes}: \texttt{cert\_type}, \texttt{facility\_type}, \texttt{epoch\_valid\_from}, \texttt{epoch\_valid\_until}, \texttt{role}) und ein Zero-Knowledge-Predicate (\texttt{requested\_predicates}: \texttt{security\_clearance\_level >= 2}) fordert, während drei Identitätsattribute (\texttt{first\_name}, \texttt{name}, \texttt{organisation}) durch Selective Disclosure geschützt bleiben (Listing~\ref{lst:Jupyter-Notebook-Cell-15}, Zeilen~21--56). Alle Attribute und Predicates enthalten eine \texttt{non\_revoked}-Constraint mit Zeitintervall \texttt{\{from: 0, to: current\_timestamp\}}, die eine Ledger-basierte Echtzeit-Revocation-Prüfung gegen die Revocation Registry erzwingt (Zeilen~24, 30, 36, 42, 48, 54). Der initiale State \texttt{request-sent} bestätigt die erfolgreiche Übermittlung des Proof Requests über die in FR2 etablierte Connection (Listing~\ref{lst:Jupyter-Notebook-Cell-15-Output}, Zeilen~11--12).

Der Holder empfängt den Proof Request (State \texttt{request-received}) und ruft via \texttt{GET /present-proof-2.0/records/\{pres\_ex\_id\}/credentials} alle Credentials ab, die die Proof-Request-Anforderungen erfüllen, wobei die Schema-ID und Credential-Definition-ID aus FR1 und FR3 zur Filterung verwendet werden (Listing~\ref{lst:Jupyter-Notebook-Cell-16-Output}, Zeilen~4--19). Der Holder konstruiert ein \texttt{requested\_credentials}-Objekt durch Mapping der fünf Attribute-Referents (\texttt{attr1\_referent} bis \texttt{attr5\_referent}) und des Predicate-Referents (\texttt{pred1\_clearance}) auf die Credential-ID \texttt{39ac5fc4...}, wobei Attribute mit \texttt{revealed: true} gekennzeichnet werden, während das Predicate ohne Offenlegung des Attributwerts evaluiert wird (Listing~\ref{lst:Jupyter-Notebook-Cell-17-Output}, Zeilen~6--11). Der Versand der Presentation via \texttt{POST .../send-presentation} erzeugt einen Zero-Knowledge-Proof, der kryptographisch beweist, dass der Holder ein Credential mit den geforderten Attributen und erfülltem Predicate besitzt, ohne die unrevealed Attribute offenzulegen (Zeilen~18--26).

Der Verifier empfängt die Presentation (State \texttt{done}, \texttt{verified: true}) und extrahiert die revealed Attributes durch dreistufiges Mapping: (1) Abruf der Attribute-Namen aus \texttt{by\_format.pres\_request.indy.requested\_attributes}, (2) Abruf der Attribut-Werte aus \texttt{by\_format.pres.indy.requested\_proof.revealed\_attrs}, (3) Konstruktion eines Name-Value-Mappings (Listing~\ref{lst:Jupyter-Notebook-Cell-18}, Zeilen~30--44), wodurch fünf offengelegte Attribute (\texttt{cert\_type: Notfall-Wartungsberechtigung}, \texttt{facility\_type: Umspannwerk Nord-Ost}, \texttt{epoch\_valid\_from: 1765026000}, \texttt{epoch\_valid\_until: 1765033200}, \texttt{role: Notfalltechniker}) extrahiert werden (Listing~\ref{lst:Jupyter-Notebook-Cell-18-Output}, Zeilen~5--9). Die drei Identitätsattribute bleiben durch Selective Disclosure geschützt und werden als \enquote{NICHT offengelegt (Zero-Knowledge-Proof)} ausgewiesen, wodurch Privacy by Design gemäß DSGVO Art.~25 realisiert wird (Listing~\ref{lst:Jupyter-Notebook-Cell-18-Output}, Zeilen~11--14).

Die Blockchain-basierte Revocation-Prüfung validiert den Credential-Status durch Vergleich des Indy-Proof-Timestamps mit dem Revocation-Registry-Delta auf dem Ledger, wobei State \texttt{done} und \texttt{verified: true} bestätigen, dass das Credential zum Verifikationszeitpunkt nicht revoked war (Listing~\ref{lst:Jupyter-Notebook-Cell-18-Output}, Zeile~18). Die Zeitgültigkeitsprüfung vergleicht einen aktuellen Epoch-Timestamp (Beispielwert \texttt{1765029600}) mit den extrahierten Zeitgrenzen (\texttt{epoch\_valid\_from: 1765026000}, \texttt{epoch\_valid\_until: 1765033200}), wobei die Bedingung \texttt{epoch\_valid\_from <= current\_epoch <= epoch\_valid\_until} die zeitliche Gültigkeit bestätigt (Zeilen~20--27). Die Zero-Knowledge-Predicate-Auswertung extrahiert das erfüllte Predicate \texttt{security\_clearance\_level >= 2} aus \texttt{requested\_proof.predicates}, wobei die exakte Clearance-Stufe (ob Ü2 oder Ü3) durch die ZKP-Eigenschaft verborgen bleibt (Zeilen~29--33).

Die finale Zugriffsentscheidung kombiniert drei Validierungsergebnisse in einer logischen UND-Verknüpfung (\texttt{not is\_revoked AND is\_time\_valid AND has\_required\_clearance}), die im demonstrierten KRITIS-Szenario zum Ergebnis \enquote{ZUGANG GEWÄHRT} führt, da alle Bedingungen erfüllt sind (Listing~\ref{lst:Jupyter-Notebook-Cell-18-Output}, Zeilen~36--42). Die Verifiable Data Registry (VDR)-Integration manifestiert sich durch drei Ledger-basierte Validierungsschritte: (1) Schema-Validation via Schema-ID aus FR1, (2) Credential-Definition-Validation via Cred-Def-ID, (3) Revocation-Registry-Validation via \texttt{non\_revoked}-Constraint, wodurch alle kryptographischen Artefakte (Schema, Cred-Def, RevReg-Def, RevReg-Delta) vom Hyperledger Indy Ledger abgerufen und validiert werden.

\subsubsection{Credential Revocation}

Die funktionale Anforderung zur Credential Revocation fordert, dass das System die Ungültigkeitserklärung ausgestellter Credentials ermöglichen muss, wobei Verifier die Gültigkeit kryptographisch überprüfen können. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines dreiphasigen Revocation-Workflows demonstriert: Registry-Management, Two-Phase-Revocation (Staging + Publishing) sowie Non-Revocation-Proof Verification.

Phase~1 implementiert die Verwaltung von Revocation Registries durch den Issuer. Listing~\ref{lst:Jupyter-Notebook-Cell-19} demonstriert den Abruf aktiver Revocation Registries via \texttt{GET /revocation/registries/created?state=active}. Das Output (Listing~\ref{lst:Jupyter-Notebook-Cell-19-output}) zeigt zwei aktive Registries mit jeweils 100 Credential-Kapazität. Jede Registry enthält kritische Metadaten: \texttt{rev\_reg\_id} (eindeutige Registry-Identifier), \texttt{tails\_hash} (kryptographischer Hash der Tails-File für Zero-Knowledge Non-Revocation-Proofs), \texttt{tails\_local\_path} (lokaler Speicherort der Tails-File) sowie \texttt{issuer\_did} (did:indy des Issuers). Die Tails-File ist essentiell für die kryptographische Accumulator-basierte Revocation-Prüfung nach dem CL-Signature-Schema und wird vom Holder benötigt, um Non-Revocation-Proofs zu generieren.

Phase~2 realisiert die Two-Phase-Revocation durch Staging und Ledger-Publishing. Listing~\ref{lst:Jupyter-Notebook-Cell-20} demonstriert die Staging-Phase via \texttt{POST /revocation/revoke} mit \texttt{publish: false}. Die Revocation-Request referenziert das Credential durch \texttt{rev\_reg\_id} (Registry-Identifier) und \texttt{cred\_rev\_id} (Credential-spezifische Revocation-ID, hier: \texttt{"1"}). Das Output (Listing~\ref{lst:Jupyter-Notebook-Cell-20-output}) bestätigt den Status \texttt{"Pending"} -- die Revocation ist lokal staged, jedoch noch nicht auf dem Ledger publiziert. Listing~\ref{lst:Jupyter-Notebook-Cell-21} führt die Publishing-Phase aus via \texttt{POST /revocation/publish-revocations}. Das Output (Listing~\ref{lst:Jupyter-Notebook-Cell-21-output}) zeigt die erfolgreiche Ledger-Transaktion: Typ \texttt{114 (REVOC\_REG\_ENTRY)}, Sequence Number \texttt{14}, mit kryptographischen Accumulator-Updates (\texttt{prevAccum}, \texttt{accum}) und der Liste revokierter Credential-IDs (\texttt{revoked: [1]}). Die Ledger-Transaktion ist durch ED25519-Signatur des Issuers authentifiziert und via Byzantine Fault Tolerance konsistent repliziert.

Phase~3 validiert die Revocation-Enforcement durch Proof-Verification. Listing~\ref{lst:Jupyter-Notebook-Cell-14-output-revocation} zeigt das Holder-Wallet nach Revocation mit \texttt{Revoked Status: True}. Listings~\ref{lst:Jupyter-Notebook-Cell-15-output-revocation} bis \ref{lst:Jupyter-Notebook-Cell-17-output-revocation} demonstrieren den Proof-Request- und Presentation-Workflow: Der Holder wählt das revokierte Credential aus (Auto-Select via \texttt{auto\_present: true}), versucht einen Non-Revocation-Proof zu generieren, jedoch schlägt die Proof-Generierung fehl, da der kryptographische Accumulator das Credential als revoked markiert. Listing~\ref{lst:Jupyter-Notebook-Cell-18-output-revocation} zeigt die finale Verifier-Entscheidung: \texttt{Verified: false}, \texttt{Credential ist REVOKED}, \texttt{ZUGANG VERWEIGERT}. Trotz zeitlicher Gültigkeit (Epoch-Check erfüllt: \texttt{1765026000 <= 1765029600 <= 1765033200}) und erfülltem ZKP-Predicate (\texttt{security\_clearance\_level >= 2}) wird der Zugang verweigert, da die Revocation-Prüfung fehlschlägt. Dies demonstriert die Enforcement-Priorität: Revocation-Status dominiert alle anderen Validierungskriterien und stellt sicher, dass kompromittierte Credentials sofort unwirksam werden -- eine kritische Sicherheitsanforderung für KRITIS-Infrastrukturen.

\subsubsection{Credential Deletion}

Die funktionale Anforderung zur Credential Deletion fordert, dass Holder die Möglichkeit besitzen müssen, Credentials lokal aus ihrem Wallet zu entfernen, wobei diese Operation ausschließlich die lokale Datenhaltung betrifft und vom Ledger-basierten Revocation-Mechanismus zu unterscheiden ist. Die Erfüllung dieser Anforderung an das finale Artefakt wird anhand eines dreiphasigen Deletion-Workflows demonstriert: Pre-Deletion Inventory, Credential Deletion sowie Post-Deletion Verification.

Phase~1 implementiert das Pre-Deletion Inventory durch Abruf aller im Holder-Wallet gespeicherten Credentials (Listing~\ref{lst:Jupyter-Notebook-Cell-22}, Zeilen~7--43). Via \texttt{GET /credentials} werden alle Credential-Metadaten abgerufen, wobei für jedes Credential der \texttt{referent} (eindeutige Wallet-Identifier), \texttt{schema\_id}, \texttt{cred\_def\_id} sowie der Revocation-Status via \texttt{GET /credential/revoked/\{referent\}} ermittelt wird. Das Output (Listing~\ref{lst:Jupyter-Notebook-Cell-22-output}, Zeilen~1--19) zeigt ein Credential mit \texttt{Revoked: True} -- dieses Credential wurde zuvor via Ledger-Revocation ungültig erklärt (Cell~20--21), befindet sich jedoch weiterhin im lokalen Wallet. Die Auflistung aller Attribute (\texttt{security\_clearance\_level: 2}, \texttt{role: Notfalltechniker}, \texttt{facility\_type: Umspannwerk Nord-Ost}) demonstriert, dass revokierte Credentials im Wallet persistieren, bis der Holder sie explizit löscht. Die \texttt{referent}-Identifier werden in der Liste \texttt{credentials\_to\_delete} gespeichert (Zeile~34) für die nachfolgende Deletion-Phase.

Phase~2 realisiert die Credential Deletion durch iterative Deletion aller erfassten Credentials (Listing~\ref{lst:Jupyter-Notebook-Cell-22}, Zeilen~45--75). Für jeden \texttt{credential\_id} wird via \texttt{DELETE /credential/\{credential\_id\}} die lokale Wallet-Entfernung ausgeführt. Die \texttt{api\_delete()}-Hilfsfunktion behandelt HTTP~204~No~Content Responses korrekt (erfolgreiches Löschen ohne Response-Body). Das Output (Listing~\ref{lst:Jupyter-Notebook-Cell-22-output}, Zeilen~21--30) zeigt die erfolgreiche Deletion: \texttt{Gelöscht: 1/1}, wobei die Deletion Summary die Erfolgsrate dokumentiert. Diese Phase demonstriert die Holder-Autonomie über lokale Wallet-Daten -- der Holder kann Credentials unilateral entfernen, ohne Issuer-Interaktion oder Ledger-Transaktion.

Phase~3 validiert die Deletion-Enforcement durch Post-Deletion Verification (Listing~\ref{lst:Jupyter-Notebook-Cell-22}, Zeilen~77--115). Ein erneuter Abruf via \texttt{GET /credentials} bestätigt das leere Wallet: \texttt{Holder Wallet ist jetzt LEER (alle Credentials gelöscht)} (Listing~\ref{lst:Jupyter-Notebook-Cell-22-output}, Zeilen~32--36). Die explizite Unterscheidung zwischen lokaler Deletion und Ledger-Revocation wird durch Hinweise dokumentiert (Listing~\ref{lst:Jupyter-Notebook-Cell-22-output}, Zeilen~38--43): \texttt{Credential ist LOKAL im Wallet gelöscht}, \texttt{Credential ist NICHT auf dem Ledger revoked}, \texttt{Issuer kann das Credential weiterhin sehen}. Diese Differenzierung ist kritisch für das Verständnis des SSI-Sicherheitsmodells: Lokale Deletion entfernt das Credential aus der Holder-Verfügungsgewalt (kein Proof mehr generierbar), jedoch bleibt die Ledger-basierte Revocation-Historie intakt (\texttt{--preserve-exchange-records} beim Issuer aktiv). Für KRITIS-konforme Audit-Trails bedeutet dies: Die Credential-Issuance-History ist unveränderlich auf dem Ledger gespeichert, während der Holder die Privacy-wahrende Möglichkeit besitzt, lokale Credential-Kopien zu entfernen.

%       - KRITIS-Szenarien (Energie, Gesundheit, Wasser)
% \subsection{Performance-Analyse}
%        - Latenz-Messungen (Baseline, PQC, Hybrid) \\
%        - Durchsatz-Analyse \\
%        - Speicher- und Rechenaufwand \\
%        - Skalierbarkeitstest



\subsection{Validierung der KRITIS-Compliance-Anforderungen}

\subsubsection{Einhaltung spezifischer Parameter-Sets für ML-DSA}

Die Compliance-Anforderung zur Einhaltung BSI-konformer ML-DSA Parameter-Sets (NIST Security Strength Category 3 oder 5) wird durch strategische Verwendung zweier Sicherheitsstufen erfüllt: ML-DSA-65 (Category 3) für operationale Signaturen sowie ML-DSA-87 (Category 5) für die Root Certificate Authority. Das finale Artefakt implementiert ML-DSA in drei Schichten: (1)~TLS~1.3 Server-Zertifikate für alle fünf Nginx Sidecar Proxies (hopE-Agenten: Issuer/Holder/Verifier, VON-Network Webserver, Tails Server) werden mittels ML-DSA-65 signiert, konfiguriert via Build-Argument (SIG\_ALG: mldsa65) in den Docker-Infrastruktur-Definitionen (Listing~\ref{lst:docker-compose.yml-DLT-Infrastruktur}, Listing~\ref{lst:docker-compose.yml-Revocation-Registry}, Listing~\ref{lst:docker-compose.yml-SSI-Agenten}), wobei die Root CA als langfristiger Trust-Anchor mit der höheren Sicherheitsstufe ML-DSA-87 geschützt ist (Listing~\ref{lst:Zertifikatserstellungsworkflow}). (2)~did:peer:4 Signing Keys für dezentrale Agent-to-Agent Authentifizierung werden mit ML-DSA-65 generiert (Listing~\ref{lst:key_types.py}). (3)~DIDComm~v1 Authcrypt Message-Signierung nutzt ML-DSA-65 via LibOQS-Integration (Listing~\ref{lst:liboqs_wrapper.py}) für kryptographisch verifizierbare Sender-Authentifizierung in verschlüsselten Nachrichten.

Die erfolgreiche operationale Integration von ML-DSA-65 wird zum einen durch \autoref{fig:Successful-Validation-Issuer-TLS1.3} demonstriert, welches die TLS~1.3 Verbindung zum Issuer-Agenten mit ML-DSA-65 signiertem Server-Zertifikat validiert, und zum anderen durch die Wallet-Übersicht in Listing~\ref{lst:Jupyter-Notebook-Cell-12-output} demonstriert, in welcher alle drei SSI-Agenten key\_type: "ml-dsa-65" für did:peer:4 DIDs nutzen.

\subsubsection{Einhaltung spezifischer Parameter-Sets für ML-KEM}

Diese Anforderung wird durch systemweite Implementierung von ML-KEM-768 (Category 3) erfüllt. Das finale Artefakt implementiert ML-KEM-768 in zwei Schichten: (1)~TLS~1.3 Key Exchange für alle fünf Nginx Sidecar Proxies nutzt ML-KEM-768 konfiguriert via \texttt{DEFAULT\_GROUPS=X25519MLKEM768} in den Docker-Infrastruktur-Definitionen (Listing~\ref{lst:docker-compose.yml-DLT-Infrastruktur}, Listing~\ref{lst:docker-compose.yml-Revocation-Registry}, Listing~\ref{lst:docker-compose.yml-SSI-Agenten}) sowie via \texttt{ssl\_ecdh\_curve X25519MLKEM768} in den Nginx-Konfigurationen (Listing~\ref{lst:nginx-pqc-konfiguration}). (2)~did:peer:4 Key Agreement für dezentrale Agent-to-Agent Verschlüsselung wird mit ML-KEM-768 realisiert (Listing~\ref{lst:key_types.py}), womit DIDComm-Nachrichten mittels Post-Quantum-resistenter Schlüsselkapselung verschlüsselt werden.

Die erfolgreiche operationale Integration von ML-KEM-768 wird zum einen durch \autoref{fig:Successful-Validation-Issuer-TLS1.3} demonstriert, welches die TLS~1.3 Verbindung zum Issuer-Agenten mit ML-DSA-65 signiertem Server-Zertifikat validiert, und zum anderen durch die Wallet-Übersicht in Listing~\ref{lst:Jupyter-Notebook-Cell-12-output} demonstriert, in welcher alle drei SSI-Agenten key\_type: "ml-dsa-65" für did:peer:4 DIDs nutzen.

\subsubsection{Implementierung hybrider Schlüsseleinigung}

Die BSI-Anforderung zur hybriden Schlüsseleinigung (Kombination klassisches Verfahren mit PQC-KEM) wird durch X25519+ML-KEM-768 Hybrid-Modus erfüllt. Das finale Artefakt implementiert hybride Schlüsseleinigung systemweit via \texttt{DEFAULT\_GROUPS=X25519MLKEM768:mlkem768:x25519} in allen Docker-Infrastruktur-Definitionen (Listing~\ref{lst:docker-compose.yml-DLT-Infrastruktur}, Listing~\ref{lst:docker-compose.yml-Revocation-Registry}, Listing~\ref{lst:docker-compose.yml-SSI-Agenten}), wobei die Priorisierung X25519MLKEM768 als primären Hybrid-Modus sicherstellt. Die TLS~1.3 Verbindungen aller fünf Nginx Sidecar Proxies kombinieren elliptische Kurven-Kryptographie (X25519, klassisch) mit gitterbasiertem ML-KEM-768 (Post-Quantum) für Perfect Forward Secrecy, wobei OpenSSL~3.5.4 mit nativer PQC-Unterstützung die kryptographische Verknüpfung beider Shared Secrets via Key Derivation Function durchführt (Listing~\ref{lst:nginx-pqc-konfiguration}). Zusätzlich implementiert did:peer:4 hybride Key Agreement zwischen X25519- und ML-KEM-768-Keys aus DID Documents für DIDComm Message Encryption (Listing~\ref{lst:key_types.py}). Die Fallback-Strategie \texttt{mlkem768:x25519} gewährleistet Interoperabilität mit Peers ohne Hybrid-Unterstützung, wobei reine PQC-Verschlüsselung via ML-KEM-768 Vorrang vor klassischem X25519 hat. Diese Architektur entspricht BSI-TR-02102-1 Kapitel~2.2 und 2.4 zur Absicherung gegen kryptanalytische Durchbrüche sowohl im klassischen als auch im Quantum-Computing-Bereich.

\subsubsection{Bevorzugte Verwendung von TLS 1.3}
  
Die BSI-Empfehlung zur vorrangigen Verwendung von TLS~1.3 wird durch systemweite TLS~1.3-Enforcement erfüllt. Das finale Artefakt erzwingt TLS~1.3 in allen fünf Nginx Sidecar Proxies via \texttt{ssl\_protocols TLSv1.3;} in den Konfigurationsdateien (Listing~\ref{lst:nginx_holder.conf}).

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{summative_evaluation_TLS1.2_error.png}
    \caption{TLS 1.2 Verbindungsversuch zum Issuer-Agenten schlägt fehl (TLS 1.3 Enforcement)}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Summative_Evaluation_TLS1.2_Error}
\end{figure}

\autoref{fig:Summative_Evaluation_TLS1.2_Error} demonstriert das Fehlschlagen des TLS~1.2 Verbindungsversuchs zum Issuer-Agenten.

Für \gls{SzA}

\subsubsection{Protokollierung Sicherheitsrelevanter Ereignisse}

Das finale Artefakt implementiert Protokollierung auf drei Ebenen:

(1)~ACA-Py Agent-Level Logging protokolliert alle Authentifizierungsversuche (Connection Requests, Credential Issuance, Proof Presentations), Zustandsübergänge (State Machines für Issue-Credential/Present-Proof Protokolle) sowie Fehlerzustände via \texttt{--log-level info} in den Docker-Infrastruktur-Definitionen (Listing~\ref{lst:docker-compose.yml-SSI-Agenten}). Listing~\ref{lst:issuer-acapy-agent-logs} demonstriert das Logging sicherheitsrelevanter kryptographischer Events wie ML-DSA-65 Signature Verification, Schlüsselabruf via DID-Resolution und erfolgreiche/fehlgeschlagene DIDcomm-Decryption während des Credential-Issuance-Workflows.

(2)~Nginx Access \& Error Logs auf allen fünf Sidecar Proxies protokollieren TLS-Handshakes, HTTP-Requests sowie fehlgeschlagene Verbindungsversuche, persistiert in Docker Volumes \texttt{nginx-logs} (Listing~\ref{lst:docker-compose.yml-DLT-Infrastruktur}, Listing~\ref{lst:docker-compose.yml-Revocation-Registry}). Listing~\ref{lst:issuer-nginx-logs} demonstriert das Logging der HTTP-Security-Events auf Proxy-Ebene, Connection-Authentifizierung, Request-Authentifizierung via User-Agent, sowie Request-Body-Buffering für PQC-Schlüsselmaterialien.

(3)~VON-Network Ledger Transaction Logs erfassen alle Blockchain-Operationen (NYM-Transaktionen für DID-Registrierung, SCHEMA/CRED\_DEF-Publikationen, REVOC\_REG\_ENTRY für Revocations) mit Zeitstempeln und Sequence Numbers für unveränderlichen Audit-Trail (demonstriert in Listing~\ref{lst:Jupyter-Notebook-Cell-4-output}, Listing~\ref{lst:Jupyter-Notebook-Cell-21-output}).

\subsubsection{Logische Netzsegmentierung}

Das finale Artefakt implementiert strikte Netzwerk-Segmentierung mittels dedizierter Docker Networks: 

(1)~hopE-Agenten nutzen isolierte Networks \texttt{hope-issuer}, \texttt{hope-holder}, \texttt{hope-verifier} pro Agent, wobei nur die zugehörigen Sidecar Proxies Zugriff haben (Listing~\ref{lst:docker-compose.yml-SSI-Agenten}). 

(2)~Shared Network \texttt{von\_sidecarproxy} verbindet ausschließlich die PQC Sidecar Proxies untereinander für Agent-to-Agent Kommunikation, während interne Agent-Container isoliert bleiben. 

(3)~VON-Network Blockchain-Nodes operieren im dedizierten \texttt{von} Network mit separatem \texttt{sidecarproxy} Network für Webserver-Zugriff (Listing~\ref{lst:docker-compose.yml-DLT-Infrastruktur}). 

(4)~Tails Server nutzt isoliertes \texttt{tails-server} Network mit kontrolliertem Zugang via \texttt{von\_sidecarproxy} (Listing~\ref{lst:docker-compose.yml-Revocation-Registry}). 

\autoref{fig:Docker-Compose-Übersicht-Iteration-1} und \autoref{fig:Darstellung-Network-Isolation} demonstrieren die logische Netzwerksegmentierung.

\subsubsection{Datenschutz durch Technikgestaltung (Privacy by Design)}
% Die Architektur realisiert dies durch den konsequenten Verzicht auf die Speicherung personenbezogener Daten (PII) auf dem unveränderlichen Ledger (Off-Chain-Architektur) und die Nutzung von Zero-Knowledge Proofs

Die DSGVO-Anforderung zu Privacy by Design (Art.~25) wird durch architektonische Trennung von öffentlichen Identifikatoren und personenbezogenen Daten erfüllt. Das finale Artefakt realisiert konsequente Off-Chain-Architektur: (1)~Personenbezogene Daten (PII) wie Name, Organisation, Sicherheitsfreigabe werden ausschließlich in verschlüsselten Verifiable Credentials gespeichert, die lokal im Holder-Wallet persistiert sind (demonstriert in Listing~\ref{lst:Jupyter-Notebook-Cell-14-output}), niemals auf dem unveränderlichen Blockchain-Ledger. (2)~Ledger-Transaktionen enthalten ausschließlich kryptographische Identifikatoren (Schema-IDs, Credential-Definition-IDs, Revocation-Registry-IDs) sowie Public Keys, jedoch keine personenbezogenen Attribute (Listing~\ref{lst:Jupyter-Notebook-Cell-4-output}, Listing~\ref{lst:Jupyter-Notebook-Cell-21-output}). (3)~Zero-Knowledge Proofs ermöglichen selektive Offenlegung: Der Verifier erhält nur explizit angeforderte Attribute (\texttt{revealed\_attrs}), während sensible Daten wie Vorname, Nachname, Organisation durch \texttt{unrevealed}-Status geschützt bleiben, sowie Predicate-basierte Proofs (\texttt{security\_clearance\_level >= 2}) ohne Offenlegung exakter Werte (Listing~\ref{lst:Jupyter-Notebook-Cell-18-output-revocation}). Diese Architektur stellt Privacy by Default sicher, da personenbezogene Daten per Design dezentral beim Holder verbleiben und nur kryptographisch verifizierbare Proofs ausgetauscht werden, womit DSGVO Art.~25 Konformität erreicht wird.

\subsubsection{Grundsatz der Datenminimierung}
% Durch den Einsatz von \textit{Pairwise DIDs} (did:peer) für jede Interaktion statt einer globalen ID wird die Korrelierbarkeit von Daten minimiert und Profilbildung technisch unterbunden

Das finale Artefakt addressiert Datenminimierung in drei Dimensionen:

(1)~Pairwise did:peer:4 DIDs eliminieren globale Identifikatoren: Für jede Agent-to-Agent Connection wird eine dedizierte DID generiert (Listing~\ref{lst:Jupyter-Notebook-Cell-12-output} zeigt pro Agent 3--4 verschiedene did:peer:4 DIDs), wodurch Transaktionskorrelation über verschiedene Verifier hinweg technisch unterbunden wird -- ein kompromittierter Verifier kann keine Aktivitäten des Holders bei anderen Verifiern nachverfolgen. 

(2)~Selective Disclosure in Proof Presentations: Der Holder offenbart ausschließlich die vom Verifier angeforderten Attribute (\texttt{revealed\_attrs}: \texttt{cert\_type, facility\_type, epoch\_valid\_from/until, role}), während Identitätsdaten (\texttt{first\_name, name, organisation}) unrevealed bleiben (Listing~\ref{lst:Jupyter-Notebook-Cell-18-output-revocation}, Zeilen~12--19), wodurch nur zweckgebundene Minimaldaten übermittelt werden. 

(3)~Predicate-basierte Zero-Knowledge Proofs reduzieren Datenoffenlegung weiter: Statt exakte Sicherheitsfreigabe-Stufe zu übermitteln, beweist der Holder kryptographisch \texttt{security\_clearance\_level >= 2} ohne Preisgabe, ob Stufe 2 oder 3 vorliegt (Listing~\ref{lst:Jupyter-Notebook-Cell-18-output-revocation}, Zeilen~21--26). 

Diese mehrstufige Datenminimierung verhindert Profilbildung und unnötige Datensammlung, womit DSGVO-konforme Zweckbindung technisch durchgesetzt wird.

\subsubsection{Recht auf Löschung}
% Durch die strikte Trennung von öffentlichen Identifikatoren (Ledger) und privaten Daten (lokale Wallet) ist eine Löschung technisch vollständig realisierbar, indem die lokale Wallet und die zugehörigen kryptografischen Schlüssel vernichtet werden (Crypto-Shredding)

Die DSGVO-Anforderung zum Recht auf Löschung (Art.~17) wird durch strikte Trennung von Ledger-Identifikatoren und Wallet-Daten erfüllt. Das finale Artefakt realisiert vollständige Löschbarkeit personenbezogener Daten mittels Crypto-Shredding: (1)~Lokale Credential-Deletion entfernt Credentials aus dem Holder-Wallet via \texttt{DELETE /credential/\{credential\_id\}}, demonstriert in Listing~\ref{lst:Jupyter-Notebook-Cell-22}: Nach Deletion ist das Wallet leer (\texttt{Holder Wallet ist jetzt LEER}), womit alle personenbezogenen Attribute (Name, Organisation, Sicherheitsfreigabe) irreversibel gelöscht sind. (2)~Crypto-Shredding durch Wallet-Destruction: Da Credentials im Holder-Wallet mit dem Wallet-Key verschlüsselt persistiert sind (\texttt{holder\_wallet\_key}), führt die Vernichtung des Wallet-Keys zur kryptographischen Unlesbarkeit aller Credential-Daten, selbst wenn Backups existieren -- ohne Key ist Decryption unmöglich. (3)~Ledger-Immutabilität ohne Privacy-Verletzung: Während Blockchain-Transaktionen (Schema-IDs, Cred-Def-IDs, Revocation-Entries) unveränderlich auf dem Ledger verbleiben, enthalten diese per Design keine personenbezogenen Daten, sondern ausschließlich kryptographische Identifier, wodurch Art.~17 DSGVO erfüllt wird (Listing~\ref{lst:Jupyter-Notebook-Cell-22-output}, Zeilen~38--43: \texttt{Credential ist LOKAL im Wallet gelöscht, NICHT auf dem Ledger revoked}). Diese Architektur gewährleistet vollständige Löschung von PII bei gleichzeitiger Beibehaltung der Audit-Trail-Integrität für KRITIS-Compliance, womit die Balance zwischen DSGVO-Rechten und regulatorischen Anforderungen erreicht wird.

\subsection{Validierung der Kryptoagilität}

\subsubsection{Transportlayer}

Die Implementierung realisiert Kryptoagilität mithilfe der eingebauten Mechanismen in TLS 1.3, welche explizit entwickelt wurden, um eine modulare Austauschbarkeit kryptografischer Primitive zu gewährleisten. Im Gegensatz zu früheren Protokollversionen entkoppelt TLS 1.3 die Aushandlung von Cipher Suite, Schlüsselaustauschverfahren und Signaturalgorithmen vollständig voneinander. Diese Parameter werden orthogonal ausgehandelt, wodurch jeder Mechanismus unabhängig modifiziert werden kann \cite[S. 26]{rescorla_TransportLayerSecurityTLSProtocolVersion13_2018}.
Diese Trennung wird technisch durch die \texttt{supported\_groups}-Erweiterung realisiert, die es Endpunkten erlaubt, präferierte Schlüsselaustauschverfahren unabhängig vom symmetrischen Verschlüsselungsverfahren (AEAD) auszuhandeln \cite[S. 47]{rescorla_TransportLayerSecurityTLSProtocolVersion13_2018}.

Dieser Ansatz korrespondiert direkt mit den von \textcite[S. 102]{mehrez_CryptoAgilityProperties_2018} identifizierten Kryptoagilitätseigenschaften. Spezifisch adressiert die hier gewählte Architektur die Eigenschaft der \enquote{Extensibility}, die Fähigkeit, neue Algorithmen effizient hinzuzufügen, sowie die \enquote{Removability}, das elegante Außerbetriebnehmen veralteter Verfahren, ohne Gefährdung der Systemintegrität.

Die konkrete Umsetzung dieser Agilität in der vorliegenden Arbeit nutzt diese Protokollstruktur, um eine nahtlose Migration zu ermöglichen. Konkret wurde eine konfigurationsbasierte Algorithm-Fallback-Chain via \texttt{DEFAULT\_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024} (Listing~\ref{lst:Dockerfile-Sidecar-Proxy-nginx}) implementiert. Diese Konfiguration instruiert den TLS-Handshake, primär hybride Verfahren zu nutzen, bietet jedoch eine automatische Rückfalloption (Fallback) auf rein klassische Verfahren (\texttt{x25519}) im Falle einer Inkompatibilität. Damit erfüllt die Lösung die Anforderung der \enquote{Fungibility} nach \textcite[S. 102]{mehrez_CryptoAgilityProperties_2018}, die es Systemen erlaubt, Algorithmen auszutauschen. Ebenfalls wird die erste Eigenschaft nach \textcite[S. 19]{cyberresilienceworkshopseriescommittee_CryptographicAgilityInteroperabilityProceedingsWorkshop_2017} adressiert Algorithmen in Echtzeit basierend auf ihrer kombinierten Sicherheitsfunktion auszuwählen.

\autoref{fig:Summative_Evaluation_TLS1.3_Kryptoagilität} demonstriert den kryptoagilen Fallback-Prozess von X25519+ML-KEM-768 zu X25519 im TLS 1.3 Protokoll.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{summative_evaluation_TLS1.3_kryptoagilität.png}
    \caption{TLS 1.3 Kryptoagiler Fallback von X25519+ML-KEM-768 zu X25519}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Summative_Evaluation_TLS1.3_Kryptoagilität}
\end{figure}

\subsubsection{Applikationslayer}

Die Kryptoagilität des SSI-Systems auf Applikationsebene manifestiert sich in der Erfüllung der ersten und zweiten Anforderung von \textcite[S. 19--20]{cyberresilienceworkshopseriescommittee_CryptographicAgilityInteroperabilityProceedingsWorkshop_2017} Sicherheitsalgorithmen in Echtzeit auf Basis ihrer kombinierten Sicherheitsfunktionen auszuwählen, und die Möglichkeit zu eröffnen, neue kryptographische Funktionen bzw. Algorithmen in bestehende Hard- und Software zu integrieren.

Dafür nutzt das SSI-System die Plugin-Architektur von Aries Cloud Agent Python (ACA-Py), die es ermöglicht, bestehende kryptographische Workflows durch gezielte Eingriffe zu modifizieren, ohne den Kerncode der Agenten zu verändern. Dieses Designprinzip entspricht dem \enquote{Open/Closed Principle} der Softwareentwicklung, welches besagt, dass Softwaremodule offen für Erweiterungen, jedoch geschlossen für Modifikationen sein sollten \cite[S. 99]{martin_AgileSoftwareDevelopmentprinciplespatternspractices_2003}.

Dafür implementiert das Plugin einen metadatengesteuerten Ansatz zur Algorithmenauswahl. Wie in Listing~\ref{lst:Jupyter-Notebook-Cell-9-Demonstration-Kryptoagilität-ed25519} demonstriert, kann der gewünschte Schlüsseltyp durch Übergabe des Parameters \enquote{metadata: \{key\_type : ed25519\}} explizit spezifiziert werden.

Bei der Verarbeitung einer Out-of-Band (OOB) Invitation durchläuft das System folgenden Entscheidungsbaum: Der \enquote{OutOfBandManager} empfängt die Metadata-Parameter aus der API-Anfrage und propagiert diese an die DID-Erstellungslogik. In der Funktion \enquote{create\_did\_peer\_4\_conditional\_pqc} (Listing~\ref{lst:base_manager_patch.py}) erfolgt eine Auswertung des \enquote{key\_type}-Parameters. Wird der Wert \enquote{ed25519} erkannt, delegiert das Plugin die gesamte DID-Erstellung an die ursprüngliche ACA-Py-Implementierung (\enquote{\_original\_create\_did\_peer\_4}), wodurch ein vollständiger Fallback auf klassische Kryptographie ohne Plugin-Interferenz gewährleistet wird. Fehlt die Metadata-Spezifikation aktiviert das System standardmäßig die Post-Quantum-Kryptographie mit den Algorithmen ML-DSA-65 für digitale Signaturen und ML-KEM-768 für Schlüsselkapselung.

Die erfolgreiche Validierung dieser Kryptoagilität zeigt Listing~\ref{lst:Jupyter-Notebook-Cell-9-Demonstration-Kryptoagilität-ed25519-output}. Trotz aktiviertem Plugin etabliert sich eine vollständig ED25519-basierte Verbindung zwischen Issuer- und Holder-Agent. Die Verbindung erreicht den Status \enquote{active} auf beiden Seiten, was die korrekte Durchführung des DID Exchange-Protokolls mit klassischen Algorithmen bestätigt.

Listing~\ref{lst:Jupyter-Notebook-Cell-11-Demonstration-Kryptoagilität-ed25519-output} verifiziert die persistierte Kryptographie-Konfiguration auf Wallet-Ebene. Die Inspektion der gespeicherten DIDs zeigt konsistent den Wert \enquote{{key\_type}:{ed25519}}. Insbesondere die Abwesenheit von PQC-spezifischen Metadata-Markern wie \enquote{pqc\_enabled}, \enquote{signature\_algorithm} oder \enquote{kem\_verkey} in den DID-Metadaten bestätigt, dass das Plugin keinerlei modifizierende Eingriffe in den ED25519-Workflow vorgenommen hat.

% \newpage
% \section{Systemarchitektur und Design} \label{sec:Systemarchitektur und Design}