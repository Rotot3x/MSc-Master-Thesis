\section{Self-Sovereign Identity}
\label{sec:Anhang_Self-Sovereign_Identity}

\begin{longtable}{L{3cm}L{8cm}}
    \caption{Zehn Prinzipien von Self-Sovereign Identity}
    \label{tab:allen_ten_ssi_principles} \\
    \toprule
    \textbf{Prinzip} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Prinzip} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf \textcite{allen_PathSelfSovereignIdentity_2016}.} \\
    \endlastfoot
    Existenz & Benutzer müssen eine unabhängige Existenz haben \\
    \midrule
    Kontrolle & Benutzer müssen die Kontrolle über ihre Identität behalten \\
    \midrule
    Zugriff & Benutzer müssen Zugriff auf ihre eigenen Daten haben \\
    \midrule
    Transparenz & Systeme und Algorithmen müssen transparent sein \\
    \midrule
    Persistenz & Identitäten müssen langlebig sein \\
    \midrule
    Übertragbarkeit & Informationen und Dienste rund um die Identität müssen übertragbar sein \\
    \midrule
    Interoperabilität &  Identitäten sollten so weit wie möglich nutzbar sein \\
    \midrule
    Einverständnis & Die Nutzer müssen der Verwendung ihrer Identität zustimmen \\
    \midrule
    Minimalisierung & Die Offenlegung von Daten muss auf ein Minimum beschränkt werden \\
    \midrule
    Schutz & Die Rechte der Nutzer müssen geschützt werden \\
\end{longtable}

\newpage
\section{Systematische Literaturrecherche}
\label{sec:Anhang_Dokumentation_der_systematischen_Literaturrecherche}

Die systematische Literaturrecherche bildet das wissenschaftliche Fundament dieser Masterarbeit und wurde in zwei methodisch konsistenten Iterationen durchgeführt, um eine höchstmögliche Aktualität im dynamischen Forschungsfeld der \ac{PQC} und \ac{SSI} zu gewährleisten.

Die erste Iteration fand im Rahmen des Exposés statt und etablierte die methodische Grundlage für das gesamte Forschungsprojekt. Diese initiale Recherche identifizierte 61 relevante Quellen mit differenzierter Relevanzklassifizierung (hoch/mittel/niedrig) und ermöglichte die Formulierung der Forschungsfragen sowie die Identifikation der Forschungslücke im Bereich \ac{PQC} für \ac{SSI}-Systeme in kritischen Infrastrukturen. Im Ergebnis dieser ersten Phase konnten fünf Kernpublikationen identifiziert werden, die die Basis der theoretischen Fundierung bildeten. Die detaillierte Dokumentation dieser ersten Iteration ist in \ref{sec:Anhang_Dokumentation_der_ersten_Iteration_Expose} integriert, um Transparenz und Reproduzierbarkeit des ursprünglichen Forschungskonzepts zu dokumentieren.

Die zweite Iteration (\ref{sec:Anhang_Dokumentation_der_zweiten_Iteration}) wurde während der Ausarbeitungsphase der Masterarbeit durchgeführt, um den Zeitraum zwischen dem Exposé und dem Abschluss der Arbeit abzudecken (30. Mai 2025 bis 02. November 2025). Unter Anwendung der identischen Suchstrategie und Selektionskriterien nach \ac{PRISMA} 2020 wurden in dieser Phase weitere 34 Quellen identifiziert.

Das kumulierte Ergebnis beider Iterationen umfasst somit die Identifikation und Analyse von insgesamt sieben hochrelevanten wissenschaftlichen Quellen, die den aktuellen Stand der Forschung in den Schnittmengen der Domänen \ac{SSI}, Blockchain, \ac{PQC} und \ac{KRITIS} repräsentieren.

\pagebreak

\subsection{Erste Iteration vom 30. Mai 2025 (Exposé)}
\label{sec:Anhang_Dokumentation_der_ersten_Iteration_Expose}

Die systematische Literaturrecherche zum Stand der Forschung orientiert sich am \enquote{iterative Review-Ansatz} nach \textcite[S. 208--209]{brocke_StandingShouldersGiantsChallengesRecommendationsLiteratureSearchInformationSystemsResearch_2015}, der mit einer initialen Recherche startet und sich iterativ vertieft. Für die Struktur und Dokumentation sind ausgewählte Methoden der \ac{PRISMA} 2020 Richtlinien zugrunde gelegt (\autoref{tab:Ausgewählte Methoden der PRISMA 2020 Richtlinien}). \ac{PRISMA} gewährleistet hierbei einen transparenten, reproduzierbaren Prozess und verbessert die Berichtqualität \parencite[S. 1, 6]{page_PRISMA2020Statementupdatedguidelinereportingsystematicreviews_2021}.

\begin{longtable}{L{0.3\textwidth}L{0.7\textwidth}}
    \caption{Ausgewählte Methoden der PRISMA 2020 Richtlinien}
    \label{tab:Ausgewählte Methoden der PRISMA 2020 Richtlinien} \\
    \toprule
    \textbf{Methode} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Methode} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} In Anlehnung an \textcite[S. 4]{page_PRISMA2020Statementupdatedguidelinereportingsystematicreviews_2021}.} \\
    \endlastfoot
    Ein- und Ausschlusskriterien & Ein- und Ausschlusskriterien für die Überprüfung. \\
    \midrule
    Suchstrategie & Vollständigen Suchstrategie für alle Datenbanken, Websites einschließlich aller verwendeten Filter. \\
    \midrule
    Selektionsprozess & Methoden an, die verwendet wurden, um zu entscheiden, ob eine Studie die Einschlusskriterien der Überprüfung erfüllt. \\
\end{longtable}

\pagebreak

Die in \autoref{tab:einausschlusskriterien} dargestellten Ein- und Ausschlusskriterien gewährleisten eine transparente, nachvollziehbare und zielgerichtete Auswahl relevanter wissenschaftlicher Quellen.

\begin{longtable}{L{3cm}L{4cm}L{4cm}L{3cm}}
    \caption{Ein- und Ausschlusskriterien für die systematische Literaturrecherche}
    \label{tab:einausschlusskriterien} \\
    \toprule
    \textbf{Kategorie} & \textbf{Einschluss} & \textbf{Ausschluss} & \textbf{Begründung} \\
    \midrule
    \endfirsthead
    \multicolumn{4}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Kategorie} & \textbf{Einschluss} & \textbf{Ausschluss} & \textbf{Begründung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{4}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{4}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung.} \\
    \endlastfoot
    Thematischer Fokus &
    \ac{SSI} und dezentrale Identitätslösungen;
    Blockchain-basierte Identitätsmanagementsysteme;
    \ac{PQC} und quantensichere Algorithmen;
    Sicherheit und Compliance in \ac{KRITIS};
    \ac{DSR}-Methodik in IT/Informationssystemen;
    Kryptoagilität und kryptografische Migration & 
    Identitätsmanagement ohne Bezug zu \ac{SSI} oder Blockchain;
    Klassische \ac{PKI} ohne \ac{PQC}-Bezug;
    Kryptografie ohne Post-Quantum-Relevanz;
    Arbeiten ohne Bezug zu \ac{KRITIS} oder ohne sicherheitskritischen Kontext;
    Nicht-\ac{DSR}-basierte Entwicklungsansätze & 
    Fokussierung auf die Forschungsfragen und relevante technologische, methodische und regulatorische Aspekte. \\
    \midrule
    Zeitrahmen & 2015 bis heute & Vor 2015 & Berücksichtigung aktueller technologischer Entwicklungen (Blockchain, \ac{PQC}, \ac{SSI}) und regulatorischer Anforderungen. \\
    \midrule
    Publikationstypen & 
    Peer-reviewed Journalartikel;
    Konferenzbeiträge anerkannter Fachgesellschaften (z. B. IEEE, ACM, IFIP); Preprints;
    Offizielle Standards und Empfehlungen (z. B. \ac{NIST}, W3C, \ac{BSI});
    Whitepaper etablierter Organisationen;
    Dissertationen und anerkannte Fachbücher & 
    Blogposts, Forenbeiträge, Marketingmaterial;
    Populärwissenschaftliche Artikel ohne wissenschaftliche Fundierung;
    Unveröffentlichte Manuskripte ohne Peer-Review;
    Seminar- und Abschlussarbeiten ohne wissenschaftliche Begutachtung & 
    Sicherstellung wissenschaftlicher Qualität, Nachvollziehbarkeit und Relevanz der Quellen für die Masterarbeit; Da das Forschungsthema aktuell noch sehr neu ist und die einschlägige Fachliteratur teilweise noch nicht den Peer-Review-Prozess durchlaufen hat, werden auch Preprints in die Analyse einbezogen. Preprints ermöglichen eine zeitnahe Verfügbarkeit aktueller Forschungsergebnisse, was insbesondere bei diesem innovativen von großer Bedeutung ist. \\
    \midrule
    Sprache & Deutsch; Englisch & Andere Sprachen als Deutsch und Englisch & Gewährleistung der Verständlichkeit und Zugänglichkeit für den deutsch- und englischsprachigen Forschungskontext. \\
    \midrule
    Zugänglichkeit & Verfügbare Volltexte & Nicht verfügbare Volltexte & Ermöglichung einer gründlichen Analyse und Bewertung der Inhalte. \\
\end{longtable}

\autoref{tab:suchstrategie} stellt ausgewählte methodische Schritte zur Entwicklung einer Suchstrategie nach \textcite[S. 532]{bramer_SystematicApproachSearchingefficientcompletemethoddevelopliteraturesearches_2018} dar, an denen sich diese Seminararbeit orientiert.

\begin{longtable}{L{0.1\textwidth}L{0.9\textwidth}}
    \caption{Überblick über die Entwicklung der Suchstrategie}
    \label{tab:suchstrategie} \\
    \toprule
    \textbf{Schritt} & \textbf{Beschreibung} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Methode} & \textbf{Beschreibung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} In Anlehnung an \textcite[S. 532]{bramer_SystematicApproachSearchingefficientcompletemethoddevelopliteraturesearches_2018}.} \\
    \endlastfoot
    1  & Identifikation relevanter Schlüsselkonzepte \\
    \midrule
    2  & Identifikation relevanter Keywords \\
    \midrule 
    3  & Erstellung einer strukturierten Suchanfrage mit Booleschen Operatoren \\
    \midrule
    4  & Auswahl geeigneter Datenbanken \\
    \midrule
    5  & Übersetzung der Suchanfrage für verschiedene Datenbanken \\
\end{longtable}

\paragraph*{Identifikation relevanter Schlüsselkonzepte}

Im ersten Schritt wird eine präzise Identifikation und Abgrenzung zentraler Schlüsselkonzepte vorgenommen (\autoref{tab:Abgrenzung zentraler Schlüsselkonzepte}).

\begin{longtable}{L{4cm}L{10cm}}
    \caption{Abgrenzung zentraler Schlüsselkonzepte}
    \label{tab:Abgrenzung zentraler Schlüsselkonzepte} \\
    \toprule
    \textbf{Schlüsselkonzept} & \textbf{Erläuterung} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Schlüsselkonzept} & \textbf{Erläuterung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf \textcite[S. 2]{solavagione_TransitionSelfSovereignIdentityPostQuantumCryptography_2025}, \textcite{nationalinstituteofstandardsandtechnologyus_ModulelatticebasedKeyencapsulationMechanismstandard_2024,nationalinstituteofstandardsandtechnologyus_ModulelatticebasedDigitalSignaturestandard_2024,nationalinstituteofstandardsandtechnologyus_StatelessHashbasedDigitalsignaturestandard_2024}, \textcite{bundesministeriumderjustiz_GesetzUeberBundesamtfuerSicherheitInformationstechnikBSIGesetzBSIG_2009}, \textcite{hevner_DesignScienceInformationsystemsresearch_2004}.} \\
    \endlastfoot
    Self-Sovereign Identity & 
    Das Paradigma der selbstbestimmten digitalen Identität, das Nutzenden die Kontrolle über ihre Identitätsdaten und deren Weitergabe ermöglicht, basierend auf dezentralen Technologien wie Blockchain und interoperablen Standards wie \ac{DID} ,\ac{VC} und \ac{VP} \parencite[S. 2]{solavagione_TransitionSelfSovereignIdentityPostQuantumCryptography_2025}. \\
    \midrule
    Blockchain-Technologie & 
    Die Nutzung von \ac{DLT} zur Sicherstellung von Integrität, Transparenz und Manipulationssicherheit im Identitätsmanagement, insbesondere im Kontext von \ac{SSI}-Systemen \parencite[S. 2]{solavagione_TransitionSelfSovereignIdentityPostQuantumCryptography_2025}. \\
    \midrule
    Post-Quantum Kryptografie & 
    Kryptografische Verfahren, die auch gegen Angriffe durch leistungsfähige Quantencomputer resistent sind, einschließlich aktueller Standardisierungsansätze (z. B. \ac{NIST} FIPS 203, 204, 205) \parencite{nationalinstituteofstandardsandtechnologyus_ModulelatticebasedKeyencapsulationMechanismstandard_2024,nationalinstituteofstandardsandtechnologyus_ModulelatticebasedDigitalSignaturestandard_2024,nationalinstituteofstandardsandtechnologyus_StatelessHashbasedDigitalsignaturestandard_2024} und Empfehlungen zur kryptoagilen Systemgestaltung. \\
    \midrule
    Kritische Infrastrukturen & 
    Sektoren und Systeme, deren Funktionsfähigkeit essenziell für das Gemeinwesen ist und die daher besonders hohe Anforderungen an Sicherheit, Compliance und Resilienz stellen \parencite{bundesministeriumderjustiz_GesetzUeberBundesamtfuerSicherheitInformationstechnikBSIGesetzBSIG_2009}. \\
    \midrule
    Design Science Research & 
    Die methodische Grundlage zur systematischen Entwicklung, Evaluation und wissenschaftlichen Fundierung innovativer IT-Artefakte nach \textcite{hevner_DesignScienceInformationsystemsresearch_2004} im Kontext der genannten Technologien und Anwendungsdomänen. \\
\end{longtable}

\paragraph*{Identifikation relevanter Keywords}

Basierend auf den zuvor definierten Schlüsselkonzepten wurden die wichtigsten Suchbegriffe in Deutsch und Englisch sowie die gängigen Akronyme zusammengestellt (\autoref{tab:keywords der schlüsselkonzepte}), um die technologische, methodische und regulatorische Breite der Recherche optimal abzudecken.

\begin{longtable}{L{4cm}L{10cm}}
    \caption{Keywords der Schlüsselkonzepte}
    \label{tab:keywords der schlüsselkonzepte} \\
    \toprule
    \textbf{Schlüsselkonzept} & \textbf{Keywords} \\
    \midrule
    \endfirsthead
    \multicolumn{2}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Schlüsselkonzept} & \textbf{Keywords} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{2}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{2}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung.} \\
    \endlastfoot
    Self-Sovereign Identity &
    Self-Sovereign Identity, \ac{SSI}, dezentrale Identität, digitale Identität, decentralized identity, user-controlled identity, identity wallet, Verifiable Credentials, \ac{VC}, Decentralized Identifiers, \ac{DID}, Identity Management, IdM, Identity Access Management, \ac{IAM} \\
    \midrule
    Blockchain-Technologie &
    Blockchain, Distributed Ledger Technology, \ac{DLT}, Distributed Ledger, Smart Contract, Ethereum, Hyperledger, IOTA, Permissioned Ledger, Consensus Mechanism, On-Chain Identity \\
    \midrule
    Post-Quantum Kryptografie &
    Post-Quantum Cryptography, \ac{PQC}, quantensichere Verschlüsselung, quantum-resistant cryptography, Lattice-based Cryptography, Hash-based Signature, Code-based Cryptography, Multivariate Cryptography, Module-Lattice-based Digital Signature, ML-DSA, Module-Lattice-based Key Encapsulation Mechanism, ML-KEM, Stateless Hash-based Digital Signature, SLH-DSA, CRYSTALS-Dilithium, SPHINCS+, Kryptoagilität, Cryptographic Agility, Algorithm Agility, Migration Strategy, Cryptographic Migration, Flexible Cryptography Update \\ 
    \midrule
    Kritische Infrastrukturen &
    Kritische Infrastrukturen, KRITIS, Critical Infrastructure Protection, CIP, sector-specific security requirements, Compliance, Privacy by Design, Privacy by Default, Data Protection, Regulatory Requirements, Bundesamt für Sicherheit in der Informationstechnik, BSI, European Union Agency for Cybersecurity, ENISA, eIDAS \\ 
    \midrule
    Design Science Research &
    Design Science Research, \ac{DSR}, Design Science Research Methodology, DSRM, Artefact Development, Evaluation of Artefacts, Method-driven Development, Iterative Design Process, Research Methodology, Framework for Evaluation in Design Science, \ac{FEDS}, Preferred Reporting Items for Systematic Reviews and Meta-Analyses, \ac{PRISMA} \\
\end{longtable}

\paragraph*{Erstellung einer strukturierten Suchanfrage mit Booleschen Operatoren}

Listing~A-\ref{lst:boolesche_suchanfrage} stellt eine strukturierte Suchstrategie mit Booleschen Operatoren auf Basis der identifizierten Keywords dar. Die Suchanfrage verbindet zentrale Schlüsselkonzepte und nutzt gezielt Synonyme und Akronyme zur Abdeckung verschiedener Terminologien und Schreibweisen.
\newline

\refstepcounter{manualListingCounterA}
\label{lst:boolesche_suchanfrage}
\begin{lstlisting}[
caption={Strukturierte Suchanfrage mit Booleschen Operatoren},
basicstyle=\small\ttfamily,
breaklines=true,
frame=single,
language=SQL
]
(
(
"self-sovereign identity" OR SSI OR "decentralized identity" OR "dezentrale Identität" OR "digitale Identität" OR "user-controlled identity" OR "identity wallet" OR "verifiable credentials" OR VC OR "decentralized identifiers" OR DID OR "identity management" OR IdM OR "identity access management" OR IAM
)
AND
(
blockchain OR "distributed ledger technology" OR DLT OR "distributed ledger" OR "smart contract" OR Ethereum OR Hyperledger OR IOTA OR "permissioned ledger" OR "consensus mechanism" OR "on-chain identity"
)
AND
(
"post-quantum cryptography" OR PQC OR "quantensichere Verschlüsselung" OR "quantum-resistant cryptography" OR "lattice-based cryptography" OR "hash-based signature" OR "code-based cryptography" OR "multivariate cryptography" OR "module-lattice-based digital signature" OR ML-DSA OR "module-lattice-based key encapsulation mechanism" OR ML-KEM OR "stateless hash-based digital signature" OR SLH-DSA OR CRYSTALS-Dilithium OR SPHINCS+ OR kryptoagilität OR "cryptographic agility" OR "algorithm agility" OR "migration strategy" OR "cryptographic migration" OR "flexible cryptography update"
)
AND
(
"kritische Infrastrukturen" OR KRITIS OR "critical infrastructure protection" OR CIP OR "sector-specific security requirements" OR compliance OR "privacy by design" OR "privacy by default" OR "data protection" OR "regulatory requirements" OR "bundesamt für sicherheit in der informationstechnik" OR BSI OR "european union agency for cybersecurity" OR ENISA OR eIDAS
)
AND
(
"design science research" OR DSR OR "design science research methodology" OR DSRM OR "artefact development" OR "evaluation of artefacts" OR "method-driven development" OR "iterative design process" OR "research methodology" OR "framework for evaluation in design science" OR FEDS OR "preferred reporting items for systematic reviews and meta-analyses" OR PRISMA
)
)
\end{lstlisting}

\pagebreak

\paragraph*{Auswahl einer geeigneten Datenbank}

Die Wahl auf \gls{EBSCO} als Datenbank für die systematische Literaturrecherche resultiert aus der Existenz von Campuslizenzen der FOM für diese Datenbank.

\paragraph*{Übersetzung der Suchanfrage für EBSCO}

Die strukturierten Suchanfrage mit den Booleschen Operatoren kann für
EBSCO ohne Anpassungen übernommen werden. Das Ergebnis der EBSCO Suchanfrage
ist in \autoref{fig:EBSCO Ergebnis} dargestellt. Insgesamt wurden mit dieser Abfrage 61 Quellen identifiziert.

\begin{figure}[H]
    \centering
    \includegraphics[width=\paperwidth, height=\paperheight, keepaspectratio, angle=90]{EBSCO.png}
    \caption{Erste Iteration - Ergebnis der EBSCO Suchanfrage}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:EBSCO Ergebnis}
\end{figure}

\paragraph*{Selektionsprozess}

Im nächsten Schritt folgt der Selektionsprozess der 61 identifizierten Quellen aufbauend auf den zuvor definierten Schlüsselkonzepten und der Suchstrategie. Dabei werden die in \autoref{tab:einausschlusskriterien} definierten Ein- und Ausschlusskriterien konsequent angewendet, sodass nur fachlich einschlägige, qualitativ belastbare und für die Forschungsfragen zentrale Studien inkludiert werden.

Zunächst werden alle über die EBSCO-Suchanfrage identifizierten 61 Treffer einer Duplikatsbereinigung unterzogen und anschließend im Rahmen des Titel- und Abstract-Screenings hinsichtlich ihres thematischen Fokus bewertet. Hohe Relevanz erhalten Quellen mit klaren Beiträgen zu \ac{SSI}, \ac{PQC}, \ac{KRITIS} oder dezentralen Identitätsarchitekturen, während Arbeiten zu angrenzenden Technologien wie Blockchain-Sicherheit im \ac{IoT} oder digitaler Forensik als mittel relevant und allgemeine Technologietrends ohne direkten Bezug zum Thema als niedrig relevant eingestuft werden. 

\autoref{fig:PRISMA_Flussdiagramm_iteration1} fasst diesen Selektionsprozess der ersten Iteration zusammen. Nach dem Screening aller 61 Publikationen folgte die Eignungsprüfung, in deren Ergebnis fünf Publikationen die Einschlusskriterien erfüllten und 56 ausgeschieden wurden, da sie entweder thematisch irrelevant (n~=~33) oder hinsichtlich ihrer Spezifität unangemessen waren (n~=~23).

\begin{figure}[H]
    \centering
    \includegraphics[width=\paperwidth]{PRISMA_2020_flow_diagram_new_SRs_v1_ITERATION_1.png}
    \caption{Erste Iteration - PRISMA 2020 Flussdiagramm}
    \begin{flushleft}
    \textit{Anmerkung.} In Anlehnung an \textcite[S. 5]{page_PRISMA2020Statementupdatedguidelinereportingsystematicreviews_2021}.
    \end{flushleft}
    \label{fig:PRISMA_Flussdiagramm_iteration1}
\end{figure}

\pagebreak

\autoref{tab:quellenuebersicht_Iteration1} stellt eine Übersicht der Bewertung der 61 identifizierten Quellen dar, welche vollständig in \autoref{tab:quellenbewertung} aufzufinden ist.

\begin{longtable}{L{1.5cm}L{11cm}L{1cm}}
    \caption{Erste Iteration - Übersicht der relevanten Quellen}
    \label{tab:quellenuebersicht_Iteration1} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endfirsthead
    \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf \autoref{tab:quellenbewertung} und Titel und Abstracts von \textcite{szymanski_QuantumSafeSoftwareDefinedDeterministicInternetThingsIoTHardwareEnforcedCyberSecurityCriticalInfrastructures_2024,nouma_TrustworthyEfficientDigitalTwinsPostQuantumEraHybridHardwareAssistedSignatures_2024,sharif_EIDASRegulationSurveyTechnologicalTrendsEuropeanElectronicIdentitySchemes_2022,alam_PrivatelyGeneratedKeyPairsPostQuantumCryptographyDistributedNetwork_2024,radanliev_ReviewComparisonUSEUUKRegulationsCyberRiskSecurityCurrentBlockchainTechnologies_2023}.} \\
    \endlastfoot
    1 & Szymanski, T. H. (2024). A Quantum-Safe Software-Defined Deterministic Internet of Things (IoT) with Hardware-Enforced Cyber-Security for Critical Infrastructures. Information (2078-2489), 15(4), 173. \url{https://doi.org/10.3390/info15040173} & Hoch \\
    \midrule
    2 & Nouma, S. E., \& Yavuz, A. A. (2024). Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid Hardware-Assisted Signatures. ACM Transactions on Multimedia Computing, Communications \& Applications, 20(6), 1–30. \url{https://doi.org/10.1145/3638250} & Hoch \\
    \midrule
    3 & Sharif, A., Ranzi, M., Carbone, R., Sciarretta, G., Marino, F. A., \& Ranise, S. (2022). The eIDAS Regulation: A Survey of Technological Trends for European Electronic Identity Schemes. Applied Sciences (2076-3417), 12(24), 12679. \url{https://doi.org/10.3390/app122412679} & Hoch \\
    \midrule
    4 & Alam, M., Hoffstein, J., \& Cambou, B. (2024). Privately Generated Key Pairs for Post Quantum Cryptography in a Distributed Network. Applied Sciences (2076-3417), 14(19), 8863. \url{https://doi.org/10.3390/app14198863} & Hoch \\
    \midrule
    5 & Radanliev, P. (2023). Review and Comparison of US, EU, and UK Regulations on Cyber Risk/Security of the Current Blockchain Technologies: Viewpoint from 2023. Review of Socionetwork Strategies, 17(2), 105–129. \url{https://doi.org/10.1007/s12626-023-00139-x} & Hoch \\
    \midrule
    6--38 & Diverse & Mittel  \\
    \midrule
    39--61 & Diverse & Niedrig \\
\end{longtable}

\paragraph*{Bewertung der identifizierten Quellen hinsichtlich ihrer Relevanz}

\begin{longtable}{L{0.5cm}L{4cm}L{1.5cm}L{7cm}}
    \caption{Erste Iteration - Bewertung der identifizierten Quellen hinsichtlich ihrer Relevanz}
    \label{tab:quellenbewertung} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} & \textbf{Kommentar} \\
    \midrule
    \endfirsthead
    \multicolumn{4}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} & \textbf{Kommentar} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{4}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{4}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf den Abstracts aller in Spalte zwei unter \enquote{Quelle} aufgeführten Quellenangaben.} \\
    \endlastfoot
1 & Szymanski, T. H. (2024). A Quantum-Safe Software-Defined Deterministic Internet of Things (IoT) with Hardware-Enforced Cyber-Security for Critical Infrastructures. Information (2078-2489), 15(4), 173. \url{https://doi.org/10.3390/info15040173} & Hoch & Fokussiert auf die Entwicklung quantensicherer Kommunikations- und Sicherheitssysteme im Kontext von \ac{KRITIS} und Industrial IoT; adressiert explizit \ac{PQC} durch Einsatz quantensicherer Verschlüsselungsmechanismen und QKD-Netzwerke; behandelt hardwarebasierte Zugriffskontrollen, Zero Trust Architekturen und AI-gestützte Sicherheit, die für Resilienz und Sicherheitsanforderungen in \ac{KRITIS} maßgeblich sind; zwar steht \ac{SSI} und Blockchain-Technologie nicht im Mittelpunkt, jedoch zeigen die vorgestellten innovativen Konzepte und die experimentelle Validierung einen sehr hohen Anwendungs- und Erkenntniswert für den methodischen und technologischen Fortschritt in mindestens zwei zentralen Domänen (\ac{PQC}, \ac{KRITIS}); damit bietet der Beitrag substanzielle Impulse für den Schutz hochsensibler digitaler Infrastrukturen. \\
\midrule
2 & Nouma, S. E., \& Yavuz, A. A. (2024). Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid Hardware-Assisted Signatures. ACM Transactions on Multimedia Computing, Communications \& Applications, 20(6), 1–30. \url{https://doi.org/10.1145/3638250} & Hoch & Betont die Notwendigkeit verlässlicher und effizienter digitaler Signaturen im Kontext von Digital Twins, die primär auf IoT-Infrastrukturen für hochsensible Daten abzielen und damit eine wichtige Schnittmenge zu \ac{KRITIS} darstellen; adressiert explizit \ac{PQC} durch die Entwicklung und Umsetzung quantensicherer und hybrider Signaturlösungen—einschließlich Forward Security und Aggregation, die auch für Blockchain-basierte und dezentrale Identitätsanwendungen (insb. mit Skalierungsbedarf) hoch relevant sind; der methodische Fortschritt im Bereich hardwareunterstützter, ressourcenschonender Kryptografie bietet substanzielle Innovationsimpulse für sicherheitskritische Systeme mit beschränkten Ressourcen, wie sie für \ac{SSI}-Lösungen und die sichere Verwaltung digitaler Identitäten in \ac{KRITIS} essenziell sind; Blockchain-Technologie und \ac{SSI} werden nicht explizit vertieft, jedoch ist die Übertragbarkeit der vorgestellten Konzepte—insbesondere hybride und aggregierbare Signaturen—auf beide Domänen methodisch und praxisnah gegeben. \\
\midrule
3 & Sharif, A., Ranzi, M., Carbone, R., Sciarretta, G., Marino, F. A., \& Ranise, S. (2022). The eIDAS Regulation: A Survey of Technological Trends for European Electronic Identity Schemes. Applied Sciences (2076-3417), 12(24), 12679. \url{https://doi.org/10.3390/app122412679} & Hoch & Adressiert zentrale Entwicklungen und Herausforderungen europäischer elektronischer Identitätssysteme im Kontext der eIDAS-Regulierung; analysiert technologische Trends und ihre Auswirkungen auf Sicherheit, Datenschutz und Interoperabilität nationaler eID-Lösungen, was unmittelbar an die Domäne \ac{SSI} und deren regulatorisches Umfeld anschließt; behandelt aktuelle Technologiestandards wie OAuth 2.0, SAML und OpenID Connect, ohne explizit Blockchain- oder \ac{PQC}-Lösungen zu integrieren, beleuchtet jedoch die (in eIDAS 2.0 antizipierte) Entwicklung hin zu dezentralisierten Identitätsarchitekturen, die als Grundlage künftiger \ac{SSI}-Lösungen dienen; liefert wesentliche Erkenntnisse für die Ausgestaltung sicherer und interoperabler digitaler Identitäten in \ac{KRITIS} und gibt Impulse für die technologische und methodische Weiterentwicklung nationalübergreifender Identitätsverwaltung. \\
\midrule
4 & Alam, M., Hoffstein, J., \& Cambou, B. (2024). Privately Generated Key Pairs for Post Quantum Cryptography in a Distributed Network. Applied Sciences (2076-3417), 14(19), 8863. \url{https://doi.org/10.3390/app14198863} & Hoch & Fokussiert auf die praktische Erzeugung, Verteilung und Verifikation privat generierter post-quanten-sicherer Schlüsselpaaren in verteilten Netzwerken, mit expliziter Anwendung von Crystals-Dilithium als \ac{PQC}-Algorithmus; adressiert wesentlich die Domäne \ac{PQC} durch Integration und Umsetzung eines aktuellen Standards, was sowohl für die Sicherheit verteilter Infrastrukturen als auch für zukünftige Identitätslösungen (z.B. im Kontext von \ac{SSI}) zentral ist; Berücksichtigung von Multi-Faktor-Authentifizierung, Challenge-Response-Mechanismen und biometrielosen Verfahren bietet substanzielle methodische Impulse für die Entwicklung von sicheren, dezentralen Schlüsselmanagement- und Authentifizierungslösungen; direkte Einbindung in Blockchain- oder spezifische \ac{KRITIS} wird nicht explizit diskutiert, ist aufgrund des Protokoll- und \ac{PKI}-Fokus jedoch technisch anschlussfähig und für die Domänen \ac{SSI} und \ac{KRITIS} innovativ und relevant. \\
\midrule
5 & Radanliev, P. (2023). Review and Comparison of US, EU, and UK Regulations on Cyber Risk/Security of the Current Blockchain Technologies: Viewpoint from 2023. Review of Socionetwork Strategies, 17(2), 105–129. \url{https://doi.org/10.1007/s12626-023-00139-x} & Hoch & Vergleichende Analyse der US-, EU- und UK-Regulierung im Bereich Cyber-Risiken und Sicherheit aktueller Blockchain-Technologien, basierend auf dem Stand 2023; systematische Prüfung und Gegenüberstellung führender Standardwerke wie \ac{NIST} (US), ISO27001 (international), und neueren Regularien wie MiCA (EU) und CPMI-IOSCO, unter Einbeziehung technologieübergreifender Aspekte (u.a. \ac{PQC}, Cloud Security, IoT); liefert bedeutende Einblicke und unmittelbaren Anwendungsbezug für die Bewertung, Weiterentwicklung und Integration internationaler Cybersecurity-Standards in neue Blockchain-Projekte—insbesondere mit Blick auf die vier Domänen moderner Identitäts- und Sicherheitssysteme \\
\midrule
6 & Enaya, A., Fernando, X., \& Kashef, R. (2025). Survey of Blockchain-Based Applications for IoT. Applied Sciences (2076-3417), 15(8), 4562. \url{https://doi.org/10.3390/app15084562} & Mittel & Betrachtet zentrale Aspekte von Blockchain-Technologien und Sicherheit; Schwerpunkt auf IoT-Anwendungen und branchenspezifische Implementierungen; explizite Bezüge zu \ac{SSI}, \ac{PQC} und \ac{KRITIS} fehlen; breiter Überblick, jedoch geringere Spezifizität hinsichtlich der vier Domänen der Masterarbeit ( \ac{SSI}, Blockchain, \ac{PQC}, \ac{KRITIS}). \\
\midrule
7 & Siam, M. K., Saha, B., Hasan, M. M., Hossain Faruk, M. J., Anjum, N., Tahora, S., Siddika, A., \& Shahriar, H. (2025). Securing Decentralized Ecosystems: A Comprehensive Systematic Review of Blockchain Vulnerabilities, Attacks, and Countermeasures and Mitigation Strategies. Future Internet, 17(4), 183. \url{https://doi.org/10.3390/fi17040183} & Mittel & Systematische Analyse von Schwachstellen, Angriffsszenarien und Gegenmaßnahmen in Blockchain-Ökosystemen; konzentriert sich auf Sicherheitsaspekte von Blockchain-Technologien ohne spezifische Betrachtung von \ac{SSI}, \ac{PQC} oder \ac{KRITIS}; hoher inhaltlicher Wert für das allgemeine Verständnis von Blockchain-Sicherheit, jedoch eingeschränkte Anwendbarkeit auf alle vier Domänen der Masterarbeit. \\
\midrule
8 & Ramirez Lopez, L. J., \& Morillo Ledezma, G. G. (2025). Employing Blockchain, NFTs, and Digital Certificates for Unparalleled Authenticity and Data Protection in Source Code: A Systematic Review. Computers (2073-431X), 14(4), 131. \url{https://doi.org/10.3390/computers14040131} & Mittel & Fokussiert auf Blockchain-basierte Technologien zur Sicherung von Authentizität und Zugriffskontrolle, jedoch im Anwendungskontext akademischer Quellcode-Sicherheit; behandelt primär NFTs und digitale Zertifikate, mit begrenztem Bezug zu \ac{SSI} und ohne Einbeziehung von \ac{PQC}; adressiert die Domäne „Blockchain“ und Aspekte der Datensicherheit, weist jedoch eine geringe Relevanz für die Domänen \ac{SSI}, \ac{PQC} und \ac{KRITIS} im Kontext der Masterarbeit auf. \\
\midrule
9 & Sebestyen, H., Popescu, D. E., \& Zmaranda, R. D. (2025). A Literature Review on Security in the Internet of Things: Identifying and Analysing Critical Categories. Computers (2073-431X), 14(2), 61. \url{https://doi.org/10.3390/computers14020061} & Mittel & Umfassende Betrachtung aktueller Sicherheitsthemen und Identitätsmanagement im IoT-Kontext, unter Rückgriff auf neue Technologien wie Blockchain; Integrationspotenzial hinsichtlich Blockchain erkennbar, jedoch keine explizite Behandlung von \ac{SSI}, \ac{PQC} oder \ac{KRITIS}; bietet einen breiten Überblick zu technologischen Lösungen und Herausforderungen, adressiert jedoch nur partiell die Anforderungen und Innovationspotenziale der vier Domänen der Masterarbeit. \\
\midrule
10 & Nambundo, J. M., de Souza Martins Gomes, O., de Souza, A. D., \& Machado, R. C. S. (2025). Cybersecurity and Major Cyber Threats of Smart Meters: A Systematic Mapping Review. Energies (19961073), 18(6), 1445. \url{https://doi.org/10.3390/en18061445} & Mittel & Konzentriert sich auf Cybersecurity-Bedrohungen und Schwachstellen im Kontext von Smart Metern als Teil kritischer Infrastruktur; adressiert relevante Sicherheitsfragen in Bezug auf Energieversorgung, thematisch verwandt mit der Schutzbedarfsanalyse für \ac{KRITIS}; der Einsatz von \ac{SSI}, Blockchain-Technologien oder \ac{PQC} wird nicht explizit thematisiert; bietet wichtige Einblicke in Bedrohungsszenarien und Mitigationsstrategien für smarte Energiesysteme, bleibt jedoch im Hinblick auf innovative Identitäts- oder Kryptografielösungen und deren methodischer Integration in Smart Metering-Systeme unspezifisch. \\
\midrule
11 & Yuan, F., Huang, X., Zheng, L., Wang, L., Wang, Y., Yan, X., Gu, S., \& Peng, Y.(2025). The Evolution and Optimization Strategies of a PBFT Consensus Algorithm for Consortium Blockchains. Information (2078-2489), 16(4), 268. \url{https://doi.org/10.3390/info16040268} & Mittel & Fokussiert auf die Optimierung des PBFT-Konsensalgorithmus, was grundlegende Bedeutung für Leistung, Sicherheit und Zuverlässigkeit von Consortium Blockchains hat; leistet methodischen Beitrag zur technologischen Weiterentwicklung im Blockchain-Bereich, jedoch ohne explizite Einbindung von \ac{SSI}, \ac{PQC} oder direktem Bezug zu \ac{KRITIS}; relevante Erkenntnisse zur Verbesserung von Konsensmechanismen, die potenziell für skalierbare und sichere Blockchain-basierte Infrastrukturen adaptierbar sind, aber inhaltlich primär auf Konsensalgorithmen begrenzt. \\
\midrule
12 & Radanliev, P. (2024). Digital security by design. Security Journal, 37(4), 1640–1679. \url{https://doi.org/10.1057/s41284-024-00435-3} & Mittel & Bietet eine umfassende, technologieübergreifende Analyse aktueller Herausforderungen im Bereich digitale Sicherheit; adressiert relevante Zukunftsthemen wie AI, Blockchain und Quantencomputing im Kontext sich wandelnder Sicherheitsparadigmen; keine explizite Schwerpunktsetzung auf \ac{SSI}, \ac{PQC} oder spezifisch \ac{KRITIS}; sektorübergreifende Betrachtung liefert wertvolle Einblicke zu regulatorischen und praxisbezogenen Aspekten, bleibt jedoch in Bezug auf die integrative Anwendung innovativer Sicherheitslösungen innerhalb der vier Domänen der Masterarbeit unspezifisch. \\
\midrule
13 & Miller, T., Durlik, I., Kostecka, E., Sokołowska, S., Kozlovska, P., \& Zwolak, R. (2025). Artificial Intelligence in Maritime Cybersecurity: A Systematic Review of AI-Driven Threat Detection and Risk Mitigation Strategies. Electronics (2079-9292), 14(9), 1844. \url{https://doi.org/10.3390/electronics14091844} & Mittel & Fokus liegt auf der Anwendung von KI-gestützten Verfahren zur Erkennung und Minderung von Cyber-Bedrohungen im maritimen Sektor, adressiert damit relevante Aspekte kritischer Infrastrukturen; Schnittstellen zu Blockchain und quantenkryptographischen Ansätzen werden als Forschungsperspektiven genannt, ohne im Review zentrale methodische oder anwendungsbezogene Ausarbeitung zu bieten; \ac{SSI} wird nicht behandelt, der primäre Schwerpunkt liegt auf KI und Cybersecurity, wodurch methodische und technische Details zu \ac{SSI} und \ac{PQC} im Kontext der vier Domänen der Masterarbeit fehlen. \\
\midrule
14 & Atlam, H. F., Ekuri, N., Azad, M. A., \& Lallie, H. S. (2024). Blockchain Forensics: A Systematic Literature Review of Techniques, Applications, Challenges, and Future Directions. Electronics (2079-9292), 13(17), 3568. \url{https://doi.org/10.3390/electronics13173568} & Mittel & Umfassende Analyse von Blockchain-Technologien im digitalen Forensik-Kontext mit Schwerpunkt auf Untersuchungsmethodik und Anwendungsfeldern; adressiert insbesondere die Herausforderungen beim Nachweis und der Verfolgung von Aktivitäten auf Blockchain-Systemen und beleuchtet regulatorische Aspekte, jedoch ohne explizite Behandlung von \ac{SSI}, \ac{PQC} oder den speziellen Anforderungen kritischer Infrastrukturen; liefert wertvolle Einblicke in forensische Anwendungen der Blockchain, bleibt jedoch mit Blick auf innovative Identitäts- und Kryptografielösungen sowie den Schutz kritischer Infrastrukturen im Rahmen der Masterarbeit begrenzt anschlussfähig. \\
\midrule
15 & Yakubu, M. M., Fadzil B Hassan, M., Danyaro, K. U., Junejo, A. Z., Siraj, M., Yahaya, S., Adamu, S., \& Abdulsalam, K. (2024). A Systematic Literature Review on Blockchain Consensus Mechanisms’ Security: Applications and Open Challenges. Computer Systems Science \& Engineering, 48(6), 1437–1481. \url{https://doi.org/10.32604/csse.2024.054556} & Mittel & Umfassende systematische Analyse der Sicherheitsaspekte und Herausforderungen von Blockchain-Konsensmechanismen mit Fokus auf deren Integrität, Zuverlässigkeit und praktische Anwendungen; adressiert die Domäne „Blockchain“ in methodischer Tiefe und liefert wertvolle Erkenntnisse hinsichtlich Sicherheit, Skalierbarkeit und Energieeffizienz von Konsensprotokollen; explizite Bezüge zu \ac{SSI}, \ac{PQC} und spezifisch \ac{KRITIS} fehlen, sodass die Übertragbarkeit auf die weiteren drei Domänen des Masterarbeit-Themas begrenzt ist. \\
\midrule
16 & Oude Roelink, B., El, H. M., \& Sarmah, D. (2024). Systematic review: Comparing zk‐SNARK, zk‐STARK, and bulletproof protocols for privacy‐preserving authentication. Security \& Privacy, 7(5), 1–59. \url{https://doi.org/10.1002/spy2.401} & Mittel & Fokussiert auf den Vergleich und die Analyse moderner Zero-Knowledge-Protokolle (zk-SNARKs, zk-STARKs, Bulletproofs) mit hoher Relevanz für die Domänen Privacy und Blockchain, insbesondere im Kontext von Authentifizierung und Datenschutz; liefert methodische und performancebezogene Einblicke, jedoch ohne explizite Berücksichtigung von \ac{SSI} oder \ac{PQC}; Anbindung an \ac{KRITIS} nicht direkt gegeben, bietet jedoch Potenzial für Integration innovativer Datenschutztechnologien in Blockchain-basierte Identitäts- und Authentifizierungssysteme. \\
\midrule
17 & Cherbal, S., Zier, A., Hebal, S., Louail, L., \& Annane, B. (2024). Security in internet of things: a review on approaches based on blockchain, machine learning, cryptography, and quantum computing. Journal of Supercomputing, 80(3), 3738–3816. \url{https://doi.org/10.1007/s11227-023-05616-2} & Mittel & Umfassende Analyse sicherheitsrelevanter Technologien im IoT-Kontext, darunter Blockchain, Kryptografie und Quantencomputing, mit breitem Überblick zu Ansätzen und Herausforderungen; adressiert insbesondere die Domänen Blockchain und \ac{PQC} durch die Einbeziehung quantenkryptographischer und klassischer kryptographischer Lösungen, ohne explizite Behandlung von \ac{SSI} oder dem spezifischen Anwendungsfeld kritischer Infrastrukturen; liefert wertvolle Vergleiche und Taxonomien zu modernen Sicherheitsmechanismen im IoT, bleibt jedoch hinsichtlich der methodischen und domänenspezifischen Vertiefung für \ac{SSI} und \ac{KRITIS} begrenzt. \\
\midrule
18 & Jagarlamudi, G. K., Yazdinejad, A., Parizi, R. M., \& Pouriyeh, S. (2024). Exploring privacy measurement in federated learning. Journal of Supercomputing, 80(8), 10511–10551. \url{https://doi.org/10.1007/s11227-023-05846-4} & Mittel & Fokussiert auf die Analyse von Privacy-Maßnahmen und deren Messbarkeit im Kontext von föderiertem Lernen, mit hohem methodischen Wert zur Bewertung von Datenschutz und Sicherheitsmetriken; Bezüge zu Blockchain oder \ac{SSI} werden nicht explizit hergestellt, und \ac{PQC} ist nur als Zukunftsperspektive am Rand angedeutet; die behandelten Konzepte und resultierenden Erkenntnisse zu Privacy-Measurement-Methoden können für sichere, dezentrale Systeme (einschließlich kritischer Infrastrukturen) grundsätzlich relevant sein, liefern jedoch keine spezifische oder anwendungsbezogene Vertiefung in den vier Kernbereichen der Masterarbeit. \\
\midrule
19 & Alzoubi, Y. I., Gill, A., \& Mishra, A. (2022). A systematic review of the purposes of Blockchain and fog computing integration: classification and open issues. Journal of Cloud Computing (2192-113X), 11(1), 1–36. \url{https://doi.org/10.1186/s13677-022-00353-y} & Mittel & Systematische Analyse der Integration von Blockchain-Technologien mit Fog Computing zur Lösung sicherheitsrelevanter Herausforderungen in IoT-Anwendungen; adressiert zentrale Aspekte wie Sicherheit, Datenschutz, Zugriffs- und Vertrauensmanagement, jedoch ohne explizite Behandlung von \ac{SSI} oder konkreten Umsetzungen von \ac{PQC}; verweist auf bestehende regulatorische und technologische Herausforderungen durch aufkommende Technologien wie Quantencomputing, aber ohne methodische oder anwendungsbezogene Vertiefung in Bezug auf \ac{KRITIS} oder innovative Identitätskonzepte; bietet wertvolle Klassifikation und Überblick zu Blockchain-Anwendungen im Kontext verteilter Edge-Infrastrukturen, bleibt jedoch bezüglich der vier Kernbereiche der Masterarbeit auf die Domäne „Blockchain“ und allgemeine Sicherheitsaspekte beschränkt. \\
\midrule
20 & Kazmi, S. H. A., Hassan, R., Qamar, F., Nisar, K., \& Ibrahim, A. A. A. (2023). Security Concepts in Emerging 6G Communication: Threats, Countermeasures, Authentication Techniques and Research Directions. Symmetry (20738994), 15(6), 1147. \url{https://doi.org/10.3390/sym15061147} & Mittel & Umfassende Analyse sicherheitsrelevanter Konzepte, Bedrohungen und Authentifizierungsmethoden im Kontext der aufkommenden 6G-Kommunikation; adressiert innovative Technologien wie Künstliche Intelligenz, Quantencomputing und Föderiertes Lernen, wodurch potenzielle Schnittstellen zu \ac{PQC} und Sicherheitsanforderungen für \ac{KRITIS} bestehen; explizite Bezüge zu \ac{SSI} und Blockchain-Technologien fehlen, ebenso eine methodische Vertiefung für spezielle \ac{SSI}- oder Blockchain-basierte Authentifizierungslösungen; bietet wertvolle Einblicke in zukünftige Forschungsrichtungen und technologische Herausforderungen, jedoch begrenzte direkte Anwendbarkeit auf alle vier Kernbereiche der Masterarbeit. \\
\midrule
21 & Attkan, A., \& Ranga, V. (2022). Cyber-physical security for IoT networks: a comprehensive review on traditional, blockchain and artificial intelligence based key-security. Complex \& Intelligent Systems, 8(4), 3559–3591. \url{https://doi.org/10.1007/s40747-022-00667-z} & Mittel & Fokussiert auf Authentifizierung und Schlüsselmanagement in IoT-Netzen unter Einbeziehung klassischer, Blockchain-basierter und KI-gestützter Sicherheitsmechanismen; adressiert im Wesentlichen die Domäne \enquote{Blockchain} und bietet innovative Perspektiven zur dezentralen Verwaltung von Session-Keys sowie zu KI-basierten Angriffserkennungsmethoden im IoT-Kontext; \ac{SSI} und \ac{PQC} werden nicht explizit behandelt, ebenso fehlt die gezielte Anwendung auf \ac{KRITIS}; der umfassende Überblick zu Authentifizierung und Schlüsselmanagement bietet methodische Anschlussmöglichkeiten für \ac{SSI}-basierte Systeme oder kritische Infrastruktur, bleibt im Kern jedoch breit und technologieorientiert ohne vertiefte Ausarbeitung der vier Domänen der Masterarbeit. \\
\midrule
22 & Ray, P. P. (2023). Web3: A comprehensive review on background, technologies, applications, zero-trust architectures, challenges and future directions. Internet of Things \& Cyber Physical Systems, 3, 213–248. \url{https://doi.org/10.1016/j.iotcps.2023.05.003} & Mittel & Bietet einen breiten Überblick über die technologischen und gesellschaftlichen Grundlagen sowie Anwendungsfelder von Web3 und dezentralen Plattformen, wobei die Domäne Blockchain als zentrales Element systematisch behandelt wird; explizite Bezüge zu \ac{SSI} und Zero-Trust-Architekturen existieren, jedoch fehlt eine detaillierte methodische Analyse spezifischer \ac{SSI}-Lösungen oder Implementierungen, und \ac{PQC} wird nicht thematisiert; der Beitrag verweist auf innovative Identitätskonzepte, Anwendungsintegration und technische Herausforderungen, bleibt jedoch hinsichtlich der anwendungsbezogenen Vertiefung zu \ac{PQC} sowie spezifischen Schutzmaßnahmen für \ac{KRITIS} auf einer konzeptionellen Ebene. \\
\midrule
23 & Stach, C., Gritti, C., Bräcker, J., Behringer, M., \& Mitschang, B. (2022). Protecting Sensitive Data in the Information Age: State of the Art and Future Prospects. Future Internet, 14(11), 302. \url{https://doi.org/10.3390/fi14110302} & Mittel & Umfassende Analyse aktueller Privacy-Mechanismen im Kontext datengetriebener Smart Services, mit Fokus auf die praktische Umsetzung nutzerfreundlicher Datenschutzlösungen; adressiert zentrale Herausforderungen und praktische Einsatzfelder moderner Datenschutzverfahren, ohne jedoch explizit auf \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder spezielle Schutzanforderungen kritischer Infrastrukturen einzugehen; liefert wertvolle Einblicke zu datenorientierten Schutzmechanismen und deren Limitationen, bleibt jedoch hinsichtlich der methodischen und domänenspezifischen Vertiefung für \ac{SSI}, \ac{PQC} und \ac{KRITIS} konzeptionell und generisch. \\
\midrule
24 & Farooq, M. S., Riaz, S., \& Alvi, A. (2023). Security and Privacy Issues in Software-Defined Networking (SDN): A Systematic Literature Review. Electronics (2079-9292), 12(14), 3077. \url{https://doi.org/10.3390/electronics12143077} & Mittel & Systematische Analyse von Sicherheits- und Datenschutzproblemen in Software-Defined Networks mit Schwerpunkt auf Schwachstellen, Angriffen und Sicherungsmechanismen entlang der verschiedenen Netzwerkebenen; adressiert grundlegende Herausforderungen für den Schutz moderner Netzwerkarchitekturen, insbesondere durch die Trennung von Steuer- und Datenebene—ein Aspekt, der für den Betrieb kritischer Infrastrukturen relevante Einblicke und Methoden liefert; explizite Bezüge zu Blockchain-Technologien, \ac{SSI} und \ac{PQC} fehlen, jedoch kann die vorgestellte Taxonomie und die Diskussion zukünftiger Forschungsrichtungen methodische Impulse für sichere Integrationskonzepte in verteilten, kritischen oder identitätsgetriebenen Architekturen bieten—die inhaltliche Tiefe und Anwendbarkeit bleibt jedoch primär auf SDN-spezifische Herausforderungen fokussiert. \\
\midrule
25 & Chanal, P. M., \& Kakkasageri, M. S. (2020). Security and Privacy in IoT: A Survey. Wireless Personal Communications, 115(2), 1667–1693. \url{https://doi.org/10.1007/s11277-020-07649-9} & Mittel & Bietet einen umfassenden Überblick über grundlegende Sicherheits- und Datenschutzherausforderungen im Kontext des Internet of Things mit Fokus auf ressourcenbeschränkte Geräte; adressiert Kernaspekte wie Vertraulichkeit, Integrität, Authentifizierung und Verfügbarkeit, ohne jedoch explizit auf \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder spezifische Anforderungen kritischer Infrastrukturen einzugehen; liefert wertvolle konzeptionelle Grundlagen zu Sicherheitsanforderungen und -architekturen im IoT, bleibt jedoch bezüglich anwendungs- oder methodenspezifischer Vertiefung zu den vier Kernbereichen der Masterarbeit generisch. \\
\midrule
26 & Choudhary, A. (2024). Internet of Things: a comprehensive overview, architectures, applications, simulation tools, challenges and future directions. Discover Internet of Things, 4(1), 1–41. \url{https://doi.org/10.1007/s43926-024-00084-3} & Mittel & Umfassende Übersicht und Analyse der IoT-Architektur, Anwendungen und Herausforderungen; adressiert technologische, soziale und funktionale Aspekte des Internet of Things, mit generischer Betrachtung von Architekturen und Simulationsumgebungen; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder spezifischen Schutzanforderungen kritischer Infrastrukturen fehlen; liefert wertvolle Grundlagen für das technologische Umfeld, bleibt jedoch hinsichtlich der vier Domänen der Masterarbeit konzeptionell und unspezifisch. \\
\midrule
27 & Sikiru, I. A., Kora, A. D., Ezin, E. C., Imoize, A. L., \& Li, C.-T. (2024). Hybridization of Learning Techniques and Quantum Mechanism for IIoT Security: Applications, Challenges, and Prospects. Electronics (2079-9292), 13(21), 4153. \url{https://doi.org/10.3390/electronics13214153} & Mittel & Systematische Analyse hybrider Sicherheitsansätze in der Industrial IoT (IIoT), insbesondere durch die Kombination klassischer Lernverfahren und quantenmechanistischer Ansätze; adressiert relevante Herausforderungen und Perspektiven der IIoT-Sicherheit mit Berücksichtigung von Blockchain-Technologien und Quantum Mechanisms, wobei \ac{PQC} eher implizit thematisiert wird; explizite Vertiefung von \ac{SSI} und spezifische Anwendungen in \ac{KRITIS} fehlen, liefert jedoch Impulse für die Integration moderner Kryptografie- und Sicherheitsverfahren im industriellen Umfeld. \\
\midrule
28 & RadRadanliev, P. (2024). Artificial intelligence and quantum cryptography. Journal of Analytical Science \& Technology, 14, 1–17. \url{https://doi.org/10.1186/s40543-024-00416-6} & Mittel & Thematisiert den aktuellen Stand und die Zukunftsperspektiven an der Schnittstelle von künstlicher Intelligenz und quantenkryptografischen Verfahren, wobei insbesondere der Einfluss von AI-Methoden auf Effizienz und Robustheit kryptografischer Systeme sowie die Herausforderungen durch das \enquote{Quantum Threat}-Szenario im Zentrum stehen; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien oder spezifischen Anwendungsfällen in \ac{KRITIS} fehlen; liefert wertvolle Impulse zu methodischen Innovationen in der \ac{PQC}, bleibt jedoch hinsichtlich der vier Domänen der Masterarbeit primär konzeptionell und technologisch fokussiert auf AI und Quantenkryptografie. \\
\midrule
29 & O’Donoghue, O., Vazirani, A. A., Brindley, D., \& Meinert, E. (2019). Design Choices and Trade-Offs in Health Care Blockchain Implementations: Systematic Review. Journal of Medical Internet Research, 21(5), e12426. \url{https://doi.org/10.2196/12426} & Mittel & Systematische Analyse von Architektur- und Design-Entscheidungen bei der Implementierung von Blockchain-Technologie im Kontext elektronischer Gesundheitsakten (EMR), mit Schwerpunkt auf sicherheitsrelevanten und skalierbaren Systemanforderungen sowie Trade-offs zwischen verschiedenen technischen, organisatorischen und anwendungsbezogenen Merkmalen; behandelt die Domäne Blockchain umfassend und liefert wertvolle Erkenntnisse über sicherheitsrelevante Kompromisse im Gesundheitswesen, adressiert jedoch weder \ac{SSI} noch \ac{PQC} oder \ac{KRITIS} explizit; die Untersuchung des Spannungsfelds zwischen Sicherheit, Skalierbarkeit und Datenmanagement bildet eine methodisch relevante Grundlage, bleibt aber in Bezug auf die vier zentralen Domänen der Masterarbeit auf anwendungsbezogene Blockchain-Implementierungen beschränkt. \\
\midrule
30 & Mulholland, J., Mosca, M., \& Braun, J. (2017). The Day the Cryptography Dies. IEEE Security \& Privacy, 15(4), 14–21. \url{https://doi.org/10.1109/MSP.2017.3151325} & Mittel & Überblickartige Darstellung der Auswirkungen von Quantencomputern auf bestehende kryptografische Verfahren; adressiert explizit die Bedrohung aktueller Sicherheitstechnologien (\ac{PQC}), jedoch ohne methodische oder technologische Vertiefung zu Blockchain, \ac{SSI} oder \ac{KRITIS}; bietet konzeptionelle Einblicke in Risikoszenarien und Bedrohungsmodelle, bleibt jedoch hinsichtlich innovativer Lösungsansätze oder spezifischer Anwendungsgebiete der vier Domänen der Masterarbeit unspezifisch. \\
\midrule
31 & G, C. A., \& Basarkod, P. I. (2024). A survey on blockchain security for electronic health record. Multimedia Tools and Applications: An Journal, 1–35. \url{https://doi.org/10.1007/s11042-024-19883-5} & Mittel & Fokus auf Blockchain-basierte Sicherheitslösungen für elektronische Gesundheitsakten (EHR) mit Einbindung von Deep-Learning-Methoden; adressiert primär die Domäne Blockchain durch Analyse von Datenschutz, Datensicherheit und Zugriffskontrolle im Gesundheitswesen, bietet wertvolle methodische Einblicke zur Anwendung verteilter Technologien im Bereich sensibler Daten; explizite Bezüge zu \ac{SSI}, \ac{PQC} und spezifisch \ac{KRITIS} außerhalb des Gesundheitssektors fehlen, wodurch die Anwendbarkeit auf alle vier Domänen der Masterarbeit beschränkt bleibt. \\
\midrule
32 & Batta, P., Ahuja, S., \& Kumar, A. (2024). Future Directions for Secure IoT Frameworks: Insights from Blockchain-Based Solutions: A Comprehensive Review and Future Analysis. Wireless Personal Communications: An International Journal, 139(3), 1749–1781. \url{https://doi.org/10.1007/s11277-024-11694-z} & Mittel & Systematische Untersuchung sicherer IoT-Frameworks unter Verwendung von Blockchain-Technologien; detaillierte Analyse verschiedener Algorithmen (u.a. Konsensmechanismen, \ac{RSA}, Hashing) und Plattformen (Ethereum, CoSMOS, Hyperledger Fabric), mit Fokus auf die Verbesserung von Sicherheit und Performance in IoT-Systemen; explizite Bezüge zu \ac{SSI}, \ac{PQC} und den besonderen Anforderungen kritischer Infrastrukturen fehlen; adressiert vor allem die Domäne „Blockchain“ und liefert grundlegende Einblicke zur Anwendung verteilter Sicherheitsmechanismen im IoT, bleibt jedoch hinsichtlich der vier Kerndomänen der Masterarbeit ( \ac{SSI}, Blockchain, \ac{PQC}, \ac{KRITIS}) methodisch und domänenspezifisch eingeschränkt. \\
\midrule
33 & Radanliev, P. (2024). Integrated cybersecurity for metaverse systems operating with artificial intelligence, blockchains, and cloud computing. Frontiers in Blockchain, 1–14. \url{https://doi.org/10.3389/fbloc.2024.1359130} & Mittel & Umfassende Analyse der Cybersicherheitslandschaft im Kontext integrierter Metaverse-Systeme unter Einbezug von Artificial Intelligence, Blockchain und Cloud Computing; adressiert zentrale Risikofelder, regulatorische Herausforderungen und die Rolle moderner Sicherheitstechnologien für die digitale Ökonomie, wobei insbesondere Blockchain in seiner Bedeutung für selbstverwaltete Systeme und Netzwerkgovernance diskutiert wird; explizite Vertiefungen zu \ac{SSI}, \ac{PQC} oder deren spezieller Anwendung im Umfeld kritischer Infrastrukturen fehlen, ebenso bleibt die methodische und technologische Anbindung an innovative Identitäts- und Kryptografieansätze im Rahmen der vier Domänen der Masterarbeit auf konzeptionelle Ausblicke beschränkt. \\
\midrule
34 & Hajian Berenjestanaki, M., Barzegar, H. R., El Ioini, N., \& Pahl, C. (2024). Blockchain-Based E-Voting Systems: A Technology Review. Electronics (2079-9292), 13(1), 17. \url{https://doi.org/10.3390/electronics13010017} & Mittel & Systematische Analyse von Blockchain-basierten E-Voting-Systemen mit Fokus auf Sicherheits-, Transparenz- und Integritätsaspekte; zentrale Bewertung technologischer Herausforderungen und zukünftiger Forschungsfragen, insbesondere zu Skalierbarkeit und Datenschutz – thematisch eng an die Domäne „Blockchain“ angelehnt; explizite Vertiefung von \ac{SSI}, \ac{PQC} oder die Anwendung in besonders schützenswerten \ac{KRITIS} fehlt, bietet jedoch methodische Ansätze und technische Perspektiven, die für die Entwicklung sicherer und vertrauenswürdiger Abstimmungssysteme in digitalen Infrastrukturen relevant sein können. \\
\midrule
35 & Pirbhulal, S., Chockalingam, S., Shukla, A., \& Abie, H. (2024). IoT cybersecurity in 5G and beyond: a systematic literature review. International Journal of Information Security, 23(4), 2827–2879. \url{https://doi.org/10.1007/s10207-024-00865-5} & Mittel & Systematische Literaturübersicht zu Cybersicherheitsaspekten in 5G- und Next-Generation-IoT-Umgebungen, insbesondere hinsichtlich Threats, Authentifizierung, Zugriffskontrolle, Netzwerk- und Anwendungsschicht sowie Herausforderungen durch Softwarisierung und Virtualisierung der Netze; adressiert methodisch den aktuellen Forschungsstand, evaluiert genutzte Validierungsansätze (Praxis, Simulation, Theorie) und liefert ein Kategorienschema für existierende Sicherheitsmechanismen und offene Forschungsfragen; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien oder \ac{PQC} fehlen, ebenso eine gezielte Betrachtung von \ac{KRITIS}—die Branchenbeispiele (z. B. Healthcare, Energie) lassen eine indirekte Bedeutung für KRITIS erkennen, ohne diese jedoch methodisch zu vertiefen; methodische und technologische Tiefe für die vier Masterarbeitsdomänen beschränkt sich auf generische Cybersicherheitsbedrohungen und Lösungsansätze in modernen IoT/5G-Systemen. \\
\midrule
36 & Asif, M., Abrar, M., Salam, A., Amin, F., Ullah, F., Shah, S., \& AlSalman, H. (2025). Intelligent two-phase dual authentication framework for Internet of Medical Things. Scientific Reports, 15(1), 1–19. \url{https://doi.org/10.1038/s41598-024-84713-5} & Mittel & Fokus liegt auf der Entwicklung und Evaluierung eines intelligenten Zwei-Phasen-Authentifizierungsframeworks für die Internet of Medical Things (IoMT) mit Ziel der effizienten und sicheren Kommunikation sensibler Gesundheitsdaten; zentrale technische Ansätze umfassen ECDH für Schlüsselaustausch und AES-GCM für Datenverschlüsselung, wobei signifikante Verbesserungen in Effizienz und Sicherheit gegenüber klassischen Authentifizierungsmethoden nachgewiesen werden; explizite Bezüge zu \ac{SSI} und Blockchain-Technologien sowie \ac{PQC} fehlen vollständig, sodass methodische und technologische Innovationen in diesen Bereichen für den Rahmen der Masterarbeit unberücksichtigt bleiben; adressiert Schutzanforderungen im Bereich kritischer Infrastrukturen exemplarisch am Gesundheitswesen, bleibt jedoch in der Tiefe auf klassische kryptografische Verfahren und Authentifizierungsprozesse limitiert. \\
\midrule
37 & Marengo, A., \& Santamato, V. (2025). Quantum algorithms and complexity in healthcare applications: a systematic review with machine learning-optimized analysis. Frontiers in Computer Science, 1–30. \url{https://doi.org/10.3389/fcomp.2025.1584114} & Mittel & Systematische Übersicht zur Anwendung von Quantenalgorithmen und quanten-inspirierten Komplexitätsanalysen im Gesundheitswesen, mit zwei Schwerpunkten: (1) Quantum Computing für KI-basierte Analysen biomedizinischer Daten und (2) quantenkryptografische Protokolle zur Absicherung medizinischer Daten. Expliziter Bezug zur Domäne \ac{PQC} durch Analyse quantensicherer und blockchain-basierter Sicherheitsmechanismen im Healthcare-Kontext. Keine explizite Behandlung von \ac{SSI} oder dezidierten Blockchain-Architekturen außerhalb sicherheitsrelevanter Frameworks; Anwendung auf \ac{KRITIS} implizit durch den Fokus auf sichere medizinische Systeme, jedoch nicht technologieübergreifend vertieft. Insgesamt methodisch relevant für die Domäne \ac{PQC} und für Sicherheitsthemen im medizinisch-kritischen Sektor, für die vier Themenbereiche der Masterarbeit aber primär im Bereich quantensicherer Daten- und KI-Anwendungen anschlussfähig. \\
\midrule
38 & Ahakonye, L. A. C., Nwakanma, C. I., \& Kim, D.-S. (2024). Tides of Blockchain in IoT Cybersecurity. Sensors (14248220), 24(10), 3111. \url{https://doi.org/10.3390/s24103111} & Mittel & Umfassende Übersicht zu Anwendungsmöglichkeiten und Herausforderungen von Blockchain-Technologie im Bereich der IoT-Cybersicherheit, insbesondere in Verbindung mit KI-unterstützten Intrusion Detection Systemen; adressiert die Domäne „Blockchain“ grundlegend sowie deren Potenzial für Transparenz, Dezentralität und Unveränderlichkeit im IoT-Kontext; Integration von AI und Blockchain als Innovationstreiber für sichere und skalierbare IDS-Lösungen im IoT/IIoT; \ac{SSI} und \ac{PQC} werden nicht explizit behandelt; spezifische Anforderungen und Anwendungsfälle für \ac{KRITIS} werden nur indirekt adressiert; liefert wertvolle Einblicke und methodische Ansätze für die Weiterentwicklung sicherer IoT-Systeme, bleibt jedoch hinsichtlich der vier Domänen der Masterarbeit vorwiegend auf Blockchain und allgemeine Sicherheitsthemen im IoT fokussiert. \\
\midrule
39 & Nain, A., Sheikh, S., Shahid, M., \& Malik, R. (2024). Resource optimization in edge and SDN-based edge computing: a comprehensive study. Cluster Computing, 27(5), 5517–5545. \url{https://doi.org/10.1007/s10586-023-04256-8} & Niedrig & Umfassende systematische Analyse aktueller Optimierungsansätze für Ressourcenmanagement in Edge-Computing-Umgebungen, insbesondere unter Integration von Software-Defined Networking (SDN); adressiert zentrale Herausforderungen der effizienten Ressourcennutzung, Kontrollarchitekturen und Netzwerkprogrammierbarkeit, was insbesondere für leistungsfähige, latenzarme Anwendungen und Systemarchitekturen an der Netzwerkkante relevant ist; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder den besonderen Anforderungen kritischer Infrastrukturen fehlen; liefert dennoch wertvolle methodische Impulse für das Design verteilter, dynamischer Infrastrukturen, bleibt aber in Bezug auf die vier Domänen der Masterarbeit überwiegend allgemein und technologieorientiert. \\
\midrule
40 & Netinant, P., Saengsuwan, N., Rukhiran, M., \& Pukdesree, S. (2023). Enhancing Data Management Strategies with a Hybrid Layering Framework in Assessing Data Validation and High Availability Sustainability. Sustainability (2071-1050), 15(20), 15034. \url{https://doi.org/10.3390/su152015034} & Niedrig & Betrachtet Methoden zur nachhaltigen und hochverfügbaren Datenmigration, insbesondere durch ein hybrides Layering-Framework im Kontext von Data Management und Data Validation; adressiert primär Herausforderungen und Optimierungsstrategien im Bereich Datenkonsistenz, Datenintegrität und Verfügbarkeitsmanagement bei Migration und Transformation—relevant für unternehmensweite IT-Systeme und Logistikdaten; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder besonderen Anforderungen kritischer Infrastrukturen fehlen vollständig; liefert wertvolle Erkenntnisse zur Bewertung und Ausgestaltung von Datenmigrationsprozessen, bleibt jedoch hinsichtlich der vier Domänen der Masterarbeit methodisch und inhaltlich unberührt. \\
\midrule
41 & Trautman, L. J., Shackelford, S., Elzweig, B., \& Ormerod, P. (2024). Understanding Cyber Risk: Unpacking and Responding to Cyber Threats Facing the Public and Private Sectors. University of Miami Law Review, 78(3), 840–916. \url{https://repository.law.miami.edu/umlr/vol78/iss3/5/} & Niedrig & Umfassende Analyse aktueller Cyberbedrohungen und deren Auswirkungen auf öffentliche und private Sektoren, mit Fokus auf Angriffsszenarien (u.a. Ransomware, Cyberwarfare, Datenlecks), regulatorische und unternehmensbezogene Steuerungsmechanismen sowie das Zusammenspiel von Recht, Unternehmensführung und geopolitischen Risiken; adressiert zentrale Aspekte der Cyber-Risikobewertung und Reaktion auf digitale Angriffe, insbesondere aus administrativer und juristischer Perspektive; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien, \ac{PQC} sowie spezifische Schutzmaßnahmen für \ac{KRITIS} fehlen; bietet wertvolle konzeptionelle Grundlagen im Bereich Cybersicherheit, Governance und Compliance, bleibt jedoch hinsichtlich der vier Schwerpunktdomänen der Masterarbeit in methodischer und technologischer Tiefe eingeschränkt. \\
\midrule
42 & Hendaoui, F., Ferchichi, A., Trabelsi, L., Meddeb, R., Ahmed, R., \& Khelifi, M. K. (2024). Advances in deep learning intrusion detection over encrypted data with privacy preservation: a systematic review. Cluster Computing, 27(7), 8683–8724. \url{https://doi.org/10.1007/s10586-024-04424-4} & Niedrig & Systematische Analyse der Fortschritte im Bereich Deep-Learning-basierter Intrusion Detection über verschlüsselte Daten mit Fokus auf Privacy-Preservation; behandelt innovative Ansätze zur Anomalieerkennung in verschlüsselten Datenströmen durch tiefe neuronale Netze, ohne auf Datenentschlüsselung angewiesen zu sein; explizite Bezüge zu \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder dem Schutz kritischer Infrastrukturen fehlen; bietet wertvolle methodische und technologische Impulse zur sicheren Datenverarbeitung und Angriffserkennung in datenschutzorientierten Systemen, bleibt aber hinsichtlich der vier Kerndomänen der Masterarbeit überwiegend auf den Bereich Deep Learning und Privacy-Preserving IDS fokussiert \\
\midrule
43 & Akartuna, E. A., Johnson, S. D., \& Thornton, A. E. (2023). The money laundering and terrorist financing risks of new and disruptive technologies: a futures-oriented scoping review: The money laundering and terrorist financing risks of new and disruptive technologies: a futures-oriented scoping review. Security Journal, 36(4), 615–650. \url{https://doi.org/10.1057/s41284-022-00356-z} & Niedrig & Systematische Analyse von Geldwäsche- und Terrorismusfinanzierungsrisiken im Zusammenhang mit neuen und disruptiven Technologien, insbesondere Distributed-Ledger-Technologien (inkl. Kryptowährungen), neue Zahlungswege und FinTech; behandelt umfassend die Risiken, Methoden sowie betroffene Akteure und skizziert daraus resultierende Trends und politische Implikationen – mit klarem Bezug zur Domäne \enquote{Blockchain} und angrenzender regulatorischer Herausforderungen; explizite Vertiefungen zu \ac{SSI} oder \ac{PQC} fehlen, ebenso eine gezielte Betrachtung kritischer Infrastrukturen im engeren technischen Sinne; liefert wichtige Einblicke für die Risiko- und Bedrohungsanalyse im Kontext innovativer Finanztechnologien, bleibt aber hinsichtlich der methodischen Tiefe und direkten Anwendbarkeit für die vier Domänen der Masterarbeit beschränkt. \\
\midrule
44 & Zboril, M., \& Svatá, V. (2025). Performance comparison of cloud virtual machines. Journal of Systems \& Information Technology, 27(2), 197–213. \url{https://doi.org/10.1108/JSIT-02-2022-0040} & Niedrig & Thematischer Fokus liegt auf der vergleichenden Performancemessung von Cloud-basierten virtuellen Maschinen (VMs) bei AWS, Microsoft Azure und Google Cloud Platform, basierend auf Benchmark-Tests unter Linux; adressiert ausschließlich Infrastruktur- und Leistungsaspekte von Cloud-Diensten sowie Auswahlkriterien für IT-Betriebsmodelle; keine Verbindung zu den vier Domänen der Masterarbeit, da keine sicherheitsrelevanten, kryptografischen oder identitätsbezogenen Aspekte behandelt werden; relevante Erkenntnisse für Cloud-Infrastrukturmanagement und Benchmarking, jedoch methodisch und inhaltlich außerhalb des Kernbereichs der Masterarbeit. \\
\midrule
45 & Radanliev, P. (2024). The rise and fall of cryptocurrencies: defining the economic and social values of blockchain technologies, assessing the opportunities, and defining the financial and cybersecurity risks of the Metaverse. Financial Innovation, 10, 1–34. \url{https://doi.org/10.1186/s40854-023-00537-8} & Niedrig & Umfassende Analyse der wirtschaftlichen, sozialen und technologischen Aspekte von Blockchain-Technologien, insbesondere im Kontext von Kryptowährungen und deren Rolle im Metaverse; untersucht wirtschaftliche Chancen, Investitionsstrategien und Cybersecurity-Risiken mit interdisziplinärem Ansatz, inklusive Risikobewertung und maschinellem Lernen im Finanzsektor; explizite Bezüge zu \ac{SSI}, \ac{PQC} und dem Schutz kritischer Infrastrukturen fehlen, ebenso eine methodische oder technologische Vertiefung zu innovativen Identitäts- oder Kryptografielösungen—fokussiert primär auf ökonomische und anwendungsbezogene Fragestellungen der Blockchain im Finanz- und Metaverse-Umfeld. \\
\midrule
46 & Bunescu, L., \& Vârtei, A. M. (2024). Modern finance through quantum computing—A systematic literature review. PLoS ONE, 19(7), 1–22. \url{https://doi.org/10.1371/journal.pone.0304317} & Niedrig & Systematische Analyse des Einsatzes von Quantencomputing im Finanzsektor, mit Fokus auf Simulation, Optimierung und maschinelles Lernen; adressiert Kernaspekte der \ac{PQC} im Hinblick auf die transformative Wirkung quantenbasierter Technologien, ohne dabei explizit auf Blockchain-Technologien oder \ac{SSI} einzugehen; liefert wertvolle Einblicke in die methodische und anwendungsbezogene Entwicklung von Quantum Finance, bleibt jedoch hinsichtlich der vier thematischen Domänen der Masterarbeit ( \ac{SSI}, Blockchain, \ac{PQC}, \ac{KRITIS}) primär auf Finanzanwendungen und damit nur partiell anschlussfähig. \\
\midrule
47 & Alzoubi, Y. I., Mishra, A., \& Topcu, A. E. (2024). Research trends in deep learning and machine learning for cloud computing security. Artificial Intelligence Review: An International Science and Engineering Journal, 57(5). \url{https://doi.org/10.1007/s10462-024-10776-5} & Niedrig & Fokussiert auf den Einsatz von Deep-Learning- und Machine-Learning-Technologien zur Identifikation und Bewältigung von Cloud-Sicherheitsbedrohungen; adressiert zentrale Herausforderungen wie Anomalieerkennung, Security Automation und die Integration neuer Technologien, ohne jedoch explizit \ac{SSI}, Blockchain-Ansätze oder \ac{PQC} systematisch einzubinden; hebt methodische, datenschutzbezogene und regulatorische Fragestellungen hervor, die für den Schutz kritischer Infrastrukturen relevant sind, bleibt jedoch bezüglich der vier Kernbereiche der Masterarbeit hauptsächlich auf Cloud Security und AI-gestützte Verfahren konzentriert und bietet nur indirekte Anschlussmöglichkeiten für innovative Kryptografie- oder Identitätslösungen \\
\midrule
48 & Williamson, S. M., \& Prybutok, V. (2024). Balancing Privacy and Progress: A Review of Privacy Challenges, Systemic Oversight, and Patient Perceptions in AI-Driven Healthcare. Applied Sciences (2076-3417), 14(2), 675. \url{https://doi.org/10.3390/app14020675} & Niedrig & Kritische Analyse von Datenschutz-, Ethik- und Compliance-Herausforderungen in AI-gestützten Gesundheitssystemen, mit Fokus auf Differential Privacy und patientenzentrierte Datenverarbeitung; adressiert relevante technologische Ansätze wie Verschlüsselung und Differential Privacy sowie organisatorische und regulatorische Rahmenbedingungen—beinhaltet zudem die Herausforderungen bei der Integration von Blockchain-Technologien im healthcare-spezifischen Kontext und deren Vereinbarkeit mit der DSGVO, wodurch ein übergreifender Bezug zur Domäne Blockchain gegeben ist; \ac{SSI} und \ac{PQC} werden nicht explizit behandelt, ebenso steht die Anbindung an \ac{KRITIS} außerhalb des engeren Fokus; bietet wertvolle Erkenntnisse zu datenschutzgerechter Systemgestaltung und Interdisziplinarität im Gesundheitswesen, bleibt aber hinsichtlich methodischer Tiefe und anwendungsbezogener Integration in allen vier Kernbereichen der Masterarbeit überblicksartig und konzeptionell. \\
\midrule
49 & Tukur, M., Schneider, J., Househ, M., Dokoro, A. H., Ismail, U. I., Dawaki, M., \& Agus, M. (2023). The metaverse digital environments: a scoping review of the challenges, privacy and security issues. Frontiers in Big Data, 1–25. \url{https://doi.org/10.3389/fdata.2023.1301812} & Niedrig & Umfassende Übersicht zu Herausforderungen, Datenschutz- und Sicherheitsfragen bei der Entwicklung und Implementierung von Metaverse-Umgebungen, insbesondere infolge der pandemiebedingten Digitalisierungsschübe; adressiert wirtschaftliche, technische, ethische und soziale Herausforderungen, darunter Hard- und Softwarekosten, digitale Ungleichheit sowie Regelwerks- und Datenmanagement-Fragen; konkrete Analyse und Klassifikation von Policy-, Privacy- und Security-Problemen, mit Fokus auf privatsphärenbezogene Risiken und Governance-Anforderungen im Metaverse. Explizite Bezüge zu \ac{SSI}, Blockchain-Technologien und \ac{PQC} fehlen; \ac{KRITIS} werden nur implizit durch den Verweis auf digitale Spaltungen und gesellschaftliche Implikationen berührt. \\
\midrule
50 & Hanafi, B., Ali, M., \& Singh, D. (2025). Quantum algorithms for enhanced educational technologies. Discover Education, 4(1), 1–33. \url{https://doi.org/10.1007/s44217-025-00400-1} & Niedrig & Fokus auf die Potenziale und Herausforderungen von Quantum Computing und Quantenkryptografie in der Bildungsbranche, z. B. für personalisiertes Lernen und sichere Datenübertragung; Bezug zu \ac{PQC} nur anwendungsbezogen im Bildungskontext, ohne technische Tiefe oder Bezug zu \ac{SSI}, Blockchain oder \ac{KRITIS}; aus Sicht der vier Masterarbeitsdomänen methodisch und thematisch nur sehr eingeschränkt anschlussfähig. \\
\midrule
51 & Pillai, S. E. V. S., Nadella, G. S., Meduri, K., Priyadharsini, N. A., Bhuvanesh, A., \& Kumar, D. (2025). A walrus optimization-enhanced long short-term memory model for credit fraud detection in banking. International Journal of Information Technology: An Official Journal of Bharati Vidyapeeth’s Institute of Computer Applications and Management, 1–17. \url{https://doi.org/10.1007/s41870-025-02574-1} & Niedrig & Beschreibung einer innovativen Framework-Kombination aus Autoencoder, Long Short-Term Memory Netzwerken und Walrus Optimization Algorithm zur Verbesserung der Betrugserkennung im Bankensektor; konzentriert sich ausschließlich auf Machine-Learning-gestützte Analyse, Datenvorverarbeitung und Hyperparameteroptimierung zur Echtzeit-Erkennung betrügerischer Transaktionen in großen Datenmengen; keinerlei Behandlung oder Integration der vier Domänen der Masterarbeit; relevante methodische Beiträge beschränken sich auf KI-basierte Fraud Detection, ohne Anschlusspunkte zu den Kernthemen der Masterarbeit. \\
\midrule
52 & Priya, S. S., Vijayabhasker, R., \& Rajaram, A. (2025). Advanced Security and Efficiency Framework for Mobile Ad-Hoc Networks Using Adaptive Clustering and Optimization Techniques. Journal of Electrical Engineering \& Technology (19750102), 20(3), 1815–1826. \url{https://doi.org/10.1007/s42835-024-02119-9} & Niedrig & Fokus auf ein innovatives Sicherheits- und Effizienz-Framework für Mobile Ad-Hoc Networks (MANETs) durch adaptive Clusterbildung, AI-unterstützte Vertrauensbewertung und quantenresistente PUF-Authentifizierung; explizite Relevanz für \ac{PQC} durch QR-PUF-Komponente; \ac{SSI} und Blockchain werden nicht behandelt, ebenso fehlt eine gezielte Betrachtung kritischer Infrastrukturen; für die vier Kerndomänen der Masterarbeit somit vor allem im Kontext quantensicherer Validierung/mobiler Netzwerksicherheit anschlussfähig, ansonsten methodisch und domänenspezifisch eingeschränkt. \\
\midrule
53 & Berkani, A.-S., Moumen, H., Benharzallah, S., Yahiaoui, S., \& Bounceur, A. (2024). Blockchain Use Cases in the Sports Industry: A Systematic Review. International Journal of Networked \& Distributed Computing, 12(1), 17–40. \url{https://doi.org/10.1007/s44227-024-00022-3} & Niedrig & Fokus auf branchenspezifische Anwendungen der Blockchain-Technologie im Sportsektor (Athleten-Datenmanagement, Fandaten, NFT-Sammlerstücke); methodische und technologische Vertiefung im Hinblick auf \ac{SSI}, \ac{PQC} oder \ac{KRITIS} fehlt; relevante Erkenntnisse nur für den Bereich Blockchain-Anwendungsfälle in Sport und Entertainment, für die vier Domänen der Masterarbeit jedoch insgesamt wenig anschlussfähig. \\
\midrule
54 & 2023 PNS Annual Meeting - Copenhagen, 17-20 June 2023. (2023). Journal of the Peripheral Nervous System: JPNS, 28 Suppl 4, S3–S254. \url{https://doi.org/10.1111/jns.12585} & Niedrig & Konferenzband ohne Bezug zu \ac{SSI}, Blockchain oder \ac{PQC}. \\
\midrule
55 & Posters. (2017). FEBS Journal, 284, 102–403. \url{https://doi.org/10.1111/febs.14174} & Niedrig & Posterband, kein Bezug zu \ac{SSI}, Blockchain oder \ac{PQC}. \\
\midrule
56 & Annotated Listing of New Books. (2024). Journal of Economic Literature, 62(4), 1696–1750. \url{https://doi.org/10.1257/jel.62.4.1696} & Niedrig & Buchliste, kein Bezug zum Thema. \\
\midrule
57 & PNS Abstracts 2023. (2023). Journal of the Peripheral Nervous System, 28, S3–S254. \url{https://doi.org/10.1111/jns.12585} & Niedrig & Abstractband, kein Bezug zu SSI, Blockchain oder \ac{PQC}. \\
\midrule
58 & Elendu, C., Omeludike, E. K., Oloyede, P. O., Obidigbo, B. T., \& Omeludike, J. C. (2024). Legal implications for clinicians in cybersecurity incidents: A review. Medicine, 103(39), 1–26. \url{https://doi.org/10.1097/MD.0000000000039887} & Niedrig & Fokus auf die rechtlichen Implikationen von Cybersecurity-Vorfällen im Gesundheitswesen, insbesondere für klinisch tätige Personen; Betrachtung technologischer Entwicklungen (u. a. künstliche Intelligenz und Quantencomputing) sowie internationaler regulatorischer Unterschiede; praxisnahe Empfehlungen und Fallstudien zu Cybersecurity-Management, ethischen und juristischen Aspekten im Gesundheitssektor; keine explizite Behandlung von \ac{SSI}, Blockchain oder \ac{PQC}, \ac{KRITIS} werden durch den Gesundheitsbereich berührt, Schwerpunkt liegt jedoch auf juristischen und ethischen Fragestellungen. \\
\midrule
59 & Albshaier, L., Almarri, S., \& Hafizur Rahman, M. M. (2024). A Review of Blockchain’s Role in E-Commerce Transactions: Open Challenges, and Future Research Directions. Computers (2073-431X), 13(1), 27. \url{https://doi.org/10.3390/computers13010027} & Niedrig & Fokus auf die Anwendung von Blockchain-Technologien zur Verbesserung von Sicherheit, Transparenz und Betrugserkennung in E-Commerce-Transaktionen; betont die Rolle verteilter, unveränderlicher digitaler Ledger für den Schutz sensibler Kundendaten und die Stärkung des Vertrauens in Online-Plattformen; adressiert die Domäne „Blockchain“ vorrangig, ohne explizite Bezüge zu \ac{SSI} oder \ac{PQC}; \ac{KRITIS} werden nicht thematisiert, da der Schwerpunkt auf E-Commerce liegt; methodische und technologische Tiefe für die vier Domänen der Masterarbeit ist auf Blockchain-Anwendungen im Bereich E-Commerce beschränkt. \\
\midrule
60 & Reddy, R. C., Bhattacharjee, B., Mishra, D., \& Mandal, A. (2022). A systematic literature review towards a conceptual framework for enablers and barriers of an enterprise data science strategy. Information Systems \& e-Business Management, 20(1), 223–255. \url{https://doi.org/10.1007/s10257-022-00550-x} & Niedrig & Fokus liegt auf der systematischen Analyse von Erfolgsfaktoren und Hindernissen bei der unternehmensweiten Einführung von Data-Science-Strategien; methodische Entwicklung eines Enabler-Barrier-Frameworks für die erfolgreiche Umsetzung datengetriebener Projekte in Unternehmen; adressiert dabei organisatorische, technologische und strategische Aspekte der digitalen Transformation im breiten Kontext, jedoch ohne explizite Behandlung oder Integration von \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder spezifischen Schutzanforderungen kritischer Infrastrukturen; liefert wertvolle Erkenntnisse zur Implementierung von Data Science im Unternehmensumfeld, ist für die vier Domänen der Masterarbeit methodisch und thematisch jedoch nicht anschlussfähig. \\
\midrule
61 & Kumar, Y., Marchena, J., Awlla, A. H., Li, J. J., \& Abdalla, H. B. (2024). The AI-Powered Evolution of Big Data. Applied Sciences (2076-3417), 14(22), 10176. \url{https://doi.org/10.3390/app142210176} & Niedrig & Fokus auf die Weiterentwicklung von Big-Data-Analyse und Management durch künstliche Intelligenz, mit Betonung neuer Rahmenwerke für die Charakterisierung und Handhabung großer, komplexer Datensätze. Der Beitrag stellt innovative AI-gestützte Tools (wie RAG-basierte Analyse-Bots/ChatGPT) zur Verbesserung der Datenanalyse vor und hebt methodologische Fortschritte im Bereich datengetriebene Entscheidungsunterstützung hervor. Keine explizite Behandlung oder Integration von \ac{SSI}, Blockchain-Technologien, \ac{PQC} oder besonderen Anforderungen kritischer Infrastrukturen; methodische und technologische Beiträge beschränken sich auf Big-Data-Management und AI-basierte Analytics, ohne Verknüpfung zu den vier zentralen Domänen der Masterarbeit. \\
\end{longtable}

\pagebreak

\subsection{Zweite Iteration vom 02. November 2025}
\label{sec:Anhang_Dokumentation_der_zweiten_Iteration}

Die zweite Iteration erfolgt im Rahmen der Masterarbeit unter Berücksichtigung der in der ersten Iteration definierten ausgewählten Methoden der PRISMA 2020 Richtlinien (\autoref{tab:Ausgewählte Methoden der PRISMA 2020 Richtlinien}).

Angepasst wurde der Suchzeitraum (Zeitrahmen: 30. Mai 2025 bis 02. November 2025) innerhalb der Ein- und Ausschlusskriterien, um die Wissensbasis zu aktualisieren und neu erschienene Publikationen im dynamischen Forschungsfeld der Post-Quantum-Kryptografie und blockchain-basierten SSI-Systeme zu erfassen (\autoref{tab:einausschlusskriterien_iteration2}).

\begin{longtable}{L{3cm}L{4cm}L{4cm}L{3cm}}
    \caption{Ein- und Ausschlusskriterien für die systematische Literaturrecherche}
    \label{tab:einausschlusskriterien_iteration2} \\
    \toprule
    \textbf{Kategorie} & \textbf{Einschluss} & \textbf{Ausschluss} & \textbf{Begründung} \\
    \midrule
    \endfirsthead
    \multicolumn{4}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Kategorie} & \textbf{Einschluss} & \textbf{Ausschluss} & \textbf{Begründung} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{4}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{4}{p{\linewidth}}{\textit{Anmerkung.} Eigene Darstellung.} \\
    \endlastfoot
    Thematischer Fokus &
    \ac{SSI} und dezentrale Identitätslösungen;
    Blockchain-basierte Identitätsmanagementsysteme;
    \ac{PQC} und quantensichere Algorithmen;
    Sicherheit und Compliance in \ac{KRITIS};
    \ac{DSR}-Methodik in IT/Informationssystemen;
    Kryptoagilität und kryptografische Migration & 
    Identitätsmanagement ohne Bezug zu \ac{SSI} oder Blockchain;
    Klassische \ac{PKI} ohne \ac{PQC}-Bezug;
    Kryptografie ohne Post-Quantum-Relevanz;
    Arbeiten ohne Bezug zu \ac{KRITIS} oder ohne sicherheitskritischen Kontext;
    Nicht-\ac{DSR}-basierte Entwicklungsansätze & 
    Fokussierung auf die Forschungsfragen und relevante technologische, methodische und regulatorische Aspekte. \\
    \midrule
    Zeitrahmen & 30. Mai 2025 bis 02. November 2025 & Alles davor oder danach & Berücksichtigung aktueller technologischer Entwicklungen (Blockchain, \ac{PQC}, \ac{SSI}) und regulatorischer Anforderungen. \\
    \midrule
    Publikationstypen & 
    Peer-reviewed Journalartikel;
    Konferenzbeiträge anerkannter Fachgesellschaften (z. B. IEEE, ACM, IFIP); Preprints;
    Offizielle Standards und Empfehlungen (z. B. \ac{NIST}, W3C, \ac{BSI});
    Whitepaper etablierter Organisationen;
    Dissertationen und anerkannte Fachbücher & 
    Blogposts, Forenbeiträge, Marketingmaterial;
    Populärwissenschaftliche Artikel ohne wissenschaftliche Fundierung;
    Unveröffentlichte Manuskripte ohne Peer-Review;
    Seminar- und Abschlussarbeiten ohne wissenschaftliche Begutachtung & 
    Sicherstellung wissenschaftlicher Qualität, Nachvollziehbarkeit und Relevanz der Quellen für die Masterarbeit; Da das Forschungsthema aktuell noch sehr neu ist und die einschlägige Fachliteratur teilweise noch nicht den Peer-Review-Prozess durchlaufen hat, werden auch Preprints in die Analyse einbezogen. Preprints ermöglichen eine zeitnahe Verfügbarkeit aktueller Forschungsergebnisse, was insbesondere bei diesem innovativen von großer Bedeutung ist. \\
    \midrule
    Sprache & Deutsch; Englisch & Andere Sprachen als Deutsch und Englisch & Gewährleistung der Verständlichkeit und Zugänglichkeit für den deutsch- und englischsprachigen Forschungskontext. \\
    \midrule
    Zugänglichkeit & Verfügbare Volltexte & Nicht verfügbare Volltexte & Ermöglichung einer gründlichen Analyse und Bewertung der Inhalte. \\
\end{longtable}

Um die Konsistenz und Vergleichbarkeit der Ergebnisse zu gewährleisten wurde die zweite Iteration mit identischer Suchstrategie durchgeführt.
Dazu zählen alle in \autoref{tab:suchstrategie} dargestellten Schritte, von der Identifikation relevanter Schlüsselkonzepte bis zur Übersetzung der Suchanfrage für die EBSCO Datenbank (\autoref{tab:Abgrenzung zentraler Schlüsselkonzepte}, \autoref{tab:keywords der schlüsselkonzepte}, Listing~A-\ref{lst:boolesche_suchanfrage}).

\paragraph*{Übersetzung der Suchanfrage für EBSCO}

Das Ergebnis der EBSCO Suchanfrage ist in \autoref{fig:EBSCO Ergebnis_11} dargestellt. Insgesamt wurden mit dieser Abfrage 34 Quellen identifiziert.

\begin{figure}[H]
    \centering
    \includegraphics[width=\paperwidth, height=\paperheight, keepaspectratio, angle=90]{EBSCO_11.png}
    \caption{Zweite Iteration - Ergebnis der EBSCO Suchanfrage}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:EBSCO Ergebnis_11}
\end{figure}

\paragraph*{Selektionsprozess}

Im nächsten Schritt folgt der Selektionsprozess der 34 identifizierten Quellen aufbauend auf den zuvor definierten Schlüsselkonzepten und der Suchstrategie. Dabei werden die in \autoref{tab:einausschlusskriterien_iteration2} definierten Ein- und Ausschlusskriterien konsequent angewendet, sodass nur fachlich einschlägige, qualitativ belastbare und für die Forschungsfragen zentrale Studien inkludiert werden.

Zunächst werden alle über die EBSCO-Suchanfrage identifizierten 34 Treffer einer Duplikatsbereinigung unterzogen und anschließend im Rahmen des Titel- und Abstract-Screenings hinsichtlich ihres thematischen Fokus bewertet. Hohe Relevanz erhalten Quellen mit klaren Beiträgen zu \ac{SSI}, \ac{PQC}, \ac{KRITIS} oder dezentralen Identitätsarchitekturen, während Arbeiten zu angrenzenden Technologien wie Blockchain-Sicherheit im \ac{IoT} oder digitaler Forensik als mittel relevant und allgemeine Technologietrends ohne direkten Bezug zum Thema als niedrig relevant eingestuft werden. 

\autoref{fig:PRISMA_Flussdiagramm_Iteration2} fasst diesen Selektionsprozess der zweiten Iteration zusammen. Nach dem Screening aller 34 Publikationen folgte die Eignungsprüfung, in deren Ergebnis zwei Publikationen die Einschlusskriterien erfüllten und 32 ausgeschieden wurden, da sie entweder thematisch irrelevant (n~=~25) oder hinsichtlich ihrer Spezifität unangemessen waren (n~=~7). Insgesamt wurden somit im Rahmen beider iterationen sieben relevante Quellen für die Masterarbeit identifiziert, die zur Synthese des aktuellen Standes der Forschung in Kapitel~\ref{sec:Stand der Forschung und Identifikation der Forschungslücke} eingeflossen sind.

\begin{figure}[H]
    \centering
    \includegraphics[width=\paperwidth-3.75cm]{PRISMA_2020_flow_diagram_updated_SRs_v1_ITERATION_2.png}
    \caption{Zweite Iteration - PRISMA 2020 Flussdiagramm}
    \begin{flushleft}
    \textit{Anmerkung.} In Anlehnung an \textcite[S. 5]{page_PRISMA2020Statementupdatedguidelinereportingsystematicreviews_2021}.
    \end{flushleft}
    \label{fig:PRISMA_Flussdiagramm_Iteration2}
\end{figure}

\pagebreak

\autoref{tab:quellenuebersicht_iteration2} stellt eine Übersicht der Bewertung der 34 identifizierten Quellen dar, welche vollständig in in \autoref{tab:quellenbewertung_iteration2} dokumentiert wurde.

\begin{longtable}{L{1.5cm}L{11cm}L{1cm}}
    \caption{Zweite Iteration - Übersicht der relevanten Quellen}
    \label{tab:quellenuebersicht_iteration2} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endfirsthead
    \multicolumn{3}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{3}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{3}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf \autoref{tab:quellenbewertung_iteration2} und Titel und Abstracts von \textcite{barrett-danes_QuantumComputingCybersecurityrigoroussystematicreviewemergingthreatspostquantumsolutionsresearchdirections_2025,feng_IdentityManagementSystemsComprehensiveReview_2025}.} \\
    \endlastfoot
    1 & Barrett-danes, F., \& Ahmad, F. (2025). Quantum computing and cybersecurity: a rigorous systematic review of emerging threats, post-quantum solutions, and research directions (2019-2024). Discover Applied Sciences, 7(10). \url{https://doi.org/10.1007/s42452-025-07322-5} & Hoch \\
    \midrule
    2 & Feng, Z., Li, Z., Cui, H., \& Whitty, M. T. (2025). Identity management systems: A comprehensive review. Information (Basel), 16(9), 778. \url{https://doi.org/10.3390/info16090778} & Hoch \\
    \midrule
    3--10 & Diverse & Mittel  \\
    \midrule
    11--34 & Diverse & Niedrig \\
\end{longtable}

\pagebreak

\paragraph*{Bewertung der identifizierten Quellen hinsichtlich ihrer Relevanz}

\begin{longtable}{L{0.5cm}L{4cm}L{1.5cm}L{7cm}}
    \caption{Zweite Iteration - Bewertung der identifizierten Quellen hinsichtlich ihrer Relevanz}
    \label{tab:quellenbewertung_iteration2} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} & \textbf{Kommentar} \\
    \midrule
    \endfirsthead
    \multicolumn{4}{l}{\textit{Tabelle \thetable\ (Fortsetzung)}} \\
    \toprule
    \textbf{Nr.} & \textbf{Quelle} & \textbf{Relevanz} & \textbf{Kommentar} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{4}{r}{\textit{Fortsetzung auf nächster Seite}} \\
    \endfoot
    \bottomrule
    \multicolumn{4}{p{\linewidth}}{\textit{Anmerkung.} Basierend auf den Abstracts aller in Spalte zwei unter \enquote{Quelle} aufgeführten Quellenangaben.} \\
    \endlastfoot
1 & Barrett-danes, F., \& Ahmad, F. (2025). Quantum computing and cybersecurity: a rigorous systematic review of emerging threats, post-quantum solutions, and research directions (2019-2024). Discover Applied Sciences, 7(10). \url{https://doi.org/10.1007/s42452-025-07322-5} & Hoch & Setzt sich systematisch, methodisch und interdisziplinär mit den Bedrohungen durch Quantum Computing für klassische Kryptosysteme auseinander und behandelt ausdrücklich Post-Quantum-Kryptografie (PQC), hybride Frameworks (u.a. QKD), Umsetzungsherausforderungen und deren Auswirkungen insbesondere für IoT-Umgebungen. Die Arbeit analysiert sowohl den Forschungsstand als auch Implementierungs- und Migrationspfade anhand konkreter Pilotstudien und adressiert Skalierbarkeit und Wirtschaftlichkeit; Die Nutzung eines PRISMA-basierten Review-Frameworks und der Fokus auf praktische Handlungs- und Politikempfehlungen verleihen der Arbeit hohe wissenschaftliche und praxisnahe Relevanz für die Entwicklung, Migration und Absicherung quantensicherer sowie zukunftsfähiger Systeme mit starker Anschlussfähigkeit an KRITIS, PQC und angrenzende sicherheitskritische Domänen. \\
\midrule
2 & Feng, Z., Li, Z., Cui, H., \& Whitty, M. T. (2025). Identity management systems: A comprehensive review. Information (Basel), 16(9), 778. \url{https://doi.org/10.3390/info16090778} & Hoch & Der Abstract beschreibt eine umfassende und systematische Übersicht zu Blockchain-basierten Identity Management Systems (IDMSs) mit explizitem Fokus auf Self-Sovereign Identity (SSI), dezentralen Identifikatoren (DIDs), Interoperabilität und Sicherheitsanalyse über den gesamten Identitätslebenszyklus hinweg. Die Arbeit adressiert technologische und organisatorische Herausforderungen, wie Revokation, Übertragbarkeit, Interoperabilität und Quantum-Resilienz für nutzerkontrollierte Identitätsmodelle. Die PRISMA-basierte Methodik sowie die sektorübergreifende Taxonomisierung und die systematische Ableitung aktuell ungelöster Probleme und künftiger Forschungsrichtungen machen die Quelle höchst wertvoll im Umfeld SSI, Blockchain, PQC und KRITIS. \\
\midrule
3 & Akkal, M., Cherbal, S., Annane, B., Lakhlef, H., \& Kharoubi, K. (2025). Quantum, post-quantum, and blockchain approaches for securing the internet of medical things: a systematic review. Cluster Computing, 28(10). \url{https://doi.org/10.1007/s10586-025-05481-z} & Mittel & Behandelt systematisch die Bedrohungen durch Quantum Computing für IoMT und fokussiert explizit auf den Schutz medizinischer Infrastrukturen mit quantensicherer Kryptografie und Blockchain-Technologie; analysiert sowohl PQC- als auch Blockchain-basierte Sicherheitslösungen und deren Anwendung in hochsensiblen medizinischen IoT-Umgebungen, adressiert Architektur, Herausforderungen und Lösungsansätze für KRITIS-nahe Gesundheitssektoren; Fokus zu stark auf IoMT ohne SSI- und ohne Implementierungsbezug. \\
\midrule
4 & Elkhodr, M. (2025). An AI-driven framework for integrated security and privacy in Internet of Things using quantum-resistant blockchain. Future Internet, 17(6), 246. \url{https://doi.org/10.3390/fi17060246} & Mittel & Der Abstract beschreibt ein ganzheitliches Framework, das KI-basierte Security-Orchestrierung, Blockchain-gestützte Identitätsverwaltung und quantenresistente Kryptografie explizit miteinander kombiniert und anwendungsnah für IoT-Umgebungen evaluiert. Die Arbeit bietet indirekten Bezug zur technischen Umsetzung (Evaluierung) von PQC, SSI und KRITIS. \\
\midrule
5 & Kameni Tcheumaga, F. N., Umba, K., Velupillaimeikandan, P., Haque, M. M., \& Ahamed, S. I. (2025). Security of quantum federated machine learning with blockchain for electronics health records. Cureus Journal of Computer Science. \url{https://doi.org/10.7759/s44389-025-03870-4} & Mittel & Der Abstract adressiert systematisch die Integration von Quantum Federated Machine Learning (QFML), Blockchain und den Schutz elektronischer Gesundheitsakten, mit explizitem Fokus auf Security und Privacy in hochsensiblen, dezentralen Architekturen. Die Kombination aus quantenresistenter Kryptografie, Blockchain-basierter Validierung, federiertem Lernen und gezieltem Schutz kritischer Infrastrukturen im Gesundheitswesen wird als innovativer Lösungsansatz diskutiert. Die Arbeit analysiert bestehende Lücken hinsichtlich Robustheit und Sicherheit und liefert damit auch Impulse für zukünftige Forschung und Entwicklung in den Feldern PQC, KRITIS und dezentraler Identitätsarchitekturen. \\
\midrule
6 & Kurt, K. K., Timurtaş, M., Pınar, S., Ozaydin, F., \& Türkeli, S. (2025). Smart contracts, blockchain, and health policies: Past, present, and future. Information (Basel), 16(10), 853. \url{https://doi.org/10.3390/info16100853} & Mittel & Der Abstract liefert eine methodisch fundierte, systematische Übersicht zu Blockchain- und Smart-Contract-basierten Lösungen für die Verwaltung von Gesundheitsdaten und das Policy Management im Gesundheitswesen. Explizit adressiert werden Sicherheit, Datenschutz, Interoperabilität und die Rolle von Smart Contracts in der Durchsetzung und Automatisierung digitaler Gesundheitsrichtlinien. Es fehlt ein expliziter Fokus auf Post-Quantum Kryptografie in Verbindung mit Self-Sovereign Identity. \\
\midrule
7 & Qatawneh, M. (2025). A framework for security risk assessment of blockchain-based applications. Indonesian Journal of Electrical Engineering and Computer Science, 39(2), 952. \url{https://doi.org/10.11591/ijeecs.v39.i2.pp952-962} & Mittel & Der Abstract beschreibt ein systematisch entwickeltes und praxisvalidiertes Framework zur Bewertung und Mitigation von Sicherheitsrisiken in Blockchain-Anwendungen über alle Schichten hinweg. Es adressiert explizit kritische Schwachstellen wie Smart-Contract-Exploits, Sybil-Attacken und Private-Key-Compromises, integriert quantitative und qualitative Risikoanalyse und demonstriert konkrete Wirksamkeit in Form signifikanter Risikoreduktionen in einem Ethereum-Case-Study-Szenario. Das resultierende BCRAM bietet einen standardisierten, adaptierbaren Bewertungsansatz, der für sichere Systemarchitektur in KRITIS-, SSI- und PQC-orientierten Blockchain-Projekten nutzbar ist. \\
\midrule
8 & Reddy, N. R., Suryadevara, S., Reddy, K. G. R., Umamaheswari, R., Guttula, R., \& Kotoju, R. (2025). Quantum secured blockchain framework for enhancing post quantum data security. Scientific Reports, 15(1), 31048. \url{https://doi.org/10.1038/s41598-025-16315-8} & Mittel & Der Abstract stellt ein hochinnovatives, ganzheitliches Framework („QuantumShield-BC“) für die Absicherung von Blockchain-Architekturen gegen die Bedrohungen durch Quantencomputer vor: Es integriert explizit Post-Quantum-Kryptografie (z.B. Dilithium, Falcon), Quantum Key Distribution (QKD) und einen Quantum Byzantine Fault Tolerance (Q-BFT) Konsens-Mechanismus auf Basis von Quantum Random Number Generation (QRNG) für Leader- und Validator-Auswahl. Das System zeigt in experimenteller Validierung sehr geringe Latenz, hohe Durchsatzraten und vollständige Immunität gegenüber typischen Quantum-Angriffen (Shor, Grover), einschließlich effektiver Sybil-, Replay- und MITM-Abwehr. Die Architektur demonstriert praktisch den Weg für interoperable, skalierbare, hochsichere Blockchains ohne direkten Bezug zu KRITIS und SSI. \\
\midrule
9 & Tawfik, A. M., Al-Ahwal, A., Eldien, A. S. T., \& Zayed, H. H. (2025). Blockchain-based access control and privacy preservation in healthcare: a comprehensive survey. Cluster Computing, 28(8). \url{https://doi.org/10.1007/s10586-025-05308-x} & Mittel & Die Arbeit bietet eine systematische Übersicht über Blockchain-basierte Ansätze zur Zugriffssteuerung und Wahrung der Privatsphäre im Gesundheitswesen, mit besonderem Fokus auf permissioned und permissionless Frameworks, Smart Contracts, kryptographische Verfahren und Plattformen wie Hyperledger Fabric und Ethereum. Die Analyse der untersuchten Lösungen zeigt, wie feingranularer Zugriff, automatisierte Autorisierung und revisionssichere Auditierung durch Blockchain und Privacy-Preserving-Techniken realisiert werden können. Es fehlen Bezüge zu SSI. \\
\midrule
10 & Aboshosha, B. W., Zayed, M. M., Khalifa, H. S., \& Ramadan, R. A. (2025). Enhancing Internet of Things security in healthcare using a blockchain-driven lightweight hashing system. Beni-Suef University Journal of Basic and Applied Sciences, 14(1). \url{https://doi.org/10.1186/s43088-025-00644-8} & Niedrig & Fokussiert auf die Verbesserung der Datensicherheit und Integrität in IoT-basierten Gesundheitsanwendungen durch Blockchain und leichtgewichtige Hashverfahren; adressiert wesentliche Aspekte dezentraler Datenverwaltungsmodelle und kryptographischer Effizienz für ressourcenbeschränkte Geräte; PQC und SSI werden jedoch nicht behandelt, und der methodische Fokus liegt auf klassischen leichten Hashfunktionen statt quantensicheren Primitive. \\
\midrule
11 & Addula, S. R. (2025). Mobile banking adoption: A multi-factorial study on social influence, compatibility, digital self-efficacy, and perceived cost among generation Z consumers in the United States. Journal of Theoretical and Applied Electronic Commerce Research, 20(3), 192. \url{https://doi.org/10.3390/jtaer20030192} & Niedrig & Konzentriert sich auf nutzerpsychologische und soziotechnische Faktoren zur Akzeptanz mobiler Banking-Anwendungen im FinTech-Bereich mit Fokus auf Konsumentenverhalten; weder Kryptografie, Blockchain, noch dezentrale Identitätssysteme werden thematisiert; auch fehlen technische oder sicherheitsbezogene Bezüge zu PQC, SSI oder KRITIS. \\
\midrule
12 & Al Jasem, M. S., De Clark, T., \& Shrestha, A. K. (2025). Toward decentralized intelligence: A systematic literature review of blockchain-enabled AI systems. Information (Basel), 16(9), 765. \url{https://doi.org/10.3390/info16090765} & Niedrig & Die Arbeit liefert einen systematischen Überblick zu Blockchain-gestützten dezentralen KI-Systemen und adressiert damit relevante Aspekte dezentraler Architekturen, insbesondere durch die Untersuchung von Governance, Integrität, skalierbaren Konsensmechanismen sowie Sicherheits- und Datenschutzherausforderungen. Explizite technologische Bezüge zu Self-Sovereign Identity, Post-Quantum-Kryptografie oder kritischen Infrastrukturen fehlen allerdings; stattdessen steht das Zusammenwirken von Blockchain, Smart Contracts und KI im Fokus. \\
\midrule
13 & Almazroi, A. A., Alqarni, M. A., Al-Shareeda, M. A., Alkinani, M. H., Almazroey, A. A., \& Gaber, T. (2025). A bilinear pairing-based anonymous authentication scheme for 5G-assisted vehicular fog computing. Arabian Journal for Science and Engineering, 50(15), 11757–11778. \url{https://doi.org/10.1007/s13369-024-09617-y} & Niedrig & Die Arbeit konzentriert sich auf Authentifizierungsverfahren für 5G-unterstützte Vehicular Fog Computing-Systeme unter Nutzung bilinearer Parings; Im Mittelpunkt stehen effiziente Überprüfungsmechanismen, die Anonymität, Authentizität und praktische Sicherheit im Fahrzeugnetz verbessern; dezentrale Identitätsarchitekturen, Blockchain, PQC oder spezielle Konzepte für Self-Sovereign Identity oder den Schutz kritischer Infrastrukturen werden jedoch nicht behandelt. \\
\midrule
14 & Alsadie, D. (2025). Cybersecurity and artificial intelligence in unmanned aerial vehicles: Emerging challenges and advanced countermeasures. IET Information Security, 2025(1). \url{https://doi.org/10.1049/ise2/2046868} & Niedrig & Stellt eine fundierte und breit gefächerte Übersicht zu aktuellen Bedrohungen und fortschrittlichen Gegenmaßnahmen im Bereich AI-gestützter UAV-Systeme bereit und greift dabei gleich mehrere Schlüsseltechnologien des Bewertungsschemas direkt und explizit auf: Es werden konkrete Methoden der Post-Quantum-Kryptografie (PQC) und blockchain-basierte Sicherheitsmechanismen analysiert und hinsichtlich ihrer Wirksamkeit im Kontext hochsensibler und autonom agierender Drohnennetze bewertet; Kein Bezug zu KRITIS \& SSI; \\
\midrule
15 & Cavus, M., Ayan, H., Bell, M., \& Dissanayake, D. (2025). Advances in energy storage, AI optimisation, and cybersecurity for electric vehicle grid integration. Energies, 18(17), 4599. \url{https://doi.org/10.3390/en18174599} & Niedrig & Die Arbeit liefert einen integrativen und interdisziplinären Überblick zu drei zentralen Säulen hochrelevanter Technologien: Fortschritte bei sicheren dezentralen Energiespeichern, KI-basierter Optimierung für Echtzeit-Energiemanagement und den Einsatz post-quantum Kryptografie und Blockchain-Systemen für die Absicherung von Vehicle-to-Grid (V2G)-Transaktionen im Smart-Grid-Kontext. Konkret fehlt es an praktischen Beiträgen bzgl. Implementierung von SSI, PQC und Blockchain für KRITIS. \\
\midrule
16 & A, C., \& Basarkod, P. I. (2024). A survey on blockchain security for electronic health record. Multimedia Tools and Applications. \url{https://doi.org/10.1007/s11042-024-19883-5} & Niedrig & Der Abstract beschreibt eine systematische Übersicht zu Blockchain-basierten Sicherheitsansätzen für elektronische Gesundheitsakten (EHR), mit Schwerpunkt auf Datenschutz, Zugriffskontrolle, Integritätsprüfung und Effizienz, insbesondere auch mit Blick auf Deep-Learning-Ansätze im Healthcare-Umfeld. Zwar adressiert die Arbeit relevante Aspekte dezentraler vernetzter Datenstrukturen und gibt einen fundierten Vergleich vorhandener Blockchain-basierten Technologien für EHR-Sicherheit, doch stehen SSI, PQC oder KRITIS-spezifische Anwendungen nicht explizit im Fokus. \\
\midrule
17 & Guayasamín, A., Fuertes, W., Carrera, N., Tello-Oquendo, L., \& Suango, V. (2025). Blockchain-empowered e-ticket distribution system for secure and efficient transactions, validation, and audits. Annals of Telecommunications - Annales Des Télécommunications. \url{https://doi.org/10.1007/s12243-025-01125-w} & Niedrig & Der Abstract beschreibt ein Blockchain-basiertes System zur sicheren und effizienten Verwaltung, Prüfung und Auditierung von E-Tickets im Unterhaltungsbereich, insbesondere für Raffles und Veranstaltungsmanagement. Der Fokus liegt auf betrugsresistenter Ticketvergabe, Transparenz, digitaler Signierung, Hash-Funktionen und Proof-of-Work-basiertem Mining, was für die Optimierung von Fälschungsschutz und Automatisierung in öffentlichen Systemen relevant ist. Es besteht kein direkter fachlicher Bezug zu den Kerndomänen PQC, SSI, oder KRITIS. \\
\midrule
18 & He, X., Xu, G., Han, X., Wang, Q., Zhao, L., Shen, C., … Feng, D. (2025). Artificial intelligence security and privacy: a survey. Science China Information Sciences, 68(8). \url{https://doi.org/10.1007/s11432-025-4388-5} & Niedrig & Der Abstract bietet eine breite, systematische Übersicht über Sicherheits- und Datenschutzherausforderungen von KI-Systemen, inklusive Bedrohungen für Datenintegrität, Trainings-, Inferenzphasen und verteilter Umgebungen. Zentral sind klassische KI-Attacken wie Datenvergiftung, Backdoor und Adversarial Attacks, jedoch fehlt ein expliziter Bezug zu Self-Sovereign Identity, Blockchain-Integration, Post-Quantum Kryptografie oder KRITIS-spezifischen Sicherheitsarchitekturen. \\
\midrule
19 & Khagga, V., Priya., S., \& Prasad, A. M. (2025). Enhanced QoS-aware secure routing protocol for WAHNs using advanced fast double decker new binary archimedes kepler pure convolutional transformer network and cryptographic techniques. Peer-to-Peer Networking and Applications, 18(4). \url{https://doi.org/10.1007/s12083-025-02035-3} & Niedrig & Der Abstract stellt eine klassische netzwerktechnische Studie dar, die ein innovatives Routing-Protokoll für Wireless Ad-Hoc Networks (WAHNs) vorstellt und dabei verschiedene Optimierungsverfahren, Deep-Learning-Modelle und komplexe kryptografische Techniken integriert. Die Arbeit adressiert primär technische Fragen der Effizienzsteigerung, Clusterbildung, Verzögerungsminimierung und Zugangssicherheit auf Netzwerkebene, beschränkt sich aber methodisch auf klassische und KI-gestützte kryptografische Mechanismen ohne expliziten Bezug zu PQC, Blockchain, SSI oder KRITIS-spezifischen Identitäts- beziehungsweise Infrastrukturarchitekturen. \\
\midrule
20 & Kumar, M., Kaur, G., \& Rana, P. S. (2025). Performance, portability, productivity, and security in HPC cloud: a systematic literature review. The Journal of Supercomputing, 81(11). \url{https://doi.org/10.1007/s11227-025-07685-x} & Niedrig & Der Abstract liefert eine umfassende systematische Übersicht aktueller Entwicklungstrends und Herausforderungen im Bereich Cloud-basierter Hochleistungsrechner (HPC), strukturiert entlang der Aspekte Performance, Portabilität, Produktivität und Security. Während das Security-Kapitel innovative Technologien wie Trusted Execution Environments, Verschlüsselung und feingranulare Zugangskontrolle behandelt und so auf zentrale Anforderungen in sensiblen Anwendungsdomänen eingeht, fehlt ein expliziter Fokus auf Post-Quantum Kryptografie, Self-Sovereign Identity oder dezentrale Identitätsarchitekturen im KRITIS-Kontext. \\
\midrule
21 & Lubis, M., Safitra, M. F., Fakhrurroja, H., \& Muttaqin, A. N. (2025). Guarding our vital systems: A metric for critical infrastructure cyber resilience. Sensors (Basel, Switzerland), 25(15), 4545. \url{https://doi.org/10.3390/s25154545} & Niedrig & Dieses Paper adressiert direkt die Entwicklung, Messung und Erhöhung der Cyber-Resilienz für kritische Infrastrukturen anhand des InfraGuard Cybersecurity Frameworks, welches etablierte Reifegradmodelle (ISO/IEC 15504, NIST CSF, COBIT) einbindet und die gesamte Breite an Abwehr-, Verteidigungs- und Wiederherstellungsmaßnahmen strukturiert abbildet. Im Mittelpunkt stehen situative Awareness, aktive Verteidigung, Risikomanagement und Incident Recovery. Es fehlt ein expliziter Fokus auf Post-Quantum Kryptografie in Verbindung mit Self-Sovereign Identity. \\
\midrule
22 & Marengo, A., \& Santamato, V. (2025). Quantum algorithms and complexity in healthcare applications: a systematic review with machine learning-optimized analysis. Frontiers in Computer Science, 7(1584114). \url{https://doi.org/10.3389/fcomp.2025.1584114} & Niedrig & Die Studie bietet eine systematische Übersicht zu quantencomputergestützten Algorithmen, deren algorithmischer Komplexität und Anwendungsbereichen im Gesundheitswesen, insbesondere für KI-gestützte Diagnostik und die Sicherheit medizinischer Daten. Sie kombiniert fortgeschrittene Literaturanalyse (PSO, LDA, LIME) mit einer empirisch validierten Kategorisierung der Forschung in „Quantum für KI in Healthcare“ und „Quantum für Datensicherheit im Gesundheitswesen“. Besonderes Gewicht liegt auf quantenkryptografischen Protokollen, Blockchain-basierten Sicherheitsarchitekturen und hybriden Quantum-KI-Ansätzen, die innovative Lösungsansätze für den Schutz sensibler Gesundheitsdaten, die Beschleunigung diagnostischer Prozesse und die Entwicklung resilienter Infrastrukturmodelle liefern. Es fehlt ein konkreter Bezug zu SSI und KRITIS. \\
\midrule
23 & Meka, C., Palakollu, K. R., Azees, M., Rajasekaran, A. S., Das, A. K., \& Hölbl, M. (2025). A comprehensive survey on integration of machine learning with secure blockchain-based applications. Cluster Computing, 28(10). \url{https://doi.org/10.1007/s10586-025-05330-z} & Niedrig & Der Abstract beschreibt eine systematische Übersicht zu Anwendungen, Herausforderungen und offenen Problemen an der Schnittstelle von Machine Learning und Blockchain, insbesondere hinsichtlich Sicherheit, Automatisierung, Auditing und Skalierung in IoT- und Industrieszenarien. Der Beitrag legt den Fokus auf die beidseitigen Synergien beider Technologien—z.B. Fraud Detection, Schutz der Modellintegrität, sichere Prozessautomatisierung per Smart Contracts und Anomalieerkennung—und skizziert aktuelle Forschung, technische Integrationsaspekte und sektorübergreifende Use Cases. Explizite Bezüge zu PQC, Self-Sovereign Identity oder KRITIS-spezifischen Identitäts- beziehungsweise Infrastrukturarchitekturen werden jedoch nicht vertieft adressiert. \\
\midrule
24 & Mustafa, R., Sarkar, N. I., Mohaghegh, M., Pervez, S., \& Vohra, O. (2025). Cross-layer analysis of machine learning models for secure and energy-efficient IoT networks. Sensors (Basel, Switzerland), 25(12), 3720. \url{https://doi.org/10.3390/s25123720} & Niedrig & Die Arbeit verfolgt einen innovativen Ansatz zur Harmonisierung von IoT-Sicherheit und Energieeffizienz durch eine umfassende, schichtübergreifende Architektur, die spezialisierte Machine-Learning-Modelle (z. B. LSTM-Netzwerke zur Anomalieerkennung, Entscheidungsbäume zur Validierung) mit leichtgewichtiger, adaptiver Kryptografie (Speck) koppelt. Durch rollenspezifische Zugriffskontrolle (RBAC) und energieorientierte Sicherheitspolitiken adressiert die Studie zentrale Herausforderungen für ressourcenbeschränkte IoT-Geräte und demonstriert signifikante Verbesserungen bei Fehlalarmraten, Zugangssicherheit und Energieverbrauch. Explizite Bezüge zu PQC, Self-Sovereign Identity oder KRITIS-spezifischen Identitäts- beziehungsweise Infrastrukturarchitekturen werden jedoch nicht vertieft adressiert. \\
\midrule
25 & Mutahhar, A., Khanzada, T. J. S., \& Shahid, M. F. (2025). Enhanced scalability and security in blockchain-based transportation systems for mass gatherings. Information (Basel), 16(8), 641. \url{https://doi.org/10.3390/info16080641} & Niedrig & Der Abstract beschreibt ein Blockchain-basiertes System zur Erhöhung von Effizienz und Sicherheit in intelligenten Transportsystemen, insbesondere für Massengroßveranstaltungen und städtische Mobilitätsnetzwerke. Die Lösung integriert State Channels und Rollups zur Skalierungsoptimierung, erreicht hohe Transaktionsgeschwindigkeiten und adressiert die Herausforderungen der Datenmanipulation, Integrität und Verschlüsselung in urbanen Verkehrsnetzen. Die Konzeption ist praxisnah für dynamische, datenschutzsensitive Transportinfrastrukturen, adressiert aber weder explizit Self-Sovereign Identity, Post-Quantum Kryptografie noch Anwendungsfälle für KRITIS-nahe Domänen. \\
\midrule
26 & Pillai, S. E. V. S., Nadella, G. S., Meduri, K., Priyadharsini, N. A., Bhuvanesh, A., \& Kumar, D. (2025). A walrus optimization-enhanced long short-term memory model for credit fraud detection in banking. International Journal of Information Technology. \url{https://doi.org/10.1007/s41870-025-02574-1} & Niedrig & Der Abstract beschreibt ein fortgeschrittenes Framework zur Nutzung von Autoencoder, LSTM-Netzwerken und Walrus Optimization Algorithm (WOA) für die Optimierung von Kreditkartenbetrugserkennung in Bankdaten. Die Kombination von modernen Machine-Learning-Techniken und bio-inspirierten Metaheuristiken erhöht die Performance und Skalierbarkeit im Datenmanagement klassischer Finanzsysteme, bleibt aber ohne expliziten Bezug zu Blockchain, Self-Sovereign Identity, Post-Quantum Kryptografie, kritischer Infrastruktur oder dezentralen Identitätsarchitekturen. \\
\midrule
27 & Rao, C. K., Sahoo, S. K., \& Yanine, F. F. (2025). A review of IoT-based smart energy solutions for photovoltaic systems. Electrical Engineering (Berlin. Print), 107(12), 15049–15068. \url{https://doi.org/10.1007/s00202-025-03312-3} & Niedrig & Der Abstract gibt einen breiten Überblick über den Stand und die Rolle von IoT-basierten Monitoring- und Managementsystemen für Photovoltaik-Anlagen, insbesondere zur Optimierung von Energieeffizienz, Datenanalyse, Cloud-Integration und Betriebsplanung. Der Schwerpunkt liegt auf technologischen Innovationen rund um IoT, Smart Grids, Energieverwaltung und Echtzeitdatenerfassung für industrielle und wissenschaftliche Anwendungen. Methodisch und thematisch fehlt jedoch ein Bezug zu Self-Sovereign Identity, Blockchain, Post-Quantum Kryptografie oder zur Absicherung kritischer digitaler Versorgungsinfrastrukturen. \\
\midrule
28 & Sefati, S. S., Arasteh, B., Halunga, S., \& Fratu, O. (2025). A comprehensive survey of cybersecurity techniques based on quality of service (QoS) on the Internet of Things (IoT). Cluster Computing, 28(12). \url{https://doi.org/10.1007/s10586-025-05449-z} & Niedrig & Der Abstract liefert einen systematischen Überblick zu aktuellen Cybersecurity-Techniken zur QoS-bewussten Absicherung von IoT-Systemen, mit besonderem Fokus auf die Trade-offs zwischen Sicherheitsmechanismen (etwa Anomalieerkennung, Hybrid- und KI-basierte Methoden, Blockchain, Federated Learning) und kritischen Leistungsparametern (Latenz, Durchsatz, Energieverbrauch) in ressourcenbeschränkten Umgebungen. Der Beitrag beleuchtet vielseitige Angriffsszenarien (DDoS, MITM, Datenextraktion), validiert aktuelle Detektionsparadigmen und hebt offene Forschungsfelder hervor, etwa adaptive und echtzeitfähige Security-Modelle sowie Recovery-Maßnahmen nach Angriffen. Explizite Bezüge zu PQC, SSI oder KRITIS-spezifischen Architekturparadigmen werden nicht gesetzt. \\
\midrule
29 & Shahzad, M., Rizvi, S., Khan, T. A., Ahmad, S., \& Ateya, A. A. (2025). An exhaustive parametric analysis for securing SDN through traditional, AI/ML, and blockchain approaches: A systematic review. International Journal of Networked and Distributed Computing, 13(1). \url{https://doi.org/10.1007/s44227-024-00055-8} & Niedrig & Das Paper bietet eine umfassende, systematische Analyse und Vergleich der wichtigsten Sicherheitsansätze für Software-defined Networking (SDN), inklusive klassischer, Machine-Learning- und Blockchain-basierter Methoden. Die Literaturauswertung demonstriert, dass Blockchain-basierte Mechanismen für Flussregelung, Datenvalidierung, Parser und Controller-Authentifizierung die Robustheit und Angriffsresistenz von SDN signifikant steigern und Aspekte wie Integrität, Trust und Dezentralität adressieren. Machine-Learning-Technologien (CNN, SVM, KNN) liefern praxisnahe Mehrwerte bei der Angriffserkennung. Jedoch bleibt ein expliziter Bezug zu Post-Quantum Kryptografie, Self-Sovereign Identity oder KRITIS-spezifischen Identitätsarchitekturen aus. \\
\midrule
30 & Singh, B., Indu, S., \& Majumdar, S. (2025). Comparative analysis of intrusion detection models using quantum machine learning techniques. Circuits, Systems, and Signal Processing. \url{https://doi.org/10.1007/s00034-025-03256-w} & Niedrig & Der Abstract und die zitierten Studien zeigen, dass Quantum Machine Learning-Techniken bei der Intrusion Detection gegenüber klassischen Machine Learning-Ansätzen, insbesondere in großen Netzwerken und komplexen Datenlagen, signifikante Vorteile in Genauigkeit und Performanz bieten können. Die experimentellen Vergleiche (z.B. QSVM, QCNN, VQC) über mehrere bekannte Datensätze belegen, dass quantenunterstützte Modelle wie das QML-IDS eine robustere Erkennung von Angriffsmustern (z.B. DDoS, BruteForce, Reconnaissance) und eine bessere Generalisierbarkeit im Kontext von Post-Quantum Cryptography und modernen Cyberbedrohungen erreichen. Auch werden praktische Herausforderungen und die Skalierbarkeit hybrider Modelle thematisiert Ein expliziter Bezug zu Post-Quantum Kryptografie, Self-Sovereign Identity oder KRITIS-spezifischen Identitätsarchitekturen bleibt aus.\\
\midrule
31 & Tom, A. K., Khraisat, A., Jan, T., Whaiduzzaman, M., Nguyen, T. D., \& Alazab, A. (2025). Survey of federated learning for cyber threat intelligence in industrial IoT: Techniques, applications and deployment models. Future Internet, 17(9), 409. \url{https://doi.org/10.3390/fi17090409} & Niedrig & Die Arbeit liefert einen detaillierten und breit gefächerten Überblick zu aktuellen Methoden für cyber threat intelligence (CTI) im Kontext industrieller IoT-Umgebungen (IIoT) unter expliziter Berücksichtigung von föderiertem Lernen (FL) als Schlüsseltechnologie für datenschutzwahrende, skalierbare und dezentrale Bedrohungsanalyse. Die systematische Betrachtung von FL-Architekturen, Aggregationsstrategien (z.B. FedAvg, FedProx, Krum) und deren Anwendungen auf Intrusion Detection, Malware-Analyse, Botnet-Mitigation und Anomalieerkennung demonstriert praxisrelevante Fortschritte für KRITIS-nahe und industriell ausgerichtete Sicherheitsarchitekturen. Es fehlen Bezüge zu SSI, Blockchain und PQC. \\
\midrule
32 & Zhang, Y., Zhao, K., Yang, Y., \& Zhou, Z. (2025). Real-time service migration in edge networks: A survey. Journal of Sensor and Actuator Networks, 14(4), 79. \url{https://doi.org/10.3390/jsan14040079} & Niedrig & Der Abstract stellt eine umfassende systematische Übersicht zu Echtzeit-Service-Migration in Edge-Netzwerken bereit, einschließlich Architekturen, Modellen, Motivationen, Techniken und Anwendungsszenarien (z.B. Smart Cities, Smart Homes, Smart Manufacturing). Der Schwerpunkt liegt auf Algorithmen, Modellen und Methoden zur Reduktion von Latenz, Lastverteilung und dynamischer Ressourcenzuteilung für zeitkritische, verteilte Dienste. Sicherheit, Privacy, PQC, Blockchain oder dezentrale Identitätsarchitekturen werden nicht explizit adressiert. Der Beitrag ist für Netzwerkarchitektur und Echtzeitfähigkeit relevant, liefert aber keinen direkten methodischen oder konzeptionellen Beitrag zu SSI/PQC/KRITIS. \\
\midrule
33 & Zinabu, N. G., Marye, Y. W., Tune, K. K., \& Demilew, S. A. (2025). Comprehensive analysis of lightweight cryptographic algorithms for battery‐limited Internet of Things devices. International Journal of Distributed Sensor Networks, 2025(1). \url{https://doi.org/10.1155/dsn/9639728} & Niedrig & Die Arbeit liefert eine systematische und breit angelegte Analyse zu aktuellen Lightweight-Kryptografie-Algorithmen speziell für energie- und ressourcenbeschränkte IoT-Geräte. Verglichen werden dabei unter anderem Algorithmen wie ASCON, SPECK, PRINCE, TWINE und modifizierte Varianten von AES-128 hinsichtlich Effizienz, Sicherheit, Energieverbrauch und Implementierungskomplexität in konkreten Hardware- und Softwareszenarien. Besonders hervorgehoben werden die trade-offs zwischen Durchsatz, Speicher- und Energiebedarf sowie Angriffstoleranz. Der systematische Review zeigt, dass etwa ASCON als sichere, performante Allround-Lösung für Authenticated Encryption anerkannt ist, während SPECK in Szenarien mit höchsten Durchsatzanforderungen durch Einfachheit punktet, jedoch unter größerer kritischer Scrutiny in puncto langfristiger Sicherheit steht. PQC- oder SSI-Verfahren werden in diesem Kontext nicht umfassend adressiert. \\
\midrule
34 & Zreikat, A. I., AlArnaout, Z., Abadleh, A., Elbasi, E., \& Mostafa, N. (2025). The integration of the Internet of Things (IoT) applications into 5G networks: A review and analysis. Computers, 14(7), 250. \url{https://doi.org/10.3390/computers14070250} & Niedrig & Die Arbeit bietet einen fundierten Review und eine Analyse zur Integration von IoT-Anwendungen in 5G-Netzwerke mit Schwerpunkt auf Konnektivität, Datenrate, Latenz, Interoperabilität und Anwendungsvielfalt (Smart Cities, Industrie 4.0, Healthcare etc.). Methodisch werden technische Potenziale (wie Network Slicing, Edge Computing, massive Machine-Type-Kommunikation) und Herausforderungen (Sicherheit, Energie, Netzmanagement) systematisch aufgearbeitet. Zwar werden Security und Privacy angesprochen, jedoch gibt es keinerlei dezidierte Analyse zu PQC, Self-Sovereign Identity, Blockchain, dezentrale Identitätsarchitekturen oder KRITIS-spezifische Schutzmaßnahmen. \\
\end{longtable}


\newpage
\section{Erste Iteration der Artefaktentwicklung} \label{sec:Anhang_Artefaktentwicklung Iteration 1}

\subsection{Framework- und Technologie-Auswahl}
\label{sec:Anhang_Framework- und Technologie-Auswahl}

\subsubsection{DLT-Plattform} \label{sec:Anhang_DLT-Plattform}

Die Auswahl der DLT-Plattform für die Implementierung des PQC-SSI-Prototypen erfolgte unter Berücksichtigung der in Kapitel~\ref{sec:Anforderungsanalyse} definierten funktionalen Anforderungen sowie der KRITIS-Compliance-Anforderungen an SSI-Systeme. Wie von \textcite[S. 3]{ghosh_DecentralizedCrossNetworkIdentityManagementBlockchainInteroperation_2021} demonstriert, setzen dezentrale Identitätsverwaltungssysteme auf existierende Frameworks als technologische Bausteine. 

Für die vorliegende Arbeit wurde Hyperledger Indy als DLT-Plattform gewählt, da es als einzige Enterprise-Blockchain-Lösung explizit für Self-Sovereign Identity konzipiert wurde \parencite[S. 547--548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}.

Hyperledger Indy bietet eine permissioned Ledger-Architektur \parencite[Tabelle~1]{dixit_DecentralizedIIoTIdentityFrameworkbasedSelfSovereignIdentityusingBlockchain_2022}, die offene Leseoperationen ermöglicht, während Schreibrechte auf designierte Trust Anchors \parencite[S. 4]{ghosh_DecentralizedCrossNetworkIdentityManagementBlockchainInteroperation_2021} beschränkt bleiben, die als autorisierte Akteure DID-Registrierungen und Credential-Schemata publizieren können.

Die Plattform implementiert nativ das \ac{VCM} mit den vier zentralen Attributen Issuer-Identität, Holder-Kontrolle, manipulationssichere Claims und Revocation-Status \parencite[S. 548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}. Hyperledger Indy unterstützt kryptographische Akkumulatoren als Revocation Registries, die Privacy-Preserving Non-Revocation Proofs via Zero-Knowledge-Verfahren ermöglichen \parencite[S. 548]{su_HyperledgerIndybasedRoamingIdentityManagementSystem_2023}.

Für die konkrete Implementierung wurde das von-network \parencite{bcgov_GitHubBcgovVonnetworkportabledevelopmentlevelIndyNodenetwork_} als containerisierte Indy-Deployment-Lösung eingesetzt. Das von-network stellt eine vorkonfigurierte Indy-Node-Pool-Infrastruktur bereit, bestehend aus vier Validator-Nodes, einem Genesis-Webserver zur Bereitstellung der Pool-Transaktionsdatei sowie einem Ledger-Browser für Transparenz- und Audit-Zwecke \parencite{bcgov_GitHubBcgovVonnetworkportabledevelopmentlevelIndyNodenetwork_}.

\subsubsection{SSI-Framework}

Für die Implementierung der SSI-Agenten wurde Hyperledger \ac{ACA-Py} \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_} als Framework ausgewählt. \ac{ACA-Py} \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_} ist eine produktionsreife, quelloffene SSI-Agenten-Infrastruktur, die es ermöglicht, Verifiable-Credential-Ökosysteme als Issuer, Holder oder Verifier zu implementieren \parencite[S. 3,7]{ghosh_DecentralizedCrossNetworkIdentityManagementBlockchainInteroperation_2021}. Die Kombination von Hyperledger Aries und Hyperledger Indy hat sich als zentrale Toolchain für die Entwicklung von SSI-Anwendungen etabliert \parencite[S. 5]{ferdous_SSI4WebSelfsovereignIdentitySSIFrameworkWeb_2022}.

Die Architektur von \ac{ACA-Py} \parencite{openwallet-foundation_GitHubOpenwalletfoundationAcapyACAPyfoundationbuildingdecentralizedidentityapplicationsservicesrunningnonmobile_} basiert auf einer Trennung zwischen Agent und Controller. Der Agent verwaltet alle Aries-Kernfunktionalitäten wie die Interaktion mit anderen Agents, sichere Speicherverwaltung und Event-Benachrichtigungen, während der Controller das spezifische Verhalten der Agenten-Instanz definiert \parencite[S. 10]{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}. Der Agent stellt dem Controller eine REST-API für administrative Nachrichten bereit, und der Controller registriert einen Webhook beim Agent, um HTTP-Callbacks für Event-Benachrichtigungen zu empfangen \parencite[S. 10--11]{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}.

Ein weiteres zentrales Feature ist die Unterstützung von \gls{DIDComm}-Messaging-Protokollen, die asynchrone, Ende-zu-Ende-verschlüsselte Kommunikation zwischen Agents ermöglichen, wobei Nachrichten über intermediäre Agent-Konfigurationen geroutet werden \parencite[S. 10]{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}.

Im Vergleich zu alternativen Frameworks wie Aries Go wurde ACA-Py ausgewählt, da Aries Go zum Zeitpunkt der Implementierung noch nicht produktionsbereit ist \parencite[S. 13--14]{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}. Die Entscheidung für ACA-Py ermöglicht zudem die Nutzung des Aries-Askar-Wallets als Speicher-Backend, welches im Vergleich zum älteren Indy-Wallet signifikante Performance-Verbesserungen bietet \parencite[S. 13]{bahce_CaseStudyMobileWalletImplementationSelfSovereignIdentityInfrastructure_2023}.

\subsubsection{Kryptografiebibliothek}

Die Auswahl von liboqs als zentrale Kryptografiebibliothek wird durch das Konzept des \ac{OQS} Projekts motiviert, das \textcite[S. 30--33]{stebila_PostquantumKeyExchangeInternetOpenQuantumSafeProject_2017} als Ansatz zur Prototypisierung quantenresistenter Kryptografie mit dem Ziel des zuverlässigen flächendeckenden Einsatzes entworfen hat. \textcite[S. 30]{stebila_PostquantumKeyExchangeInternetOpenQuantumSafeProject_2017} definiert OQS als open-source Softwareframework bestehend aus liboqs, einer C-Bibliothek mit quantenresistenten Algorithmen, und deren Integration in etablierte Protokolle wie OpenSSL.

Die Bibliothek bietet eine gemeinsame Schnittstelle für Schlüsselaustausch und digitale Signaturschemen \parencite[S. 31]{stebila_PostquantumKeyExchangeInternetOpenQuantumSafeProject_2017} sowie Implementierungen verschiedener Post-Quantum-Algorithmen \parencite[S. 3--4, Tab. 1]{solavagione_TransitionSelfSovereignIdentityPostQuantumCryptography_2025}.

\textcite[S. 5]{sikeridis_PostQuantumAuthenticationTLS13PerformanceStudy_2020} demonstrieren die praktische Integration von liboqs-basierten Algorithmen in das OQS OpenSSL Fork für TLS 1.3. Ihre Ergebnisse zeigen, dass die liboqs-Abstraktionsschicht es ermöglicht verschiedene PQ-Signaturen in X.509-Zertifikaten zu integrieren \parencite[S. 11--14]{sikeridis_PostQuantumAuthenticationTLS13PerformanceStudy_2020}.

\subsubsection{Revocation-Infrastruktur}

Für die Revocation-Anforderung wird das Revocation-Schema von Hyperledger Indy (\ref{sec:Anhang_DLT-Plattform}) gewählt. Indy implementiert Privacy-Preserving-Revocation \parencite[S. 1]{fraser_FormalSecurityAnalysisHyperledgerAnonCreds_2025} durch kryptografische Akkumulatoren, wodurch Holder Nicht-Widerrufung via Zero-Knowledge-Proofs nachweisen können, ohne ihre Credential-IDs oder Indizes preiszugeben \parencite{hardman_0011CredentialRevocationHyperledgerIndyHIPEdocumentation_2018}.

Der Indy Tails Server \parencite{bcgov_GitHubBcgovIndytailsserverThissoftwarestoresmakesavailabletailsfilesuseHyperledger_} stellt einen dedizierten File-Server für die Speicherung und Distribution von Revocation-Registry-Tails-Dateien bereit \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}. Er fungiert als zentraler Storage-Service, auf den sowohl Credential-Issuer (zum Upload der Tails-Dateien) als auch Verifier (zum Download für Proof-Verifikation) zugreifen \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}. Während die speicherintensiven Tails-Dateien physisch auf dem Tails-Server liegen, wird lediglich ihr kryptografischer Hashwert fileHash in der Revocation Registry Definition auf dem unveränderlichen Ledger persistiert \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}.

Die BC-Government-Referenzimplementierung bietet native Integration mit \ac{ACA-Py} \parencite{_ACAPyDocs_} und stellt sicher, dass Holder und Verifier die notwendigen kryptografischen Daten abrufen können.

\subsubsection{Sidecar-Proxy}
\label{sec:Anhang_Sidecar-Proxy}

Das \gls{Sidecar-Proxy}-Pattern wird zur transparenten Integration der Post-Quantum-Kryptografie auf Transportebene gewählt, ohne die zugrundeliegende ACA-Py-Anwendungslogik zu modifizieren. \textcite[S.~2, 5]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunicationsCloudNativeApplications_2024} identifizieren Transparenz, Automatisierung und Modularität als zentrale Eigenschaften dieses Architekturmusters. Die vorliegende Arbeit adaptiert dieses Pattern von Cryptographic Access Control auf die PQC-Integration für SSI-Systeme.

Durch die Deployment-seitige Ergänzung von Sidecar-Containern können unterstützende Funktionalitäten für alle Services zentral implementiert werden, ohne die Komplexität einzelner Microservices zu erhöhen \parencite[S. 158]{meadows_SidecarbasedPathawareSecurityMicroservices_2023}.

\autoref{fig:sidecar_proxy_pod} visualisiert die grundlegende Funktionsweise eines \gls{Sidecar-Proxy} innerhalb eines Pods.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Sidecar-Proxy_POD.png}
    \caption{Pod mit Sidecar-Proxy}
    \begin{flushleft}
    \textit{Anmerkung.} Aus \textcite[S.~3]{berlato_WorkinProgressSidecarProxyUsablePerformanceAdaptableEndtoEndProtectionCommunicationsCloudNativeApplications_2024}.
    \end{flushleft}
    \label{fig:sidecar_proxy_pod}
\end{figure}

nginx \parencite{nginx_GitHubNginxNginxofficialNGINXOpenSourcerepository_} wird als Implementierung gewählt, da es mit OpenSSL~3.5+ kompiliert über den OQS-Provider NIST-standardisierte PQC-Algorithmen unterstützt \parencite{_BuildingNginxSources_}.

\subsection{Implementierung}

\subsubsection{Setup der Entwicklungsumgebung}
\label{sec:Anhang_Setup der Entwicklungsumgebung}

Die Sicherstellung der wissenschaftlichen Validität von Software-Experimenten erfordert eine robuste Konzeption der zugrundeliegenden Entwicklungsumgebung. Gerade im Kontext komplexer Systemarchitekturen stellt die heterogene und sich rasch wandelnde Natur von Computerumgebungen eine signifikante Hürde für die Reproduzierbarkeit und Erweiterbarkeit von Forschungsergebnissen dar \parencite[S.~1]{boettiger_IntroductionDockerReproducibleresearch_2015}. Um diesen Herausforderungen zu begegnen und die Abhängigkeiten des SSI-Prototypen kontrollierbar zu machen, wurde eine hierarchisch isolierte Umgebung entworfen (\autoref{fig:Setup der Entwicklungsumgebung}).

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Entwicklungsumgebung}
    \caption{Setup der Entwicklungsumgebung}
    \begin{flushleft}
    \textit{Anmerkung.} Eigene Darstellung.
    \end{flushleft}
    \label{fig:Setup der Entwicklungsumgebung}
\end{figure}

\pagebreak

\textbf{Host-System und Virtualisierung}

Als physische Basis dient eine Workstation, auf der das Betriebssystem Windows 10 Pro \parencite{microsoft_Windows10HomeundProMicrosoftLifecycle_} (Build 10.0.19045) ausgeführt wird. Um eine hinreichende Isolation zwischen der regulären Arbeitsumgebung zu erreichen, kommt der Typ-1-Hypervisor Hyper-V (Version 9.0) zum Einsatz. Hyper-V wird direkt auf der Computerhardware ausgeführt und liefert robuste Isolation für virtualisierte Workloads \parencite{microsoft_HyperVVirtualisierungWindowsServerundWindows_2025}, indem es jedem virtuellen Gastsystem einen separaten Kernel bereitstellt und damit eine stärkere Isolation erreicht als Systeme, die einen gemeinsamen Kernel nutzen \parencite[S. 3]{moore_GoldilocksIsolationHighPerformanceVMsEdera_2025}.

Innerhalb dieser Virtualisierungsschicht wird Ubuntu 24.04 LTS \parencite{canonicalltd._UbuntuServer2404LTSDocumentation_2024} als Gast-Betriebssystem betrieben. Die Wahl einer Linux-Distribution mit Long Term Support ist dadurch begründet, da diese fünf Jahre lang Standard-Support und Sicherheitsupdates bietet, während Zwischenversionen nur neun Monate lang unterstützt werden \parencite[S. 350]{canonicalltd._UbuntuServer2404LTSDocumentation_2024}.

\textbf{Containerisierung und Orchestrierung}

Zur Lösung des in der Softwaretechnik bekannten Problems der Versionsinkompatibilität, bei dem unterschiedliche Versionen von Softwareverzeichnissen die erfolgreiche Ausführung von Code auf fremden Systemen verhindern \parencite[S. 1--2]{boettiger_IntroductionDockerReproducibleresearch_2015}, wird Docker (Engine Version 28.2.2, API 1.50) eingesetzt. Docker ist eine offene Plattform zur Anwendungsentwicklung und zum Betrieb von Anwendungen in isolierten Umgebungen, die es Entwicklern ermöglicht, Anwendungen unabhängig von der zugrundeliegenden Infrastruktur zu verpacken und auszuführen \parencite{dockerinc._WhatDocker_}. Die Containerisierung adressiert die Herausforderung der Reproduzierbarkeit durch die Kapslung des gesamten Softwarestacks in unveränderlichen Images, wobei eine Dockerfile die notwendigen Abhängigkeiten vom Betriebssystem aufwärts definiert \parencite[S. 3--4]{boettiger_IntroductionDockerReproducibleresearch_2015}. Ein Docker-Image, das von einer Dockerfile erstellt wurde, enthält alle Betriebssystem-Bibliotheken, Konfigurationsdateien und Laufzeitumgebungen, welche die Wiederholung der Forschung auf verschiedenen Maschinen konsistent ermöglichen \parencite[S.~4]{boettiger_IntroductionDockerReproducibleresearch_2015}, da Docker das Packaging und die Ausführung eines Containers so handhabt, dass dieser auf verschiedenen Maschinen identisch funktioniert \parencite{dockerinc._WhatDocker_}.

Die Orchestrierung der verteilten Dienste erfolgt mittels Docker Compose (Version 2.37.1). Docker Compose ist ein Werkzeug zur Definition und zum Ausführen von Multi-Container-Anwendungen, das die Kontrolle über den gesamten Anwendungsstack durch die Verwaltung von Services, Netzwerken und Volumes in einer einzigen YAML-Konfigurationsdatei vereinfacht \parencite{dockerinc._DockerCompose_}, wodurch das komplexe Zusammenspiel aus Ledger-Knoten, Agenten und Revocation-Servern mit einem einzigen Befehl deterministisch gestartet werden kann.

\textbf{Laufzeitumgebung und Versionskontrolle}

Die Anwendungslogik der SSI-Agenten und Evaluationsskripte ist in Python 3.12.3 implementiert. Python 3.12 ist eine stabile Hauptversion der Python-Programmiersprache, die sich auf Benutzerfreundlichkeit konzentriert, da f-Strings viele Einschränkungen entfernt wurden und \enquote{Did-you-mean}-Vorschläge weiterhin verbessert wurden \parencite{turner_WhatsNewPython312_}. 

Die Versionierung des Quellcodes erfolgt über Git (Version 2.43.0), ein verteiltes Versionskontrollsystem, das die Verlaufsverfolgung von Änderungen ermöglicht \parencite{github_Git_}. Git bietet Entwicklern die Möglichkeit, die vollständige chronologische Dokumentation sämtlicher Projektänderungen, Entscheidungsprozesse und Fortschrittsmeilensteine zentral einzusehen. Durch den Zugriff auf die Versionshistorie eines Projekts erhalten Entwickler den erforderlichen Kontext, um das System zu verstehen und sachgerecht zu Projektbeiträgen zu beginnen, wodurch eine kontinuierliche und nachvollziehbare Änderungshistorie gewährleistet wird \parencite{github_Git_}.

Um die Stabilität der Umgebung zu gewährleisten, beruhen die zentralen Repositories auf einem festen Commit-Stand vom 17. September 2025 (\autoref{fig:Setup der Entwicklungsumgebung}).

\subsubsection{Zertifikatserstellungsworkflow}
\label{sec:Anhang_Zertfikatserstellungsworkflow}

Listing~A-\ref{lst:Zertifikatserstellungsworkflow} zeigt den vollständigen Workflow  der Post-Quantum-Zertifikatserstellung für die Sidecar-Proxies mittels OpenSSL~3.5.4 am Beispiel des Issuers.

\refstepcounter{manualListingCounterA}
\label{lst:Zertifikatserstellungsworkflow}
\begin{lstlisting}[language=bash, caption={Zertifikatserstellungsworkflow}, numbers=left, frame=single]
# Openssl 3.5.4 LTS (version mit PQC support)
OpenSSL 3.5.4 30 Sep 2025 (Library: OpenSSL 3.5.4 30 Sep 2025)

# ML-DSA-87 Private Key (höchste Sicherheit für Root)
openssl genpkey -algorithm mldsa87 -out rootCA.key

# Self-signed Root Certificate (10 Jahre)
openssl req -x509 -new -key rootCA.key -out rootCA.crt \
  -days 3650 -subj "/CN=My PQC Root CA/O=MyOrg/C=DE" \
  -addext "basicConstraints=critical,CA:TRUE" \
  -addext "keyUsage=critical,keyCertSign,cRLSign"

# ML-DSA-65 Private Keys für Sidecar-Proxy
openssl genpkey -algorithm mldsa65 -out server.key

# Certificate Signing Requests (CSRs) für Sidecar-Proxy mit SAN
openssl req -new -key server.key -out server.csr \
    -subj "/CN=server proxy/O=FM/C=DE" \
    -addext "subjectAltName=DNS:issuer,DNS:pqc-sidecarproxy-issuer,DNS:host.docker.internal,DNS:localhost,IP:127.0.0.1"

# Signierung mit ML-DSA-65 (Balance zwischen Sicherheit und Performance)
openssl x509 -req -in server.csr \
  -CA rootCA.crt -CAkey rootCA.key -CAcreateserial \
  -out server.crt -days 365 -sha3-256 \
  -copy_extensions copy
\end{lstlisting}

Die Zertifikatsinfrastruktur für die \ac{PQC}-Sidecar-Proxies basiert auf einer selbstsignierten Root Certificate Authority, die mit dem Post-Quantum-Signaturalgorithmus ML-DSA-87 erstellt wurde. Dieses Zertifikat besitzt eine Gültigkeit von zehn Jahren und wird in den System-Trust-Store aller Container importiert, sodass Python-Bibliotheken und Kommandozeilen-Tools die ML-DSA-65-signierten Zertifikate automatisch als vertrauenswürdig erkennen.

Für jeden der fünf Sidecar-Proxies wurde ein dediziertes Server-Zertifikat generiert: issuer.crt, holder.crt, verifier.crt, von-webserver.crt und tails-server.crt. Jedes dieser Zertifikate wird mittels Certificate Signing Request erstellt und von der Root Certificate Authority mit ML-DSA-65 signiert. Die Zertifikate enthalten Subject Alternative Names, die sowohl den DNS-Namen \enquote{localhost} als auch die IP-Adresse \enquote{127.0.0.1} umfassen, um flexible Zugriffsmöglichkeiten während der Entwicklung zu ermöglichen.

Die Integration der Zertifikate erfolgt über Docker-Volume-Mounts. Das Verzeichnis \enquote{hopE/pqc\_sidecarproxy\_nginx/certs/} wird als Read-Only-Volume in jeden nginx-Container unter \enquote{/opt/nginx/certs/} eingebunden. In der nginx-Konfiguration werden die Zertifikate durch die Direktiven \enquote{ssl\_certificate /opt/nginx/certs/<agent>.crt} und \enquote{ssl\_certificate\_key /opt/nginx/certs/<agent>.key} referenziert. Parallel dazu wird das Root-CA-Zertifikat in allen \ac{ACA-Py}-Agent-Containern nach \enquote{/usr/local/share/ca-certificates/pqc-root-ca.crt} kopiert und mittels \enquote{update-ca-certificates} in den System-Trust-Store importiert. Zusätzlich werden die Umgebungsvariablen \enquote{SSL\_CERT\_FILE}, \enquote{REQUESTS\_CA\_BUNDLE} und \enquote{CURL\_CA\_BUNDLE} auf \enquote{/etc/ssl/certs/ca-certificates.crt} gesetzt, um sicherzustellen, dass Python-Anwendungen und HTTP-Clients die importierte Root Certificate Authority verwenden und TLS-Verbindungen zu den \ac{PQC}-Sidecar-Proxies erfolgreich validieren können.

\pagebreak

\subsubsection{Dockerfile: Sidecar-Proxy nginx}
\label{sec:Anhang_Dockerfile Sidecar-Proxy nginx}

Listing~A-\ref{lst:Dockerfile-Sidecar-Proxy-nginx} beinhaltet den Code des Dockerfiles parallel zur visuellen Darstellung in \autoref{fig:Sidecar_Proxy_nginx_Dockerfile}.

In der Build-Phase findet die gesamte Kompilierung und Zusammenführung der für den Betrieb erforderlichen Abhängigkeiten statt. Zunächst werden Versionsangaben für Bibliotheken, Algorithmen, TLS-Gruppen und Installationsverzeichnisse als Build-Parameter definiert. Anschließend werden die projektrelevanten Quellen wie liboqs, oqs-provider, openssl, nginx und curl gezielt und versioniert aus ihren jeweiligen Repositories geladen. Die Kompilierung erfolgt dabei in einer spezifischen Reihenfolge. OpenSSL und liboqs werden mit definierten Einstellungen gebaut, nginx wird unter Einbindung des OQS Providers und einer hybrid gelinkten OpenSSL-Version kompiliert, und curl wird dynamisch mit OpenSSL gebaut, um HTTPS-Unterstützung sicherzustellen. Abschließend werden die generierten Binaries optimiert und nicht benötigte Dateien entfernt, um das finale Image so schlank wie möglich zu gestalten.

Die Runtime-Stage bildet daraufhin das minimalistische Endprodukt, das zum produktiven Einsatz bestimmt ist. Die in der Build-Phase erzeugten Binaries und Konfigurationen werden hier in das neue Runtime-Image übernommen. Die notwendigen Einstellungen für Post-Quantum-TLS-Gruppen und Logging werden über Umgebungsvariablen und angepasste Konfigurationen gesetzt. Das Image läuft nach dem Least-Privilege-Prinzip \parencite[S. 1]{khattar_DockerProEssentialPracticesSecureScalableContainers_2025} mit einem dedizierten Non-Root User (oqs), und der Container-Entrypoint ist klar über den Aufruf von nginx definiert.

Gegenüber dem Original-Dockerfile \parencite{open-quantum-safe_OpenquantumsafeOqsdemosNginxDockerfile_2025} wurden mehrere zielgerichtete Modifikationen (in \autoref{fig:Sidecar_Proxy_nginx_Dockerfile} in rot markiert) vorgenommen. Die OpenSSL-Version wurde von variablen Tags auf die explizite Versionsnummer \enquote{3.5.4} festgelegt. Die für den Key-Encapsulation-Mechanism zentralen TLS-Gruppen wurden auf X25519MLKEM768, mlkem768, x25519 und mlkem1024 reduziert, um gezielt ML-KEM-768 zu forcieren. Im Originalskript werden Zertifikate direkt während des Builds generiert. Die modifizierte Version hingegen verzichtet darauf und sieht vor, dass Zertifikate via Volume von außen eingebracht werden. Die Build-Reihenfolge wurde neu strukturiert. OpenSSL wird zuerst als Shared Library kompiliert, um eine dynamische Verlinkung von curl und nginx zu ermöglichen. Dies umgeht auch Inkompatibilitäten zwischen Alpine Linux und statisch gelinkten OpenSSL-Versionen. Die Optimierung und Entfernung nicht benötigter Artefakte erfolgt granularer, wobei dynamische Binaries und Modul-Dateien wie oqsprovider.so gezielt gestreift werden. Das finale Runtime-Image wurde zusätzlich verschlankt: Es werden ca-certificates als Runtime-Abhängigkeit hinzugefügt, Ports werden explizit nicht mehr im Dockerfile exponiert, dies erfolgt stattdessen via Docker Compose und der ENTRYPOINT ist strikt auf den Sidecar-Proxy-Einsatz ausgerichtet.

\refstepcounter{manualListingCounterA}
\label{lst:Dockerfile-Sidecar-Proxy-nginx}
\begin{lstlisting}[language=bash, caption={Dockerfile - Sidecar-Proxy nginx}, numbers=left, frame=single]
# Nginx with OpenSSL 3.5.4 (LTS) + OQS Provider for Post-Quantum Cryptography
# Based on: https://github.com/open-quantum-safe/oqs-demos/blob/main/nginx/Dockerfile
#
# Customizations:
# - OpenSSL 3.5.4 (LTS with Security Fixes)
# - Uses custom certificates (mounted via volume)
# - ML-KEM-768 Key Exchange enabled

# Define build arguments for version tags, installation paths, and configurations
ARG ALPINE_VERSION=3.21
ARG OPENSSL_TAG=openssl-3.5.4
ARG LIBOQS_TAG=0.13.0
ARG OQSPROVIDER_TAG=0.9.0
ARG NGINX_VERSION=1.28.0
ARG BASEDIR="/opt"
ARG INSTALLDIR=${BASEDIR}/nginx

# Specify supported signature and key encapsulation mechanisms (KEM) algorithms
ARG SIG_ALG="mldsa65"
ARG DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024

# Stage 1: Build - Compile and assemble all necessary components and dependencies
FROM alpine:${ALPINE_VERSION} AS intermediate
ARG OPENSSL_TAG
ARG LIBOQS_TAG
ARG OQSPROVIDER_TAG
ARG NGINX_VERSION
ARG BASEDIR
ARG INSTALLDIR
ARG SIG_ALG
ARG DEFAULT_GROUPS
ARG OSSLDIR=${BASEDIR}/openssl/.openssl

# Install required build tools and system dependencies
RUN apk update && apk --no-cache add \
    build-base linux-headers libtool \
    automake autoconf make cmake ninja \
    openssl openssl-dev git wget pcre-dev

# Download and prepare source files needed for the build process
WORKDIR /opt
RUN git clone --depth 1 --branch ${LIBOQS_TAG} https://github.com/open-quantum-safe/liboqs \
    && git clone --depth 1 --branch ${OQSPROVIDER_TAG} https://github.com/open-quantum-safe/oqs-provider.git \
    && git clone --depth 1 --branch ${OPENSSL_TAG} https://github.com/openssl/openssl.git \
    && wget -q nginx.org/download/nginx-${NGINX_VERSION}.tar.gz \
    && tar -zxf nginx-${NGINX_VERSION}.tar.gz \
    && rm nginx-${NGINX_VERSION}.tar.gz

# Build and install OpenSSL with shared libraries (for curl and nginx)
WORKDIR /opt/openssl
RUN ./Configure --prefix=${OSSLDIR} \
                --openssldir=${OSSLDIR}/ssl \
                shared \
                enable-fips \
    && make -j"$(nproc)" \
    && make install_sw install_ssldirs

# Configure OpenSSL to support the oqs-provider
RUN cp /opt/openssl/apps/openssl.cnf ${OSSLDIR}/ssl/ && \
    sed -i "s/default = default_sect/default = default_sect\noqsprovider = oqsprovider_sect/g" ${OSSLDIR}/ssl/openssl.cnf && \
    sed -i "s/\[default_sect\]/\[default_sect\]\nactivate = 1\n\[oqsprovider_sect\]\nactivate = 1\n/g" ${OSSLDIR}/ssl/openssl.cnf && \
    sed -i "s/providers = provider_sect/providers = provider_sect\nssl_conf = ssl_sect\n\n\[ssl_sect\]\nsystem_default = system_default_sect\n\n\[system_default_sect\]\nGroups = \$ENV\:\:DEFAULT_GROUPS\n/g" ${OSSLDIR}/ssl/openssl.cnf && \
    sed -i "s/HOME\t\t\t= ./HOME\t\t= .\nDEFAULT_GROUPS\t= ${DEFAULT_GROUPS}/g" ${OSSLDIR}/ssl/openssl.cnf

# Build and install liboqs
WORKDIR /opt/liboqs/build
RUN cmake -G"Ninja"  \
    -DOQS_DIST_BUILD=ON  \
    -DBUILD_SHARED_LIBS=OFF  \
    -DCMAKE_INSTALL_PREFIX="${INSTALLDIR}" ..  \
    && ninja -j"$(nproc)" && ninja install

# Build and install Nginx with shared OpenSSL
WORKDIR /opt/nginx-${NGINX_VERSION}
RUN ./configure --prefix=${INSTALLDIR} \
    --with-debug --with-http_ssl_module  \
    --with-cc-opt="-I${OSSLDIR}/include" \
    --with-ld-opt="-L${OSSLDIR}/lib64 -Wl,-rpath,${OSSLDIR}/lib64" \
    --without-http_gzip_module && \
    make -j"$(nproc)" && make install

# Build and install OQS provider
WORKDIR /opt/oqs-provider
RUN ln -s "/opt/nginx/include/oqs" "${OSSLDIR}/include" && \
    rm -rf build && \
    cmake -DCMAKE_BUILD_TYPE=Debug \
          -DOPENSSL_ROOT_DIR="${OSSLDIR}" \
          -DCMAKE_PREFIX_PATH="${INSTALLDIR}" \
          -S . -B build && \
    cmake --build build && \
    MODULESDIR=$(find "${OSSLDIR}" -name ossl-modules -type d | head -1) && \
    export MODULESDIR && \
    cp build/lib/oqsprovider.so "${MODULESDIR}" && \
    rm -rf "${INSTALLDIR:?}/lib64"

# Build curl with shared OpenSSL 3.5.4
ARG CURL_VERSION=8.11.1
WORKDIR /opt
RUN wget -q https://curl.se/download/curl-${CURL_VERSION}.tar.gz && \
    tar -zxf curl-${CURL_VERSION}.tar.gz && \
    rm curl-${CURL_VERSION}.tar.gz

WORKDIR /opt/curl-${CURL_VERSION}
RUN LDFLAGS="-Wl,-rpath,${OSSLDIR}/lib64 -L${OSSLDIR}/lib64" \
    PKG_CONFIG_PATH="${OSSLDIR}/lib64/pkgconfig" \
    ./configure \
    --prefix=${INSTALLDIR} \
    --with-openssl=${OSSLDIR} \
    --with-ca-bundle=/etc/ssl/certs/ca-certificates.crt \
    --disable-manual \
    --disable-ldap \
    --disable-ldaps \
    --without-libpsl \
    --without-zlib \
    --without-brotli \
    --without-zstd && \
    make -j"$(nproc)" && \
    make install

# Minimize image size by stripping binaries
WORKDIR ${INSTALLDIR}
ENV PATH="${INSTALLDIR}/sbin:${INSTALLDIR}/bin:${OSSLDIR}/bin:${PATH}"

RUN set -ex && \
    strip "${OSSLDIR}/lib64/"*.a \
          "${OSSLDIR}/lib64/ossl-modules/oqsprovider.so" \
          "${INSTALLDIR}/sbin/"* \
          "${INSTALLDIR}/bin/curl" \
          "${OSSLDIR}/bin/openssl" && \
    mkdir -p certs

# Stage 2: Runtime - Create a lightweight image with essential binaries and configurations
FROM alpine:${ALPINE_VERSION}
ARG INSTALLDIR
ARG BASEDIR
ARG OSSLDIR=${BASEDIR}/openssl/.openssl

# Install runtime dependencies
RUN apk update && apk --no-cache add pcre-dev ca-certificates

# Copy compiled artifacts and configuration from the intermediate stage
COPY --from=intermediate ${INSTALLDIR} ${INSTALLDIR}
COPY --from=intermediate ${OSSLDIR} ${OSSLDIR}

# Link logs to Docker collector
RUN set -ex && \
    mkdir -p "${INSTALLDIR}/logs" && \
    ln -sf /dev/stdout "${INSTALLDIR}/logs/access.log" && \
    ln -sf /dev/stderr "${INSTALLDIR}/logs/error.log"

# Expose HTTPS port
# EXPOSE 443

# Set OpenSSL configuration environment
# From Nginx 1.25.2: "nginx does not try to load OpenSSL configuration if the
# --with-openssl option was used to build OpenSSL and the OPENSSL_CONF
# environment variable is not set." Hence we must explicitly set OPENSSL_CONF.
ENV PATH="${INSTALLDIR}/sbin:${INSTALLDIR}/bin:${OSSLDIR}/bin:${PATH}" \
    OPENSSL_CONF="${OSSLDIR}/ssl/openssl.cnf" \
    DEFAULT_GROUPS="X25519MLKEM768:mlkem768:x25519:mlkem1024"

# Create non-root user and update permissions
RUN addgroup -g 1000 -S oqs \
 && adduser --uid 1000 -S oqs -G oqs \
 && chown -R oqs:oqs "${INSTALLDIR}"

# Run as non-root user
USER oqs
WORKDIR ${INSTALLDIR}

STOPSIGNAL SIGTERM
CMD ["nginx", "-c", "nginx-conf/nginx.conf", "-g", "daemon off;"]
\end{lstlisting}

\subsubsection{nginx\_holder.conf}
\label{sec:Anhang_nginx_holder.conf}

Strukturell folgen alle nginx-Konfigurationen einem konsistenten Schichtenmodell. Die globale Konfigurationsebene definiert Worker-Prozesse, Logging-Parameter sowie HTTP-Grundeinstellungen. Die mittlere Ebene besteht aus Upstream-Blöcken, die interne Services abstrahieren. Die oberste Ebene enthält Server-Blöcke, die für jeden öffentlich erreichbaren Endpoint eine HTTPS-Schnittstelle exponieren.

Das zentrale Sicherheitsmerkmal aller Konfigurationen ist die Direktive \enquote{ssl\_ecdh\_curve X25519MLKEM768}, die den Hybrid-Key-Exchange zwischen klassischer Elliptischen-Kurven-Kryptografie und dem post-quantensicheren ML-KEM-768-Algorithmus aktiviert. Die konkrete Gruppenauswahl wird extern über die Umgebungsvariable \enquote{DEFAULT\_GROUPS} gesteuert, was eine Trennung von Konfiguration und Deploymentslogik ermöglicht. Komplementär hierzu werden SSL-Zertifikate mit ML-DSA-65-Signaturalgorithmus verwendet, die als Volume-Mounts unter \enquote{/opt/nginx/certs/} eingebunden werden. Dieser Ansatz gewährleistet, dass sowohl der Schlüsselaustausch als auch die Server-Authentifikation post-quantensicher erfolgen, während die Zertifikate unabhängig von der Container-Runtime verwaltet und rotiert werden können.

Die Reverse-Proxy-Funktionalität wird durch \enquote{location /}-Blöcke realisiert, die Anfragen an Upstream-Services weiterleiten. Alle Konfigurationen exponieren einen \enquote{/health}-Endpoint für Orchestrierungs-Health-Checks.

\pagebreak

\refstepcounter{manualListingCounterA}
\label{lst:nginx_holder.conf}
\begin{lstlisting}[language=bash, caption={nginx\_holder.conf}, numbers=left, frame=single]
# OQS Nginx Configuration for VON Network Webserver Reverse Proxy
# Post-Quantum Cryptography enabled with ML-KEM

worker_processes auto;
error_log /opt/nginx/logs/error.log info;
pid /opt/nginx/logs/nginx.pid;

events {
    worker_connections 1024;
}

http {
    include /opt/nginx/conf/mime.types;
    default_type application/octet-stream;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /opt/nginx/logs/access.log main;

    sendfile on;
    keepalive_timeout 65;

    # Upstream: Holder Agent Inbound Transport (Port 8030)
    upstream holder_inbound {
        server holder:8030;
    }

    # Upstream: Holder Agent Admin API (Port 8031)
    upstream holder_admin {
        server holder:8031;
    }

    # HTTPS Server for Holder Inbound Transport (Port 8030)
    server {
        listen 8030 ssl;
        server_name pqc-sidecarproxy-holder;

        # SSL Certificates (custom ML-DSA-65 certificates)
        ssl_certificate /opt/nginx/certs/holder.crt;
        ssl_certificate_key /opt/nginx/certs/holder.key;

        # TLS 1.3 with Post-Quantum Cryptography
        ssl_protocols TLSv1.3;
        ssl_ecdh_curve X25519MLKEM768:x25519;
        # Quantum-Safe Key Exchange Groups (ML-KEM from NIST FIPS-203)
        # Groups are set via DEFAULT_GROUPS environment variable
        # Default: mlkem768:x25519:mlkem1024
        # TLS 1.3 cipher suites are automatically selected

        ssl_prefer_server_ciphers off;

        # Reverse Proxy to Holder Inbound Transport
        location / {
            proxy_pass http://holder_inbound;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;

            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
    }

    # HTTPS Server for Holder Admin API (Port 8031)
    server {
        listen 8031 ssl;
        server_name pqc-sidecarproxy-holder-admin;

        # SSL Certificates (ML-DSA-65)
        ssl_certificate /opt/nginx/certs/holder.crt;
        ssl_certificate_key /opt/nginx/certs/holder.key;

        # TLS 1.3 with Post-Quantum Cryptography
        ssl_protocols TLSv1.3;
        ssl_ecdh_curve X25519MLKEM768:x25519;
        # Quantum-Safe Key Exchange Groups (ML-KEM from NIST FIPS-203)
        # Groups are set via DEFAULT_GROUPS environment variable
        # Default: X25519MLKEM768:mlkem768:x25519:mlkem1024

        ssl_prefer_server_ciphers off;

        # Reverse Proxy to Holder Admin API
        location / {
            proxy_pass http://holder_admin;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;

            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # Health Check
        location /health {
            access_log off;
            return 200 "Holder Admin PQC Proxy OK\n";
            add_header Content-Type text/plain;
        }
    }
}
\end{lstlisting}

\subsubsection{DLT-Infrastruktur}
\label{sec:Anhang_DLT-Infrastruktur}

Die unmodifizierte von-network-Implementierung stellt vier Indy-Validator-Nodes \parencite{bcgov_VonnetworkDocsAddNewNodemdmainbcgovvonnetworkGitHub_} bereit, die über das \enquote{Practical Byzantine Fault Tolerance}-Konsensprotokoll synchronisiert werden \parencite{hyperledger_IndyNodeTroubleshootingguideHyperledgerIndyNodedocumentation_}. Jeder Node exponiert zwei Ports für die Indy-Protokollkommunikation (9701--9708) und verfügt über ein dediziertes Docker-Volume zur Persistierung des Ledger-Zustands \parencite{bcgov_VonnetworkDockercomposeymlMainbcgovvonnetworkGitHub_}. Zusätzlich zu den Validator-Nodes wird ein Webserver-Container deployt, der ein Web-Interface zur Ledger-Visualisierung sowie einen HTTP-Endpoint zur Genesis-File-Distribution bereitstellt \parencite{bcgov_VonnetworkDockercomposeymlMainbcgovvonnetworkGitHub_}. Diese Architektur ermöglicht es \ac{ACA-Py}-Agents, beim Start die Genesis-Transaktionsdatei über eine konfigurierbare URL abzurufen und sich automatisch mit dem Indy-Netzwerk zu verbinden.

Neben der Proxy-Integration wurden die Indy-Validator-Nodes und der Webserver-Container vollständig unverändert aus dem Original-Repository übernommen. Dies umfasst das Base-Image \enquote{ghcr.io/bcgov/von-image:node-1.12-6} \parencite{bcgov_VonimageNode1126_}, die Genesis-Generierungslogik in \enquote{scripts/init\_genesis.sh} \parencite{bcgov_VonnetworkScriptsMainbcgovvonnetworkGitHub_} sowie die Node-Startup-Scripts \enquote{scripts/start\_node.sh} \parencite{bcgov_VonnetworkScriptsMainbcgovvonnetworkGitHub_}. Die Beibehaltung des ursprünglichen Indy-Node-Codes gewährleistet, dass die blockchain-interne Validierung, Konsens-Mechanismen und Ledger-Operationen identisch mit etablierten Indy-Deployments funktionieren und ausschließlich die externe Kommunikationsschicht durch Post-Quantum-Kryptografie abgesichert wird.

Die Ledger-Struktur von Hyperledger Indy bleibt ebenfalls unverändert und umfasst drei logische Ledger-Typen. Der Domain Ledger speichert DIDs, Schemas und Credential Definitions, der Pool Ledger verwaltet die Validator-Node-Registry, der Config Ledger enthält Netzwerk-Konfigurationen wie Transaction Author Agreements und Acceptable Mechanism Lists \parencite{bcgov_VonnetworkDocsUsingVONNetworkmdmainbcgovvonnetworkGitHub_}. Die Persistierung dieser Ledger erfolgt über Docker-Volumes (node1-data bis node4-data) \parencite{bcgov_VonnetworkDockercomposeymlMainbcgovvonnetworkGitHub_}.

\refstepcounter{manualListingCounterA}
\label{lst:docker-compose.yml-DLT-Infrastruktur}
\begin{lstlisting}[language=bash, caption={docker-compose.yml: DLT-Infrastruktur}, numbers=left, frame=single]
version: '3'
services:
  #
  # Client
  #
  client:
    image: von-network-base
    command: ./scripts/start_client.sh
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - RUST_LOG=${RUST_LOG}
    networks:
      - von
    volumes:
      - client-data:/home/indy/.indy_client
      - ./tmp:/tmp

  #
  # Webserver
  #
  webserver:
    image: von-network-base
    command: bash -c 'sleep 10 && ./scripts/start_webserver.sh'
    container_name: von-webserver
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
      - GENESIS_URL=${GENESIS_URL}
      - LEDGER_SEED=${LEDGER_SEED}
      - LEDGER_CACHE_PATH=${LEDGER_CACHE_PATH}
      - MAX_FETCH=${MAX_FETCH:-50000}
      - RESYNC_TIME=${RESYNC_TIME:-120}
      - POOL_CONNECTION_ATTEMPTS=${POOL_CONNECTION_ATTEMPTS:-5}
      - POOL_CONNECTION_DELAY=${POOL_CONNECTION_DELAY:-10}
      - REGISTER_NEW_DIDS=${REGISTER_NEW_DIDS:-True}
      - ENABLE_LEDGER_CACHE=${ENABLE_LEDGER_CACHE:-True}
      - ENABLE_BROWSER_ROUTES=${ENABLE_BROWSER_ROUTES:-True}
      - DISPLAY_LEDGER_STATE=${DISPLAY_LEDGER_STATE:-True}
      - LEDGER_INSTANCE_NAME=${LEDGER_INSTANCE_NAME:-localhost}
      - LEDGER_DESCRIPTION=${LEDGER_DESCRIPTION}
      - WEB_ANALYTICS_SCRIPT=${WEB_ANALYTICS_SCRIPT}
      - INFO_SITE_TEXT=${INFO_SITE_TEXT}
      - INFO_SITE_URL=${INFO_SITE_URL}
      - INDY_SCAN_URL=${INDY_SCAN_URL}
      - INDY_SCAN_TEXT=${INDY_SCAN_TEXT}
    networks:
      - von
    # ports:
    #   - ${WEB_SERVER_HOST_PORT:-9000}:8000
    volumes:
      - ./config:/home/indy/config
      - ./server:/home/indy/server
      - webserver-cli:/home/indy/.indy-cli
      - webserver-ledger:/home/indy/ledger

  #
  # Synchronization test
  #
  synctest:
    image: von-network-base
    command: ./scripts/start_synctest.sh
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    networks:
      - von
    ports:
      - ${WEB_SERVER_HOST_PORT:-9000}:8000
    volumes:
      - ./config:/home/indy/config
      - ./server:/home/indy/server
      - webserver-cli:/home/indy/.indy-cli
      - webserver-ledger:/home/indy/ledger

  #
  # Nodes
  #
  nodes:
    image: von-network-base
    command: ./scripts/start_nodes.sh
    networks:
      - von
    ports:
      - 9701:9701
      - 9702:9702
      - 9703:9703
      - 9704:9704
      - 9705:9705
      - 9706:9706
      - 9707:9707
      - 9708:9708
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    volumes:
      - nodes-data:/home/indy/ledger

  node1:
    image: von-network-base
    command: ./scripts/start_node.sh 1
    networks:
      - von
    ports:
      - 9701:9701
      - 9702:9702
    container_name: von-node1
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    volumes:
      - node1-data:/home/indy/ledger

  node2:
    image: von-network-base
    command: ./scripts/start_node.sh 2
    networks:
      - von
    ports:
      - 9703:9703
      - 9704:9704
    container_name: von-node2
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    volumes:
      - node2-data:/home/indy/ledger

  node3:
    image: von-network-base
    command: ./scripts/start_node.sh 3
    networks:
      - von
    ports:
      - 9705:9705
      - 9706:9706
    container_name: von-node3
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    volumes:
      - node3-data:/home/indy/ledger

  node4:
    image: von-network-base
    command: ./scripts/start_node.sh 4
    networks:
      - von
    ports:
      - 9707:9707
      - 9708:9708
    container_name: von-node4
    environment:
      - IP=${IP}
      - IPS=${IPS}
      - DOCKERHOST=${DOCKERHOST}
      - LOG_LEVEL=${LOG_LEVEL}
      - RUST_LOG=${RUST_LOG}
    volumes:
      - node4-data:/home/indy/ledger

  # Post-Quantum Nginx Reverse Proxy für VON Network Webserver
  pqc-sidecarproxy-webserver:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: von-pqc-sidecarproxy-webserver
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von
      - sidecarproxy
    ports:
      - 8000:8000  # HTTPS with Post-Quantum Cryptography (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_webserver.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - webserver
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

networks:
  von:
  sidecarproxy:

volumes:
  client-data:
  webserver-cli:
  webserver-ledger:
  node1-data:
  node2-data:
  node3-data:
  node4-data:
  nodes-data:
  nginx-logs:
\end{lstlisting}

\subsubsection{Revocation Registry}
\label{sec:Anhang_Revocation Registry}

Der unmodifizierte indy-tails-server wird durch das Skript \enquote{./manage start} \parencite{bcgov_IndytailsserverDockerManagemainbcgovindytailsserverGitHub_} aus dem \enquote{docker}-Verzeichnis deployt, welches als Wrapper um Docker-Compose-Befehle fungiert und die Orchestrierung mehrerer Container-Services koordiniert. Der indy-tails-server läuft standardmäßig auf Port 6543 \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}.

Der Server exponiert vier REST-Endpunkte für Upload- und Download-Operationen, wobei zwei Adressierungsmethoden unterstützt werden. Für Upload-Operationen steht der Endpunkt \enquote{PUT /{revoc\_reg\_id}} zur Verfügung, der als Multipart-File-Upload mit zwei Pflichtfeldern implementiert ist. Das erste Feld muss \enquote{genesis} (Genesis-Transaktionsdatei) heißen, das zweite \enquote{tails} (Tails-Datei) \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}. Der Server validiert die Integrität der hochgeladenen Datei, indem er die Revocation Registry Definition aus dem Ledger abruft und den Hash mit dem \enquote{fileHash}-Attribut abgleicht \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}. Als Alternative existiert der Hash-basierte Upload-Endpunkt \enquote{PUT /hash/{tails-hash}}, der die Datei gegen den übermittelten Hash validiert und zusätzliche Struktur-Checks durchführt \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}. Die Download-Endpunkte folgen einem analogen Schema mit \enquote{GET /{revoc\_reg\_id}} und \enquote{GET /hash/{tails-hash}} für den Abruf existierender Tails-Dateien \parencite{bcgov_IndytailsserverREADMEmdMainbcgovindytailsserverGitHub_}.

\refstepcounter{manualListingCounterA}
\label{lst:docker-compose.yml-Revocation-Registry}
\begin{lstlisting}[language=bash, caption={docker-compose.yml: Revocation Registry}, numbers=left, frame=single]
services:
  ngrok-tails-server:
    image: ngrok/ngrok
    networks:
      - tails-server
    ports:
      - 4044:4040
    command: start --all
    environment:
      - NGROK_CONFIG=/etc/ngrok.yml
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    volumes:
      - ./ngrok.yml:/etc/ngrok.yml
  tails-server:
    build:
      context: ..
      dockerfile: docker/Dockerfile.tails-server
    networks:
      - tails-server
    command: >
      tails-server
        --host 0.0.0.0
        --port 6543
        --storage-path $STORAGE_PATH
        --log-level $LOG_LEVEL
        --log-config $LOGGING_CONFIG
  tester:
    build:
      context: ..
      dockerfile: docker/Dockerfile.test

  # Post-Quantum Nginx Reverse Proxy für Tails Server
  pqc-sidecarproxy-tails-server:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-tails-server
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - tails-server
      - von_sidecarproxy
    ports:
      - 6543:6543  # HTTPS with Post-Quantum Cryptography (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_tails-server.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - tails-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:6543/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

networks:
  tails-server:
  von_sidecarproxy:
    external: true

volumes:
  nginx-logs:
\end{lstlisting}

\pagebreak

\subsubsection{Dockerfile: acapy-base}

\refstepcounter{manualListingCounterA}
\label{lst:Dockerfile-acapy-base}
\begin{lstlisting}[language=bash, caption={Dockerfile - acapy-base}, numbers=left, frame=single]
ARG python_version=3.12
FROM python:${python_version}-slim-bookworm AS build

RUN pip install --no-cache-dir poetry==2.1.1

WORKDIR /src

COPY ./pyproject.toml ./poetry.lock ./
RUN poetry install --no-root

COPY ./acapy_agent ./acapy_agent
COPY ./README.md /src
RUN poetry build

FROM python:${python_version}-slim-bookworm AS main

ARG uid=1001
ARG user=aries
ARG acapy_name="acapy-agent"
ARG acapy_version
ARG acapy_reqs=[didcommv2]

ENV HOME="/home/$user" \
    APP_ROOT="/home/$user" \
    LC_ALL=C.UTF-8 \
    LANG=C.UTF-8 \
    PIP_NO_CACHE_DIR=off \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    RUST_LOG=warn \
    SHELL=/bin/bash \
    SUMMARY="$acapy_name image" \
    DESCRIPTION="$acapy_name provides a base image for running acapy agents in Docker. \
    This image layers the python implementation of $acapy_name $acapy_version. Based on Debian Buster."

LABEL summary="$SUMMARY" \
    description="$DESCRIPTION" \
    io.k8s.description="$DESCRIPTION" \
    io.k8s.display-name="$acapy_name $acapy_version" \
    name=$acapy_name \
    acapy.version="$acapy_version" \
    maintainer=""

# Add aries user
RUN useradd -U -ms /bin/bash -u $uid $user

# Install environment
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    apt-transport-https \
    ca-certificates \
    curl \
    git \
    libffi-dev \
    libgmp10 \
    libncurses5 \
    libncursesw5 \
    openssl \
    sqlite3 \
    zlib1g && \
    apt-get autopurge -y && \
    apt-get clean -y && \
    rm -rf /var/lib/apt/lists/* /usr/share/doc/*

WORKDIR $HOME

# Add local binaries and aliases to path
ENV PATH="$HOME/.local/bin:$PATH"

# - In order to drop the root user, we have to make some directories writable
#   to the root group as OpenShift default security model is to run the container
#   under random UID.
RUN usermod -a -G 0 $user

# Create standard directories to allow volume mounting and set permissions
# Note: PIP_NO_CACHE_DIR environment variable should be cleared to allow caching
RUN mkdir -p \
    $HOME/.acapy_agent \
    $HOME/.cache/pip/http \
    $HOME/.indy_client \
    $HOME/ledger/sandbox/data \
    $HOME/log

# The root group needs access the directories under $HOME/.indy_client and $HOME/.acapy_agent for the container to function in OpenShift.
RUN chown -R $user:root $HOME/.indy_client $HOME/.acapy_agent && \
    chmod -R ug+rw $HOME/log $HOME/ledger $HOME/.acapy_agent $HOME/.cache $HOME/.indy_client

# Create /home/indy and symlink .indy_client folder for backwards compatibility with artifacts created on older indy-based images.
RUN mkdir -p /home/indy
RUN ln -s /home/aries/.indy_client /home/indy/.indy_client

# Install ACA-py from the wheel as $user,
# and ensure the permissions on the python 'site-packages' and $HOME/.local folders are set correctly.
USER $user
COPY --from=build /src/dist/acapy_agent*.whl .
RUN acapy_agent_package=$(find ./ -name "acapy_agent*.whl" | head -n 1) && \
    echo "Installing ${acapy_agent_package} ..." && \
    pip install --no-cache-dir --find-links=. ${acapy_agent_package}${acapy_reqs} && \
    rm acapy_agent*.whl && \
    chmod +rx $(python -m site --user-site) $HOME/.local

ENTRYPOINT ["aca-py"]
\end{lstlisting}

\subsubsection{SSI-Agenten}
\label{sec:Anhang_SSI-Agenten}

Die Agent-Konfiguration in Listing~A-\ref{lst:docker-compose.yml-SSI-Agenten} erfolgt vollständig deklarativ über Docker-Compose-Service-Definitionen, die jeweils den \enquote{start}-Befehl von ACA-Py mit rollenspezifischen Parametern aufrufen. Zentrale Konfigurationsparameter umfassen \enquote{--label} (zur Identifikation des Agents), \enquote{--inbound-transport http 0.0.0.0 <port>} (zur Definition des \gls{DIDComm}-Message-Endpoints), \enquote{--outbound-transport http} (für ausgehende Verbindungen), \enquote{--admin 0.0.0.0 <port>} (zur Aktivierung der Admin-API) sowie \enquote{--wallet-type askar} (zur Spezifikation des Wallet-Backends Aries Askar).

Ein kritischer Konfigurationsparameter ist \enquote{--endpoint https://host.docker.internal:<port>}, der spezifiziert, unter welcher URL der Agent für eingehende \gls{DIDComm}-Verbindungen erreichbar ist. Dieser Parameter wird in Out-of-Band-Invitations und DID-Exchange-Nachrichten eingebettet und muss auf den externen PQC-Proxy-Endpoint zeigen, nicht auf den internen Agent-Container. Beispielsweise konfiguriert der Issuer-Agent \enquote{--endpoint https://host.docker.internal:8020}, wodurch andere Agents ihre \gls{DIDComm}-Nachrichten an den PQC-Proxy auf Port 8020 senden, der diese nach TLS-Terminierung an den internen Issuer-Container weiterleitet. Diese Indirektion ist essentiell, um sicherzustellen, dass alle Agent-zu-Agent-Verbindungen die Post-Quantum-gesicherte Transport-Layer-Sicherheit nutzen.

Die Wallet-Konfiguration erfolgt über die Parameter \enquote{--wallet-name <name>}, \enquote{--wallet-key <key>} und \enquote{--auto-provision}, wobei letzterer sicherstellt, dass das Wallet automatisch beim ersten Start initialisiert wird. Jedes Wallet wird in einem dedizierten Docker-Volume persistiert (\enquote{issuer-data}, \enquote{holder-data}, \enquote{verifier-data}), sodass DID-Keypairs, Credentials und Connections über Container-Neustarts hinweg erhalten bleiben.

Ein wesentlicher Aspekt der Agent-Konfiguration ist die Anbindung an die DLT-Infrastruktur über den Parameter \enquote{--genesis-url https://host.docker.internal:8000/genesis}. Diese URL referenziert den PQC-gesicherten Webserver der DLT-Infrastruktur und ermöglicht es den Agents, beim Start die Genesis-Transaktionsdatei über eine quantensichere TLS-1.3-Verbindung abzurufen. Analog dazu wird die Verbindung zur Revocation Registry über \enquote{--tails-server-base-url https://host.docker.internal:6543} konfiguriert, wobei ebenfalls der nginx-\ac{PQC}-\gls{Sidecar-Proxy} als Endpoint fungiert. Die Verwendung von \enquote{host.docker.internal} ermöglicht dabei die Adressierung des Docker-Hosts aus Container-Perspektive und abstrahiert plattformspezifische Netzwerk-Unterschiede zwischen Linux, macOS und Windows.

Zusätzlich zu den grundlegenden Konfigurations-Parametern aktivieren die Agents mehrere Auto-Response-Features, die für Entwicklungs- und Testzwecke die manuelle Interaktion reduzieren: \enquote{--auto-accept-invites} (automatisches Akzeptieren von Connection-Invitations), \enquote{--auto-respond-credential-proposal}, \enquote{--auto-respond-credential-offer} und \enquote{--auto-respond-credential-request} (automatische Credential-Exchange-Antworten beim Issuer), \enquote{--auto-store-credential} (automatisches Speichern empfangener Credentials beim Holder) sowie \enquote{--auto-verify-presentation} (automatische Presentation-Verifikation beim Verifier). Diese Automatisierungen ermöglichen vollständig scriptgesteuerte SSI-Workflows über die Admin-API, wie sie in Jupyter-Notebook-basierten Demonstrationen implementiert sind.

Für die Post-Quantum-Absicherung der Agent-Kommunikation wurde analog zur DLT-Infrastruktur und Revocation Registry ein dedizierter nginx-\ac{PQC}-\gls{Sidecar-Proxy} pro Agent implementiert. Die Docker-Compose-Konfiguration definiert drei zusätzliche Services -- \enquote{pqc-sidecarproxy-issuer}, \enquote{pqc-sidecarproxy-holder} und \enquote{pqc-sidecarproxy-verifier} --, die jeweils als Reverse Proxy vor dem zugehörigen Agent platziert werden. Jeder Agent-Container exponiert zwei interne HTTP-Ports: einen für Inbound-Transport (8020, 8030, 8040) und einen für die Admin-API (8021, 8031, 8041). Diese Ports sind ausschließlich im agent-spezifischen internen Docker-Netzwerk (\enquote{hope-issuer}, \enquote{hope-holder}, \enquote{hope-verifier}) zugänglich.

Die Sidecar-Proxies terminieren alle eingehenden TLS-1.3-Verbindungen und leiten die Anfragen nach erfolgreicher quantensicherer Authentifizierung und Schlüsselvereinbarung als unverschlüsseltes HTTP an die internen Agent-Ports weiter. Jeder Proxy ist mit zwei Docker-Netzwerken verbunden, dem agent-spezifischen internen Netzwerk für Backend-Kommunikation sowie dem gemeinsamen externen \enquote{von\_sidecarproxy}-Netzwerk für Client-Zugriffe. Diese Dual-Network-Architektur erzwingt, dass sämtliche externe Kommunikation -- einschließlich \gls{DIDComm}-Nachrichten zwischen Agents, Admin-API-Zugriffe durch Controller-Anwendungen sowie Ledger- und Tails-Server-Requests -- über quantensichere TLS-Verbindungen erfolgt.

Die Netzwerk-Architektur folgt einem strikten Isolation-Prinzip. Jeder Agent residiert in einem dedizierten internen Docker-Netzwerk, das ausschließlich den Agent-Container und den zugehörigen PQC-Proxy umfasst. Die Agents können untereinander nicht direkt kommunizieren, sondern ausschließlich über das externe \enquote{von\_sidecarproxy}-Netzwerk, das die \ac{PQC}-Sidecar-Proxies verbindet. Diese Segmentierung erzeugt eine \enquote{Defense in Depth}-Architektur nach \textcite[S. 242--243]{alsaqour_DefenseDepthMultilayersecurity_2021}, bei der selbst bei einer Kompromittierung eines Agent-Containers der Zugriff auf andere Agents durch Netzwerk-Isolation verhindert wird.

Die Health-Check-Konfiguration der Agent-Container überwacht die Verfügbarkeit der Admin-API mittels periodischer HTTP-Requests an \enquote{http://localhost:<admin-port>/status/ready}. Zusätzlich definieren die Service-Dependencies (\enquote{depends\_on}) eine Startup-Reihenfolge, die sicherstellt, dass die \ac{PQC}-Sidecar-Proxies vor den Agents starten und dass die Infrastruktur-Services (von-network, Tails-Server) vollständig initialisiert sind, bevor die Agents ihre Genesis-Datei abrufen. Diese Orchestrierung eliminiert Race-Conditions während des Deployment-Prozesses und gewährleistet eine deterministische Startup-Sequenz.

\refstepcounter{manualListingCounterA}
\label{lst:docker-compose.yml-SSI-Agenten}
\begin{lstlisting}[language=bash, caption={docker-compose.yml: SSI-Agenten}, numbers=left, frame=single]

version: '3.8'

services:
  issuer:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base
    image: acapy-base
    container_name: issuer-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
      - TAILS_FILE_COUNT=100
    networks:
      - hope-issuer
    volumes:
      - issuer-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Issuer Agent"
      --inbound-transport http 0.0.0.0 8020
      --outbound-transport http
      --endpoint https://host.docker.internal:8020
      --admin 0.0.0.0 8021
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name issuer_wallet
      --wallet-key issuer_wallet_key_000000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-respond-credential-proposal
      --auto-respond-credential-offer
      --auto-respond-credential-request
      --auto-verify-presentation
      --public-invites
      --preserve-exchange-records
      --tails-server-base-url https://host.docker.internal:6543
      --notify-revocation
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8021/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  holder:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base
    image: acapy-base
    container_name: holder-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
    networks:
      - hope-holder
    volumes:
      - holder-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Holder Agent"
      --inbound-transport http 0.0.0.0 8030
      --outbound-transport http
      --endpoint https://host.docker.internal:8030
      --admin 0.0.0.0 8031
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name holder_wallet
      --wallet-key holder_wallet_key_000000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-respond-credential-offer
      --auto-store-credential
      --public-invites
      --tails-server-base-url https://host.docker.internal:6543
      --preserve-exchange-records
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8031/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  verifier:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base
    image: acapy-base
    container_name: verifier-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
    networks:
      - hope-verifier
    volumes:
      - verifier-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Verifier Agent"
      --inbound-transport http 0.0.0.0 8040
      --outbound-transport http
      --endpoint https://host.docker.internal:8040
      --admin 0.0.0.0 8041
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name verifier_wallet
      --wallet-key verifier_wallet_key_00000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-verify-presentation
      --public-invites
      --tails-server-base-url https://host.docker.internal:6543
      --preserve-exchange-records
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8041/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  pqc-sidecarproxy-issuer:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-issuer
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-issuer
    ports:
      - "8020:8020"  # Issuer Inbound Transport HTTPS (ML-KEM-768)
      - "8021:8021"  # Issuer Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_issuer.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8021/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  pqc-sidecarproxy-holder:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-holder
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-holder
    ports:
      - "8030:8030"  # Holder Inbound Transport HTTPS (ML-KEM-768)
      - "8031:8031"  # Holder Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_holder.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8031/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  pqc-sidecarproxy-verifier:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-verifier
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-verifier
    ports:
      - "8040:8040"  # Verifier Inbound Transport HTTPS (ML-KEM-768)
      - "8041:8041"  # Verifier Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_verifier.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8041/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

networks:
  hope-issuer:
  hope-holder:
  hope-verifier:
  von_sidecarproxy:
    external: true

volumes:
  issuer-data:
  holder-data:
  verifier-data:
  nginx-logs:
\end{lstlisting}

\subsubsection{Docker Orchestrierung der Gesamtarchitektur}
\label{sec:Anhang_Docker Orchestrierung der Gesamtarchitektur}

\refstepcounter{manualListingCounterA}
\label{lst:Docker-Compose-Start-der-Gesamtarchitektur}
\begin{lstlisting}[language=bash, caption={Docker Compose Start der Gesamtarchitektur}, numbers=left, frame=single]
(.venv) ferris@blockchain-ssi-pqc:~/github/MSc-blockchain-ssi-pqc$ ./von-network/manage start && ./indy-tails-server/docker/manage start && docker compose -f ./hopE/docker-compose.yml up -d
[+] Running 15/15
Network von_von                                 Created     0.0s 
Network von_sidecarproxy                        Created     0.0s 
Volume "von_webserver-ledger"                   Created     0.0s 
Volume "von_node1-data"                         Created     0.0s 
Volume "von_node3-data"                         Created     0.0s 
Volume "von_node2-data"                         Created     0.0s 
Volume "von_node4-data"                         Created     0.0s 
Volume "von_nginx-logs"                         Created     0.0s 
Volume "von_webserver-cli"                      Created     0.0s 
Container von-node4                             Started     0.6s 
Container von-webserver                         Started     0.4s 
Container von-node3                             Started     0.4s 
Container von-node2                             Started     0.5s 
Container von-node1                             Started     0.5s 
Container von-pqc-sidecarproxy-webserver        Started     0.6s 
Want to see the scrolling container logs? Run "./manage logs"
[+] Running 4/4
Network docker_tails-server                     Created     0.0s 
Volume "docker_nginx-logs"                      Created     0.0s 
Container docker-tails-server-1                 Started     0.3s 
Container pqc-sidecarproxy-tails-server         Started     0.5s 
Run './manage logs' for logs
WARN[0000] /home/ferris/github/MSc-blockchain-ssi-pqc/hopE/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 13/13
Network hope_hope-issuer                        Created     0.0s 
Network hope_hope-holder                        Created     0.0s 
Network hope_hope-verifier                      Created     0.0s 
Volume "hope_issuer-data"                       Created     0.0s 
Volume "hope_holder-data"                       Created     0.0s 
Volume "hope_verifier-data"                     Created     0.0s 
Volume "hope_nginx-logs"                        Created     0.0s 
Container issuer-agent                          Started     0.6s 
Container holder-agent                          Started     0.5s 
Container verifier-agent                        Started     0.5s 
Container pqc-sidecarproxy-holder               Started     1.3s 
Container pqc-sidecarproxy-issuer               Started     1.0s 
Container pqc-sidecarproxy-verifier             Started     1.2s
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:von-network-manage-script}
\begin{lstlisting}[language=bash, caption={von-network manage Skript}, numbers=left, frame=single]
#!/bin/bash
export MSYS_NO_PATHCONV=1

# Default Options
export TA_RATIFICATION_TIME_OPS='+%s --date='
export STAT_OPS='-c '%a''

# MAC OS Options
if [[ $OSTYPE == 'darwin'* ]]; then
  # Set default platform to linux/amd64 when running on Arm based MAC since there are no arm based images available currently.
  architecture=$(uname -m)
  if [[ "${architecture}" == 'arm'* ]] || [[ "${architecture}" == 'aarch'* ]]; then
    export DOCKER_DEFAULT_PLATFORM=linux/amd64
  fi

  # Set the date and stat options appropriatly for MAC OS.
  export TA_RATIFICATION_TIME_OPS='-jf '%Y-%m-%dT%H:%M:%S%Z' +%s '
  export STAT_OPS='-f '%A''
fi

# getDockerHost; for details refer to https://github.com/bcgov/DITP-DevOps/tree/main/code/snippets#getdockerhost
. /dev/stdin <<<"$(cat <(curl -s --raw https://raw.githubusercontent.com/bcgov/DITP-DevOps/main/code/snippets/getDockerHost))"
export DOCKERHOST=$(getDockerHost)

SCRIPT_HOME="$( cd "$( dirname "$0" )" && pwd )"
export COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-von}"

export TMP_FOLDER='./tmp'
export CLI_SCRIPTS_FOLDER='./cli-scripts'
export DEFAULT_CLI_SCRIPT_DIR="${CLI_SCRIPTS_FOLDER}"

export LEDGER_TIMEOUT="${LEDGER_TIMEOUT:-60}"
export LEDGER_URL_CONFIG="${LEDGER_URL_CONFIG}"

export ROOT_BACKUP_DIR=backup
export SHELL_CMD='bash'

# ===========================================================
# Check Docker Compose
# ===========================================================

# Default to deprecated V1 'docker-compose'.
dockerCompose="docker-compose --log-level ERROR"

# Prefer 'docker compose' V2 if available
if [[ $(docker compose version 2> /dev/null) == 'Docker Compose'* ]]; then
  dockerCompose="docker --log-level error compose"
fi

# ===========================================================
# Usage:
# ===========================================================
usage () {
  cat <<-EOF

  Usage: $0 [command] [--logs] [options]

  Commands:

  build - Build the docker images for the project.
          You need to do this first.

  start | up - Starts all containers
       You can include a '--wait' parameter which will wait until the ledger is active
       You can include a '--taa-sample' parameter which will install the sample Transaction Authorisation Agreement
        files into the ledger. Alternatively you can create your own config/aml.json and config/taa.json files which
        will get installed into the ledger at start (see config/sample_aml.json and config/sample_taa.json for format)
       When using the '--logs' option, use ctrl-c to exit logging. Use "down" or "stop" to stop the run.
        Examples:
        $0 start
        $0 start --logs
        $0 start <ip_proxy_1>,<ip_proxy_2>,<ip_proxy_3>,<ip_proxy_4> &
        $0 start --wait --logs
        $0 start --taa-sample

  start-web - Start the web server to monitor an existing ledger, requires GENESIS_URL and LEDGER_SEED params
        Example:
        $0 start-web GENESIS_URL=http://foo.bar/genesis.txt LEDGER_SEED=00000000000000000000000000000012

  logs - To tail the logs of running containers (ctrl-c to exit).
         Use the '--no-tail' option to only print log without tailing.
          Examples:
          $0 logs
          $0 logs --no-tail

  down | rm - Brings down the services and removes the volumes (storage) and containers.

  stop - Stops the services.  This is a non-destructive process.  The volumes and containers
         are not deleted so they will be reused the next time you run start.

  rebuild - Rebuild the docker images.

  dockerhost - Print the ip address of the Docker Host Adapter as it is seen by containers running in docker.

  generateSecrets - Generate a random set of secrets using openssl; a Seed and a Key.

  generateDid - Generates a DID and Verkey from a Seed.
        $0 generateDid [seed]
          - Optional [seed]; if one is not provided a random one will be generated using openssl.

  generateGenesisFiles - Generates pool and domain genesis files from data input via csv files.
        $0 generategenesisfiles <trustees_csv_file> <stewards_csv_file>

        This is a convenience command wrapped around the Steward Tools script for generating genesis files found here;
        https://github.com/sovrin-foundation/steward-tools/tree/master/create_genesis

        The script is downloaded and hosted in a running container which has the required packages installed.

        Examples of the csv files can be downloaded from here;
        https://docs.google.com/spreadsheets/d/1LDduIeZp7pansd9deXeVSqGgdf0VdAHNMc7xYli3QAY/edit#gid=0
        Download each sheet separately in csv format and fill them out with the data specific to your network.

        The input csv files must be placed into the ./tmp/ folder.
        The resulting output 'pool_transactions_genesis' and 'domain_transactions_genesis' files will be placed
        in the ./tmp/ folder.

        Example:
        $0 generategenesisfiles "./tmp/CANdy Beta Genesis File Node info - Trustees.csv" "./tmp/CANdy Beta Genesis File Node info - Stewards.csv"

  apply-taa - Registers an AML and TAA on a given network.
              This is a convent wrapper around the cli-scripts/apply-taa batch script,
              which simplifies and automates the process of registering an AML and TAA.

              The following parameters are simply the standard inputs for the indy-cli batch script.
              Refer to the cli-scripts/apply-taa documentation for details;
              - walletName          - Required
              - storageType         - Optional
              - storageConfig       - Optional
              - storageCredentials  - Optional
              - poolName            - Required
              - useDid              - Required

              The following parameters are required:
              amlUrl
                - The URL to the raw content of the AML.  Provided the link contains the AML expected content in json format
                  (contained in {}s) it will be parsed from the link's text.
              amlVersion
                - The version of the AML.
              taaUrl
                - The URL to the raw content of the TAA in markdown format.
              taaVersion
                - The version of the TAA
              taaRatificationTime
                - The date and time of TAA ratification by network government.
                - On Linux and Windows (Git Bash):
                  - The date format is "%Y-%m-%dT%H:%M:%S%Z", where `%Z` is a numeric time zone offset, or an alphabetic time zone abbreviation.
                  - Examples:
                    - "2022-08-09T11:05:00-0700"
                    - "2022-08-09T11:05:00PDT"
                - On Mac:
                  - The date format is "%Y-%m-%dT%H:%M:%S%Z", where `%Z` is an alphabetic time zone abbreviation.
                  - Example:
                    - "2022-08-09T11:05:00PDT"

        Example:
        $0 apply-taa \
                walletName=local_net_trustee_wallet \
                poolName=local_net \
                useDid=V4SGRU86Z58d6TV7PBUe6f \
                amlUrl='https://raw.githubusercontent.com/wiki/bcgov/bc-vcpedia/(Layer-1)-CANdy-Acceptance-Mechanism-List-(AML).md' \
                amlVersion=0.1 \
                taaUrl='https://raw.githubusercontent.com/wiki/bcgov/bc-vcpedia/(Layer-1)-CANdy-Transaction-Author-Agreement.md' \
                taaVersion=0.1 \
                taaRatificationTime="2022-08-09T11:05:00PDT"

  truncateLogs - Truncate the container logs.

  indy-cli - Run Indy-Cli commands in a Indy-Cli container environment.

        $0 indy-cli -h
          - Display specific help documentation.

  cli - Run a command in an Indy-Cli container.

        $0 cli -h
          - Display specific help documentation.

  backup [description] - Backup the current von-network environment.
          Creates a set of tar.gz archives of each of the environment's volumes.
          Backup sets are stored in a ./backup/date/time folder structure.
          Examples:
          $0 backup
          $0 backup "The description of my environment's current state."
            - Make a backup and include the description in a ReadMe.txt file.

  restore [filter] - Restore a given backup set.  Defaults to restoring the most recent backup.
          Examples:
          $0 restore
            - Restore the most recent backup.
          $0 restore 13-15-37
            - Restore the backup from 13-15-37 today.
          $0 restore von_client-data_2021-08-23_08-21-08.tar
            - Restore the backup set containing the von_client-data_2021-08-23_08-21-08.tar archive.
          $0 restore 2021-08-23
            - Restore the most recent backup set from 2021-08-23.

  restoreArchive <archive> <volume> [tarOptions]- Restore a tar.gz archive to a named volume.
          Useful for restoring an archive for inspection and debugging purposes.
          Examples:
          $0 restoreArchive ./backup/Node3-ledger-backup.tar.gz node3-bcovrin-tes
          $0 restoreArchive ./backup/Node3-ledger-backup.tar.gz node3-bcovrin-test --strip=1
            - Restore the archive to the named volume, stripping the first level directory from the archive.
            - Useful in the scenario where the archive contains additional directory levels that aren't needed in the restored copy.

  debugVolume <volume> [volumeMountFolder] - Mount a named volume into a 'debug' instance of the 'von-network-base' image with an interactive shell.
          Provides a containerized environment to perform analysis on the ledger databases and files.
          Starting with 'von-image:node-1.12-4' the base image for von-network contains the RocksDB sst_dump tool that can be used to verify
          and inspect the RocksDB database files; '*.sst' files.
          For example the command 'find /debug_volume/ -name "*.sst" | xargs -I {} sst_dump --file={} --command=verify' can be used to do a quick
          verification on all the database files once the container starts.
          Usage information for sst_dump can be found here; https://github.com/facebook/rocksdb/wiki/Administration-and-Data-Access-Tool
          Examples:
          $0 debugVolume node3-bcovrin-test
          $0 debugVolume node1-bcovrin-test /home/indy/ledger
            - Mount the named volume to '/home/indy/ledger'
EOF
exit 1
}

indyCliUsage () {
  cat <<-EOF

  Usage:
    $0 [options] indy-cli [-h] [command] [parameters]

    Run Indy-Cli commands in a Indy-Cli container environment.
      - Refer to the cli-scripts directory for available scripts and their parameters.
      - Refer to './docs/Writing Transactions to a Ledger for an Un-privileged Author.md' for
        additional examples.

    Options:
      -v <FullyQualifiedPathToScripts/>
        - Mount a script volume to the container.  By default the 'cli-scripts' directory is mounted to the container.

    Examples:

    $0 indy-cli
      - Start an interactive indy-cli session in your Indy-Cli Container.

    $0 indy-cli --help
      - Get usage information for the indy-cli.
EOF
exit 1
}

cliUsage () {
  cat <<-EOF

  Usage:
    $0 [options] cli [-h] [command]

    Run a command in an Indy-Cli container.

    Options:
      -v <FullyQualifiedPathToScripts/>
        - Mount a script volume to the container.  By default the 'cli-scripts' directory is mounted to the container.

    Examples:

    $0 cli reset
      - Reset your Indy-CLI container's environment

    $0 cli init-pool localpool http://192.168.65.3:9000/genesis
    $0 cli init-pool MainNet https://raw.githubusercontent.com/sovrin-foundation/sovrin/stable/sovrin/pool_transactions_live_genesis
      - Initialize the pool for your Indy-CLI container's environment.
EOF
exit 1
}

# ----------------------------------------------------------
# Initialization:
# ----------------------------------------------------------
while getopts v:h FLAG; do
  case $FLAG in
    v ) VOLUMES=$OPTARG ;;
    h ) usage ;;
    \? ) #unrecognized option - show help
      echo -e \\n"Invalid script option: -${OPTARG}"\\n
      usage
      ;;
  esac
done
shift $((OPTIND-1))

# ----------------------------------------------------------
# Functions:
# ----------------------------------------------------------
function toLower() {
  echo $(echo ${@} | tr '[:upper:]' '[:lower:]')
}

function initDockerBuildArgs() {
  dockerBuildArgs=""

  # HTTP proxy, prefer lower case
  if [[ "${http_proxy}" ]]; then
    dockerBuildArgs=" ${dockerBuildArgs} --build-arg http_proxy=${http_proxy}"
  else
    if [[ "${HTTP_PROXY}" ]]; then
      dockerBuildArgs=" ${dockerBuildArgs} --build-arg http_proxy=${HTTP_PROXY}"
    fi
  fi

  # HTTPS proxy, prefer lower case
  if [[ "${https_proxy}" ]]; then
    dockerBuildArgs=" ${dockerBuildArgs} --build-arg https_proxy=${https_proxy}"
  else
    if [[ "${HTTPS_PROXY}" ]]; then
      dockerBuildArgs=" ${dockerBuildArgs} --build-arg https_proxy=${HTTPS_PROXY}"
    fi
  fi

  echo ${dockerBuildArgs}
}

function initEnv() {

  if [ -f .env ]; then
    while read line; do
      if [[ ! "$line" =~ ^\# ]] && [[ "$line" =~ .*= ]]; then
        export ${line//[$'\r\n']}
      fi
    done <.env
  fi

  for arg in "$@"; do
    # Remove recognized arguments from the list after processing.
    shift
    case "$arg" in
      *=*)
        export "${arg}"
        ;;
      --logs)
        TAIL_LOGS=1
        ;;
      --wait)
        WAIT_FOR_LEDGER=1
        ;;
      --taa-sample)
        USE_SAMPLE_TAA=1
        ;;
      *)
        # If not recognized, save it for later processing ...
        set -- "$@" "$arg"
        ;;
    esac
  done

  IP=""
  IPS=""
  if [ ! -z $(echo ${1} | grep '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}') ]; then
    if [[ $1 == *","* ]]; then
      IPS="$1"
    else
      IP="$1"
    fi
  fi
  export IP="$IP" IPS="$IPS"

  export LEDGER_SEED=${LEDGER_SEED}

  export LOG_LEVEL=${LOG_LEVEL:-info}
  export RUST_LOG=${RUST_LOG:-warning}
}

function runCliCommand() {

  unset displayCliUsage
  for arg in "$@"; do
    # Remove recognized arguments from the list after processing.
    shift
    case "$arg" in
      -h)
        displayCliUsage=1
        ;;
      *)
        # If not recognized, save it for later processing ...
        set -- "$@" "$arg"
        ;;
    esac
  done

  initEnv "$@"
  cliCmd="${1}"
  shift || cliCmd=""

  if [ ! -z "${displayCliUsage}" ] && [[ "${cliCmd}" == "indy-cli" ]]; then
    indyCliUsage
  elif [ ! -z "${displayCliUsage}" ] && [[ -z "${cliCmd}" ]]; then
    cliUsage
  fi

  cmd="${dockerCompose} \
    run "

  if [ -z "${VOLUMES}" ] && [ -d "${DEFAULT_CLI_SCRIPT_DIR}" ] ; then
    VOLUMES=$(realpath ${DEFAULT_CLI_SCRIPT_DIR})
  fi

  if [ ! -z "${VOLUMES}" ]; then
    shopt -s extglob
    paths=$(echo "${VOLUMES}" | sed -n 1'p' | tr ',' '\n')
    for path in ${paths}; do
      path=${path%%+(/)}
      mountPoint=${path##*/}
      if [[ "$OSTYPE" == "msys" ]]; then
        # When running on Windows, you need to prefix the path with an extra '/'
        path="/${path}"
      fi
      cmd+=" -v '${path}:/home/indy/${mountPoint}:Z'"
    done
  fi

  # Need to escape quotes and commas so they don't get removed along the way ...
  escapedArgs=$(echo $@ | sed "s~'~\\\'~g" | sed 's~\"~\\"~g' | sed 's~(~\\(~g' | sed 's~)~\\)~g')

  # Quote the escaped args so docker compose does not try to perform any processing on them ...
  # Separate the command and the args so they don't get treated as one argument by the scripts in the container ...
  cmd+="
    --rm client \
    ./scripts/manage ${cliCmd} \"${escapedArgs}\""

  eval ${cmd}
}

function logs() {
  (
    local OPTIND
    local unset _force
    local unset no_tail
    while getopts ":f-:" FLAG; do
      case $FLAG in
        f ) local _force=1 ;;
        - )
            case ${OPTARG} in
                "no-tail"*) no_tail=1
              ;;
            esac
      esac
    done
    shift $((OPTIND-1))

    log_args=()
    (( no_tail != 1 )) && log_args+=( '-f' )
    if [ ! -z "${TAIL_LOGS}" ] || [ ! -z "${_force}" ]; then
      ${dockerCompose} \
        logs \
        "${log_args[@]}" "$@"
    fi
  )
}

pingLedger(){
  ledger_url=${1}

  # ping ledger web browser for genesis txns
  local rtnCd=$(curl -s --write-out '%{http_code}' --output /dev/null ${ledger_url}/genesis)
  if (( ${rtnCd} == 200 )); then
    return 0
  else
    return 1
  fi
}

function wait_for_ledger() {
  (
    # if flag is set, wait for ledger to activate before continuing
    local rtnCd=0
    if [ ! -z "${WAIT_FOR_LEDGER}" ]; then
      # Wait for ledger server to start ...
      local startTime=${SECONDS}
      # use global LEDGER_URL
      local LEDGER_URL="${LEDGER_URL_CONFIG:-http://localhost:9000}"
      printf "waiting for ledger to start"
      while ! pingLedger "$LEDGER_URL"; do
        printf "."
        local duration=$(($SECONDS - $startTime))
        if (( ${duration} >= ${LEDGER_TIMEOUT} )); then
          echoRed "\nThe Indy Ledger failed to start within ${duration} seconds.\n"
          rtnCd=1
          break
        fi
        sleep 1
      done
    fi
    return ${rtnCd}
  )
}

function install_taa() {
  (
    # if flag is set, copy the sample config/sample_aml.json and config/sample_taa.json
    # to config/aml.json and config/taa.json. Also create a marker file so that we
    # clean up on shutdown and still support backward compatibility where people previously
    # their own custom versions and don't want them removed.
    local rtnCd=0
    if [ ! -z "${USE_SAMPLE_TAA}" ]; then
      rtnCd=$(cp -f ./config/sample_aml.json ./config/aml.json) && $(cp -f ./config/sample_taa.json ./config/taa.json)
      touch ./config/.samples.used
    fi
    return ${rtnCd}
  )
}

function remove_taa() {
  (
    # if the marker exists indicating we created the aml and taa files, make sure we remove them to clean up.
    if [ -f "./config/.samples.used" ]; then
      rm -f ./config/aml.json ./config/taa.json ./config/.samples.used
    fi
  )
}

function generateKey(){
  (
    _length=${1:-48}
    # Format can be `-base64` or `-hex`
    _format=${2:--base64}
    echo $(openssl rand ${_format} ${_length})
  )
}

function generateSeed(){
  (
    _prefix=${1}
    _seed=$(echo "${_prefix}$(generateKey 32)" | fold -w 32 | head -n 1 )
    _seed=$(echo -n "${_seed}")
    echo ${_seed}
  )
}

function generateSecrets() {
  echo
  echo "Seed: $(generateSeed)"
  echo "Key: $(generateKey)"
  echo
}

function generateDid() {
  seed=${1}
  if [ -z ${seed} ]; then
    seed=$(generateSeed)
  fi
  runCliCommand python cli-scripts/generate_did.py --seed ${seed}
}

function generateGenesisFiles() {
  trustee_csv="${1}"
  steward_csv="${2}"
  genesis_from_files_filename="genesis_from_files.py"
  genesis_from_files_url="https://raw.githubusercontent.com/sovrin-foundation/steward-tools/master/create_genesis/genesis_from_files.py"

  if [ -z "${trustee_csv}" ] || [ -z "${steward_csv}" ]; then
    echoYellow "You must supply both the trustee and steward csv files."
    exit 1
  fi

  if [[ "${trustee_csv}" != ${TMP_FOLDER}/* ]]; then
    trustee_csv="${TMP_FOLDER}/${trustee_csv}"
  fi

  if [ ! -f "${trustee_csv}" ]; then
    echoYellow "${trustee_csv} not found, please make sure you placed ${trustee_csv} in the ${TMP_FOLDER} folder."
    exit 1
  fi

  if [[ "${steward_csv}" != ${TMP_FOLDER}/* ]]; then
    steward_csv="${TMP_FOLDER}/${steward_csv}"
  fi

  if [ ! -f "${steward_csv}" ]; then
    echoYellow "${steward_csv} not found, please make sure you placed ${steward_csv} in the ${TMP_FOLDER} folder."
    exit 1
  fi

  if [ ! -f "${CLI_SCRIPTS_FOLDER}/${genesis_from_files_filename}" ]; then
    echo "Downloading the latest version of ${genesis_from_files_filename} from ${genesis_from_files_url} ..."
    curl -s -L -o ${CLI_SCRIPTS_FOLDER}/${genesis_from_files_filename} ${genesis_from_files_url}
    chmod +x ${CLI_SCRIPTS_FOLDER}/${genesis_from_files_filename}
  fi

  # Escape spaces in path ...
  trustee_csv_esc=$(echo ${trustee_csv##*/} | sed 's/ /\\ /g')
  steward_csv_esc=$(echo ${steward_csv##*/} | sed 's/ /\\ /g')
  runCliCommand cli-scripts/${genesis_from_files_filename} --pool /tmp/pool_transactions --domain /tmp/domain_transactions --trustees /tmp/${trustee_csv_esc} --stewards /tmp/${steward_csv_esc}
}

function apply-taa() {
  # Parse Args
  for arg in "$@"; do
    # Remove recognized arguments from the list after processing.
    shift
    case "${arg}" in
      *=*)
        if [[ "${arg}" == *"amlUrl"* ]]; then
          amlUrl=${arg#*=}
          continue
        fi

        if [[ "${arg}" == *"taaUrl"* ]]; then
          taaUrl=${arg#*=}
          continue
        fi

        if [[ "${arg}" == *"taaRatificationTime"* ]]; then
          taaRatificationTime=${arg#*=}
          taaRatificationTimestamp=$(date ${TA_RATIFICATION_TIME_OPS}"${taaRatificationTime}")
          continue
        fi

        batchFileArgs+=" ${arg}"
        ;;
    esac
  done

  # Create temp files for the AML and ATT
  amlPath="${TMP_FOLDER}/aml-$(date +%s).txt"
  taaPath="${TMP_FOLDER}/att-$(date +%s).txt"

  # Download the AML and ATT files
  curl -s -o ${amlPath} "${amlUrl}"
  curl -s -o ${taaPath} "${taaUrl}"

  # Parse the AML content from the file; the bit inside the '{}'s
  amlContent="{$(sed -n '/{/,/}/{/{/!{/}/!p;};}' ${amlPath} | sed 's/^ *//g')}"
  echo ${amlContent} > ${amlPath}

  # Generate the required apply-taa batch script parameters
  batchFileArgs+=" amlFile=/tmp/${amlPath##*/}"
  batchFileArgs+=" amlContext=${amlUrl}"
  batchFileArgs+=" taaFile=/tmp/${taaPath##*/}"
  batchFileArgs+=" taaRatificationTimestamp=${taaRatificationTimestamp}"

  # Run the apply-taa batch script
  runCliCommand indy-cli --config /tmp/cliconfig.json apply-taa \
                ${batchFileArgs}

  # Remove the temp files
  rm "${amlPath}" "${taaPath}"
}

function backup() {
  (
    _msg=$@
    volumes=$(${dockerCompose} config --volumes)
    timeStamp=`date +\%Y-\%m-\%d_%H-%M-%S`
    datePart=${timeStamp%%_*}
    timePart=${timeStamp#*_}
    backupDir=${ROOT_BACKUP_DIR}/${datePart}/${timePart}
    backupVolumeMount=$(getVolumeMount ./${ROOT_BACKUP_DIR})/
    mkdir -p ./${backupDir}
    chmod -R ug+rw ./${backupDir}
    if [ ! -z "${_msg}" ]; then
      echo "${_msg}" > ./${backupDir}/ReadMe.txt
    fi

    for volume in ${volumes}; do
      volume=$(echo ${volume} |sed 's~\r$~~')
      sourceVolume=${COMPOSE_PROJECT_NAME}_${volume}
      archiveName=${sourceVolume}_${timeStamp}.tar.gz
      archivePath="/${backupDir}/${archiveName}"

      echoYellow \\n"Backing up ${sourceVolume} to ${archivePath} ..."
      docker run \
        --rm \
        --name von-network-backup \
        -v ${backupVolumeMount}:/${ROOT_BACKUP_DIR} \
        -v ${sourceVolume}:/source_volume von-network-base \
        tar -czvf ${archivePath} -C /source_volume/ .
    done
  )
}

function restore() {
  (
    _fileName=${1}
    archivePath=$(findBackup ${1})
    archiveDirectory=${archivePath%/*}
    datePart=$(echo ${archivePath} | awk -F_ '{print $3}')
    timePart=$(echo ${archivePath} | awk -F_ '{print $4}')
    archiveSuffix="${datePart}_${timePart}"

    if promptForConfirmation "You are about to restore from the '${archiveDirectory}' backup set.\nYour existing data will be lost if not backed up first."; then
      volumes=$(${dockerCompose} config --volumes)
      for volume in ${volumes}; do
        volume=$(echo ${volume} |sed 's~\r$~~')
        targetVolume=${COMPOSE_PROJECT_NAME}_${volume}
        archiveName=${targetVolume}_${archiveSuffix}
        archivePath="${archiveDirectory}/${archiveName}"

        restoreArchive -q "${archivePath}" "${targetVolume}"
      done
    else
      echo -e \\n"Restore aborted."
    fi
  )
}

function restoreArchive()
{
  (
    local OPTIND
    local quiet
    unset quiet
    while getopts q FLAG; do
      case $FLAG in
        q ) quiet=1 ;;
      esac
    done
    shift $((OPTIND-1))

    archive=${1}
    volume=${2}
    tarOptions=${3} # Example "--strip=1", to remove the first directory level.
    if [ -z ${archive} ] || [ -z ${volume} ]; then
      echoYellow "You must supply the path to the archive and the name of the volume to which the archive will be restored."
      exit 1
    fi

    archiveFolder=${archive%/*}
    archiveName=${archive##*/}
    archiveToRestore=/${ROOT_BACKUP_DIR}/${archiveName}
    archiveVolumeMount=$(getVolumeMount ${archiveFolder})

    if [ ! -z "${quiet}" ] || promptForConfirmation "You are about to restore '${archive}' to ${volume}.\nYour existing data will be lost if not backed up first." ; then
      deleteVolume ${volume}
      echoYellow \\n"Restoring ${volume} from ${archive} ..."
      docker run \
        --rm \
        --name von-network-restore \
        --user root \
        -v ${archiveVolumeMount}:/${ROOT_BACKUP_DIR} \
        -v ${volume}:/target_volume von-network-base \
        tar --same-owner -xvpf ${archiveToRestore} -C /target_volume/ ${tarOptions}
    else
      echo -e \\n"Restore aborted."
    fi
  )
}

function debugVolume()
{
  (
    volume=${1}
    volumeMountFolder=${2:-/debug_volume}
    if [ -z ${volume} ]; then
      echoYellow "You must supply the name of the volume to attach to the debug session."
      exit 1
    fi

    backupVolumeMount=$(getVolumeMount ./${ROOT_BACKUP_DIR})/

    echo -e "\nOpening a debug session with the following volume mounts:\n  - '${volume}':'${volumeMountFolder}'\n  - '${backupVolumeMount}':'/${ROOT_BACKUP_DIR}'\n"
    docker run \
      --rm \
      -it \
      --network="host" \
      --user root \
      -v ${backupVolumeMount}:/${ROOT_BACKUP_DIR} \
      -v ${volume}:${volumeMountFolder} \
      --entrypoint ${SHELL_CMD} \
      von-network-base
  )
}

function findBackup(){
  (
    _fileName=${1}

    # If no backup file was specified, find the most recent set.
    # Otherwise treat the value provided as a filter to find the most recent backup set matching the filter.
    if [ -z "${_fileName}" ]; then
      _fileName=$(find ${ROOT_BACKUP_DIR}* -type f -printf '%T@ %p\n' | grep .tar.gz | sort | tail -n 1 | sed 's~^.* \(.*$\)~\1~')
    else
      _fileName=$(find ${ROOT_BACKUP_DIR}* -type f -printf '%T@ %p\n' | grep .tar.gz | grep ${_fileName} | sort | tail -n 1 | sed 's~^.* \(.*$\)~\1~')
    fi

    echo "${_fileName}"
  )
}

# OSX ships with an old version of Bash that does not support the \e escape character.
function echoYellow (){
  (
  _msg=${1}
  _yellow='\x1B[33m'
  _nc='\x1B[0m' # No Color
  echo -e "${_yellow}${_msg}${_nc}" >&2
  )
}

function promptForConfirmation(){
  (
    _msg=${@}
    echoYellow "\n${_msg}"
    read -n1 -s -r -p $'\x1B[33mWould you like to continue?\x1B[0m  Press \'Y\' to continue, or any other key to exit ...\n' key
    if [[ $(toLower ${key}) == 'y' ]]; then
      return 0
    else
      return 1
    fi
  )
}

function deleteVolume() {
  (
    volume=${1}

    echoYellow \\n"Deleting volume '${volume}' ..."
    containerId=$(docker volume rm ${volume} 2>&1 >/dev/null | sed -e 's~.*\[\(.*\)\]~\1~' | grep -v ${volume})
    if [ ! -z "${containerId}" ]; then
      # The volume is in use by a container.  Remove the container before deleting the volume.
      docker stop ${containerId} > /dev/null 2>&1
      docker rm ${containerId} > /dev/null 2>&1
      docker volume rm ${volume} > /dev/null 2>&1
    fi
  )
}

function deleteVolumes() {
  (
    _projectName=${COMPOSE_PROJECT_NAME:-docker}

    echoYellow \\n"Stopping and removing any running containers ..."
    ${dockerCompose} down -v

    _pattern="^${_projectName}_\|^docker_"
    _volumes=$(docker volume ls -q | grep ${_pattern})
    if [ ! -z "${_volumes}" ]; then
      echoYellow "Removing project volumes ..."
      echo ${_volumes} | xargs docker volume rm
    fi
  )
}

function getVolumeMount() {
  path=${1}
  path=$(realpath ${path})
  path=${path%%+(/)}
  if [[ "$OSTYPE" == "msys" ]]; then
    # When running on Windows, you need to prefix the path with an extra '/'
    path="/${path}"
  fi
  echo ${path}
}

function checkFolderPermissions() {
  # Create the tmp folder if it does not exist ...
  if [ ! -d "${TMP_FOLDER}" ]; then
    echo "Creating ${TMP_FOLDER} ..."
    mkdir -p tmp
  fi

  # Ensure folder permissions are set correctly for use inside the docker container ...
  setFolderReadWriteAll "${TMP_FOLDER}"
  setFolderReadWriteAll "${CLI_SCRIPTS_FOLDER}"
}

function setFolderReadWriteAll() {
  # This has no impact on Windows.  The chmod command will run but the permissions don't actually change.
  if [[ "$OSTYPE" != "msys" ]]; then
    folder=${1}
    permissions=$(stat ${STAT_OPS} ${folder})

    if [[ "${permissions:0-1}" != 7 ]]; then
      echo "Setting ${folder} to read/write for all users ..."
      chmod a+rws ${folder}
    fi
  fi
}

function isUsingWSL() {
    kernelVersion=$(docker -l error info 2> /dev/null | grep "Kernel Version")
    if [[ "$OSTYPE" == "msys" ]] && [[ "${kernelVersion}" == *'WSL'* ]]; then
      return 0
    else
      return 1
    fi
}

function getContainers() {
      initEnv "$@"
      ${dockerCompose} \
        ps --all | tail -n +2 | awk '{ print $1 }'
}

varDockerFolder="/var/lib/docker"
varDockerFolderMsys="//wsl$/docker-desktop-data/version-pack-data/community/docker"
function truncateLogs() {

  if [[ "$OSTYPE" == "msys" ]] && ! isUsingWSL ; then
    echoYellow "Currently, the truncateLogs function is only supported in Windows when using Docker on WSL"
    exit 1
  fi

  containers=$(getContainers)
  for container in ${containers}; do
    log=$(docker inspect -f '{{.LogPath}}' ${container} 2> /dev/null)

    if [[ "$OSTYPE" == "msys" ]]; then
      log="${log/${varDockerFolder}/${varDockerFolderMsys}}"
    fi

    if [ -f "${log}" ]; then
      echo "Truncating log for ${container}: ${log}"
      truncate -s 0 ${log}
    else
      echoYellow "Unable to locate log for ${container}: ${log}"
    fi
  done
}
# ===========================================================

pushd "${SCRIPT_HOME}" >/dev/null
checkFolderPermissions
COMMAND=$(toLower ${1})
shift || COMMAND=usage

case "${COMMAND}" in
  start|up)
      initEnv "$@"
      export LEDGER_SEED=${LEDGER_SEED:-000000000000000000000000Trustee1}
      install_taa
      ${dockerCompose} \
        up \
        -d webserver node1 node2 node3 node4 pqc-sidecarproxy-webserver
      wait_for_ledger
      logs
      echo 'Want to see the scrolling container logs? Run "./manage logs"'
    ;;
  start-combined)
      initEnv "$@"
      install_taa
      ${dockerCompose} \
        up \
        -d webserver nodes pqc-sidecarproxy-webserver
      wait_for_ledger
      logs
    ;;
  start-web)
      initEnv "$@"
      ${dockerCompose} \
        up \
        -d webserver pqc-sidecarproxy-webserver
      wait_for_ledger
      logs webserver
    ;;
  synctest)
      initEnv "$@"
      ${dockerCompose} \
        up \
        -d synctest node1 node2 node3 node4
      logs -f synctest
    ;;
  cli)
      runCliCommand $@
    ;;
  indy-cli)
      runCliCommand indy-cli $@
    ;;
  logs)
      initEnv "$@"
      logs -f "$@"
    ;;
  stop)
      initEnv "$@"
      ${dockerCompose} \
        stop
      remove_taa
    ;;
  down|rm)
      initEnv "$@"
      deleteVolumes
      remove_taa
    ;;
  build)
      docker build $(initDockerBuildArgs) -t von-network-base .
    ;;
  rebuild)
      docker build --no-cache --progress plain $(initDockerBuildArgs) -t von-network-base .
    ;;
  dockerhost)
      echo -e \\n"DockerHost: ${DOCKERHOST}"\\n
    ;;
  generatesecrets)
      generateSecrets
    ;;
  generatedid)
      generateDid $@
    ;;
  generategenesisfiles)
    trustee_csv="${1}"
    steward_csv="${2}"
    generateGenesisFiles "${trustee_csv}" "${steward_csv}"
    ;;
  apply-taa)
      apply-taa $@
    ;;

  backup)
      backup "$@"
    ;;
  restore)
      restore $@
    ;;
  restorearchive)
      archive=${1}
      volume=${2}
      tarOptions=${3}
      restoreArchive ${archive} ${volume} ${tarOptions}
    ;;
  debugvolume)
      volume=${1}
      volumeMountFolder=${2}
      debugVolume ${volume} ${volumeMountFolder}
    ;;
  truncatelogs)
    isUsingWSL
    truncateLogs
    ;;
  *)
      usage;;
esac

popd >/dev/null
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:indy-tails-server-manage-script}
\begin{lstlisting}[language=bash, caption={Indy-Tails-Server Manage Skript}, numbers=left, frame=single]
#!/bin/bash

# ===========================================================
# Check Docker Compose
# ===========================================================

# Default to deprecated V1 'docker-compose'.
dockerCompose="docker-compose --log-level ERROR"

# Prefer 'docker compose' V2 if available
if [[ $(docker compose version 2> /dev/null) == 'Docker Compose'* ]]; then
  dockerCompose="docker --log-level error compose"
fi
export MSYS_NO_PATHCONV=1
# getDockerHost; for details refer to https://github.com/bcgov/DITP-DevOps/tree/main/code/snippets#getdockerhost
. /dev/stdin <<<"$(cat <(curl -s --raw https://raw.githubusercontent.com/bcgov/DITP-DevOps/main/code/snippets/getDockerHost))"
export DOCKERHOST=$(getDockerHost)
set -e

SCRIPT_HOME="$(cd "$(dirname "$0")" && pwd)"

# ===========================================================
# Usage:
# ===========================================================
usage() {
  cat <<-EOF
      Usage: $0 [command] [options]
      Commands:
      build       - Build the tails-server docker images
      start | up  - Run tails-server
      logs        - To tail the logs of running containers (ctrl-c to exit).
      stop | down - Stop tails-server
      rm          - Stop tails-server and remove volumes
EOF
  exit 1
}

toLower() {
  echo $(echo ${@} | tr '[:upper:]' '[:lower:]')
}

exportEnvironment() {
  for arg in "$@"; do
    case "$arg" in
      *=*)
        export "${arg}"
        ;;
      --logs)
        TAIL_LOGS=1
        ;;
      *)
        # If not recognized, save it for later procesing ...
        set -- "$@" "$arg"
        ;;
    esac
  done

  export GENESIS_URL=${GENESIS_URL:-http://$DOCKERHOST:9000/genesis}
  export STORAGE_PATH=${STORAGE_PATH:-/tmp/tails-files}
  export LOG_LEVEL=${LOG_LEVEL:-info}
  export LOGGING_CONFIG=${LOGGING_CONFIG:-/tails_server/config/logging-config.yml}
  export TAILS_SERVER_URL=${TAILS_SERVER_URL:-http://$DOCKERHOST:6543}
}

function logs() {
  (
    local OPTIND
    local unset _force
    local unset no_tail
    while getopts ":f-:" FLAG; do
      case $FLAG in
        f ) local _force=1 ;;
        - )
            case ${OPTARG} in
                "no-tail"*) no_tail=1
              ;;
            esac
      esac
    done
    shift $((OPTIND-1))

    log_args=()
    (( no_tail != 1 )) && log_args+=( '-f' )
    if [ ! -z "${TAIL_LOGS}" ] || [ ! -z "${_force}" ]; then
      ${dockerCompose} logs \
         "${log_args[@]}" "$@"
    fi
  )
}

# ===========================================================

pushd "${SCRIPT_HOME}" >/dev/null
COMMAND=$(toLower ${1})
shift || COMMAND=usage

case "${COMMAND}" in
build)
  ${dockerCompose} build $@
  ;;
start|up)
  exportEnvironment "$@"
  ${dockerCompose} up -d tails-server pqc-sidecarproxy-tails-server
  logs
  echo "Run './manage logs' for logs"
  ;;
test)
  exportEnvironment "$@"
  ${dockerCompose} up -d ngrok-tails-server tails-server
  ${dockerCompose} run tester --genesis-url $GENESIS_URL --tails-server-url $TAILS_SERVER_URL
  # ${dockerCompose} down
  ;;
logs)
  ${dockerCompose} logs -f
  ;;
stop)
  ${dockerCompose} stop
  ;;
down|rm)
  ${dockerCompose} down -v
  ;;
*)
  usage
  ;;
esac

popd >/dev/null
\end{lstlisting}

\subsection{Formative Evaluation}

\subsubsection{Eigenkompilation eines PQC-Chromium-Browsers}
\label{sec:Anhang_Eigenkompilation eines Chromium-Browsers mit PQC-Unterstützung}

Zur experimentellen Evaluation von Verfahren der \ac{PQC} im Kontext realer Webbrowser wurde ein Chromium-basierter Browser mit erweiterten TLS-Fähigkeiten selbst kompiliert. Grundlage bildete das von \ac{OQS} bereitgestellte Chromium-Demoprojekt, das eine Integration der \emph{liboqs}-Bibliothek in die TLS-Implementierung von Chromium (BoringSSL) demonstriert und hybride sowie rein PQ-basierte Schlüsselaustausch- und Signaturverfahren bereitstellt \parencite{open-quantum-safe_OqsdemosChromium643ef99297fe8c6ebd3587b5dd238d5e7a457037openquantumsafeoqsdemos_,open-quantum-safe_OqsdemosChromiumREADMELinuxmd643ef99297fe8c6ebd3587b5dd238d5e7a457037openquantumsafeoqsdemos_}. Ziel war es, eine lauffähige Build-Umgebung unter Linux aufzusetzen, den \ac{OQS}-angepassten Quellcode zu beziehen, die notwendigen Abhängigkeiten zu installieren und anschließend ein reproduzierbares Build-Artefakt des Browsers mit PQC-Unterstützung zu erzeugen.

Die Einrichtung der Build-Umgebung erfolgte weitgehend entsprechend der offiziellen Linux-Build-Dokumentation des Chromium-Projekts \parencite{_ChromiumDocsCheckingoutbuildingChromiumLinux_}. Hierzu wurden zunächst die von Chromium bereitgestellten depot\_tools geklont und in den PATH eingebunden, um Werkzeuge wie fetch, gclient und gn verwenden zu können. Anschließend wurde ein separates Arbeitsverzeichnis angelegt und über fetch ein Chromium-Checkout inklusive aller benötigten Abhängigkeiten durchgeführt. Unter einer aktuellen Ubuntu-Linux-Distribution wurden im nächsten Schritt die von Chromium empfohlenen Systemabhängigkeiten installiert, etwa über das Skript \enquote{build/install-build-deps.sh}, welches Compiler, Entwicklungsbibliotheken und Laufzeitbibliotheken für das spätere Linken der Browser-Binärdateien bereitstellt \parencite{_ChromiumDocsCheckingoutbuildingChromiumLinux_}. Nach Abschluss dieser Vorbereitungen wurden mittels \enquote{gclient runhooks} die Chromium-spezifischen Hooks ausgeführt, um zusätzliche Werkzeuge und vorkompilierte Komponenten nachzuladen.

Aufbauend auf dieser Standard-Umgebung wurde der von \ac{OQS} bereitgestellte Chromium-Zweig eingebunden, der Anpassungen an BoringSSL sowie die Einbindung von liboqs enthält \parencite{open-quantum-safe_OqsdemosChromium643ef99297fe8c6ebd3587b5dd238d5e7a457037openquantumsafeoqsdemos_,open-quantum-safe_OqsdemosChromiumREADMELinuxmd643ef99297fe8c6ebd3587b5dd238d5e7a457037openquantumsafeoqsdemos_}. Dazu wurde das entsprechende Repository aus dem \ac{OQS}-Demoprojekt geklont und gemäß der dort beschriebenen Struktur so in die bestehende Chromium-Arbeitsumgebung integriert, dass die PQC-Erweiterungen anstelle der unveränderten Upstream-Kryptografiebibliothek verwendet werden. Zentral war dabei die Übernahme der in der \ac{OQS}-Dokumentation beschriebenen Build-Konfigurationen, insbesondere GN-Argumente, die das Linken gegen liboqs aktivieren und die experimentellen PQ- bzw. Hybrid-Ciphersuites in der TLS-Konfiguration von Chromium einschalten \parencite{open-quantum-safe_OqsdemosChromiumREADMELinuxmd643ef99297fe8c6ebd3587b5dd238d5e7a457037openquantumsafeoqsdemos_}. Diese Konfiguration wurde in einer eigenen Build-Directory, etwa \enquote{out/oqs-Default}, über den Aufruf \enquote{gn gen} mit den projektspezifischen Argumenten erzeugt.

Im Anschluss daran erfolgte der eigentliche Kompiliervorgang des Browsers mit dem von Chromium vorgesehenen Build-Werkzeug autoninja, das die GN-Konfiguration nutzt, um alle notwendigen Targets effizient zu bauen \parencite{_ChromiumDocsCheckingoutbuildingChromiumLinux_}. Durch den Aufruf von \enquote{autoninja -C out/oqs-Default chrome} wurde eine Browser-Binärdatei erzeugt, die die OQS-Erweiterungen in der TLS-Schicht enthält. Der resultierende Browser konnte direkt aus dem Build-Verzeichnis gestartet und gegen PQC-fähige Testserver genutzt werden, um TLS-Verbindungen mit hybriden oder rein PQ-basierten Schlüsselaustauschmechanismen zu etablieren. Die so aufgebaute Umgebung ermöglicht eine kontrollierte experimentelle Analyse der praktischen Auswirkungen von PQC im Browserkontext, etwa hinsichtlich Kompatibilität, Performance und Protokollhandshake, auf Basis eines realen Chromium-Builds mit explizit aktivierter \ac{PQC}.


\subsubsection{Issuer Agent Boot Logs}

\refstepcounter{manualListingCounterA}
\label{lst:Issuer-Agent-Boot-Logs}
\begin{lstlisting}[language=bash, caption={Issuer Agent Boot Logs}, numbers=left, frame=single]
Executing task in folder ferris: docker logs --tail 1000 -f 179e43b336fa16b399efa7326cdc0b8bfa6ab24c8f86a2b4b630fe57fc382064 

2025-11-28 23:49:38,921 acapy_agent.config.default_context INFO Registering default plugins
2025-11-28 23:49:39,083 acapy_agent.config.default_context INFO Registering askar plugins
2025-11-28 23:49:39,308 acapy_agent.config.ledger INFO Fetching genesis transactions from: https://host.docker.internal:8000/genesis
2025-11-28 23:49:46,340 acapy_agent.core.profile INFO Create profile manager: askar
2025-11-28 23:49:46,827 acapy_agent.config.wallet INFO Created new profile - Profile name: issuer_wallet, backend: askar
2025-11-28 23:49:46,829 acapy_agent.config.wallet INFO No public DID created
2025-11-28 23:49:46,885 acapy_agent.config.ledger INFO Ledger configuration complete
2025-11-28 23:49:46,885 acapy_agent.core.conductor INFO Ledger configured successfully.
2025-11-28 23:49:46,893 acapy_agent.core.conductor INFO Wallet type record not found.
2025-11-28 23:49:46,894 acapy_agent.core.conductor INFO New agent. Setting wallet type to askar.
2025-11-28 23:49:47,028 acapy_agent.config.banner INFO 
::::::::::::::::::::::::::::::::::::::::::::::
::               Issuer Agent               ::
::                                          ::
::                                          ::
:: Inbound Transports:                      ::
::                                          ::
::   - http://0.0.0.0:8020                  ::
::                                          ::
:: Outbound Transports:                     ::
::                                          ::
::   - http                                 ::
::   - https                                ::
::                                          ::
:: Administration API:                      ::
::                                          ::
::   - http://0.0.0.0:8021                  ::
::                                          ::
::                               ver: 1.3.2 ::
::::::::::::::::::::::::::::::::::::::::::::::

2025-11-28 23:49:47,028 acapy_agent.config.banner INFO 
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::                             DEPRECATION NOTICE:                                  ::
:: -------------------------------------------------------------------------------- ::
:: Receiving a core DIDComm protocol with the `did:sov:BzCbsNYhMrjHiqZDTUASHg;spec` ::
:: prefix is deprecated. All parties sending this prefix should be notified that    ::
:: support for receiving such messages will be removed in a future release. Use     ::
:: https://didcomm.org/ instead.                                                    ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0160: Connection Protocol is deprecated and support will be removed in ::
:: a future release; use RFC 0023: DID Exchange instead.                            ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0036: Issue Credential 1.0 is deprecated and support will be removed   ::
:: in a future release; use RFC 0453: Issue Credential 2.0 instead.                 ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0037: Present Proof 1.0 is deprecated and support will be removed in a ::
:: future release; use RFC 0454: Present Proof 2.0 instead.                         ::
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

2025-11-28 23:49:47,029 acapy_agent.core.conductor INFO Wallet version storage record not found.
2025-11-28 23:49:47,030 acapy_agent.core.conductor INFO No upgrade from version was found from wallet or via --from-version startup argument. Defaulting to v0.7.5.
2025-11-28 23:49:47,031 acapy_agent.core.conductor INFO Upgrade configurations available. Initiating upgrade.
2025-11-28 23:49:47,033 acapy_agent.commands.upgrade INFO No ACA-Py version found in wallet storage.
2025-11-28 23:49:47,033 acapy_agent.commands.upgrade INFO Selecting v0.7.5 as --from-version from the config.
2025-11-28 23:49:47,033 acapy_agent.commands.upgrade INFO Running upgrade process for v0.8.1
2025-11-28 23:49:47,034 acapy_agent.commands.upgrade INFO No records of <class 'acapy_agent.connections.models.conn_record.ConnRecord'> found
2025-11-28 23:49:47,040 acapy_agent.commands.upgrade INFO acapy_version storage record set to v1.3.2
2025-11-28 23:49:47,042 acapy_agent.core.conductor INFO Listening...
2025-11-28 23:49:52,008 aiohttp.access INFO 127.0.0.1 [28/Nov/2025:23:49:52 +0000] "GET /status/ready HTTP/1.1" 200 138 "-" "curl/7.88.1"
2025-11-28 23:50:22,041 aiohttp.access INFO 127.0.0.1 [28/Nov/2025:23:50:22 +0000] "GET /status/ready HTTP/1.1" 200 138 "-" "curl/7.88.1"
\end{lstlisting}


\newpage
\section{Zweite Iteration der Artefaktentwicklung}
\label{sec:Anhang_Artefaktentwicklung Iteration 2}


\subsection{ACA-Py-Plugin-Quellcode (pqc\_didpeer4\_fm)}
\label{sec:Anhang_ACA-Py-Plugin-Quellcode-pqc_didpeer4_fm}

\subsubsection{liboqs\_wrapper.py}

\refstepcounter{manualListingCounterA}
\label{lst:liboqs_wrapper.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - liboqs\_wrapper.py}, numbers=left, frame=single]
"""Direct liboqs-python integration for PQC key generation and operations.

This module provides a wrapper around liboqs-python for generating and using
ML-DSA-65 (Dilithium3) and ML-KEM-768 (Kyber768) keys.
"""

import logging
from typing import Tuple, Optional

try:
    import oqs
except ImportError:
    raise ImportError(
        "liboqs-python is required for PQC support. "
        "Install with: pip install liboqs-python>=0.10.0"
    )

LOGGER = logging.getLogger(__name__)


class LibOQSWrapper:
    """Wrapper for liboqs-python providing PQC key generation and operations."""

    def __init__(self):
        """Initialize LibOQS wrapper."""
        self.oqs = oqs
        LOGGER.info("LibOQS wrapper initialized")

    def generate_ml_dsa_65_keypair(self) -> Tuple[bytes, bytes]:
        """Generate ML-DSA-65 (Dilithium3) keypair.

        Returns:
            Tuple[bytes, bytes]: (public_key, secret_key)
        """
        try:
            sig = self.oqs.Signature("Dilithium3")
            public_key = sig.generate_keypair()
            secret_key = sig.export_secret_key()
            LOGGER.debug(
                f"Generated ML-DSA-65 keypair: "
                f"public_key={len(public_key)} bytes, "
                f"secret_key={len(secret_key)} bytes"
            )
            return public_key, secret_key
        except Exception as e:
            LOGGER.error(f"Failed to generate ML-DSA-65 keypair: {e}")
            raise

    def generate_ml_kem_768_keypair(self) -> Tuple[bytes, bytes]:
        """Generate ML-KEM-768 (Kyber768) keypair.

        Returns:
            Tuple[bytes, bytes]: (public_key, secret_key)
        """
        try:
            # Use KeyEncapsulation (not KEM) - correct liboqs-python API
            kem = self.oqs.KeyEncapsulation("ML-KEM-768")
            public_key = kem.generate_keypair()
            secret_key = kem.export_secret_key()
            LOGGER.debug(
                f"Generated ML-KEM-768 keypair: "
                f"public_key={len(public_key)} bytes, "
                f"secret_key={len(secret_key)} bytes"
            )
            return public_key, secret_key
        except Exception as e:
            LOGGER.error(f"Failed to generate ML-KEM-768 keypair: {e}")
            raise

    def sign_ml_dsa_65(self, message: bytes, secret_key: bytes) -> bytes:
        """Sign a message using ML-DSA-65 (Dilithium3).

        Args:
            message: Message to sign
            secret_key: ML-DSA-65 secret key

        Returns:
            bytes: Signature
        """
        try:
            sig = self.oqs.Signature("Dilithium3", secret_key)
            signature = sig.sign(message)
            LOGGER.debug(f"Created ML-DSA-65 signature: {len(signature)} bytes")
            return signature
        except Exception as e:
            LOGGER.error(f"Failed to sign with ML-DSA-65: {e}")
            raise

    def verify_ml_dsa_65(
        self, message: bytes, signature: bytes, public_key: bytes
    ) -> bool:
        """Verify an ML-DSA-65 (Dilithium3) signature.

        Args:
            message: Original message
            signature: Signature to verify
            public_key: ML-DSA-65 public key

        Returns:
            bool: True if signature is valid, False otherwise
        """
        try:
            sig = self.oqs.Signature("Dilithium3")
            is_valid = sig.verify(message, signature, public_key)
            LOGGER.debug(f"ML-DSA-65 signature verification: {is_valid}")
            return is_valid
        except Exception as e:
            LOGGER.error(f"Failed to verify ML-DSA-65 signature: {e}")
            return False

    def encapsulate_ml_kem_768(self, public_key: bytes) -> Tuple[bytes, bytes]:
        """Encapsulate a shared secret using ML-KEM-768 (Kyber768).

        Args:
            public_key: ML-KEM-768 public key

        Returns:
            Tuple[bytes, bytes]: (ciphertext, shared_secret)
        """
        try:
            kem = self.oqs.KeyEncapsulation("ML-KEM-768")
            ciphertext, shared_secret = kem.encap_secret(public_key)
            LOGGER.debug(
                f"ML-KEM-768 encapsulation: "
                f"ciphertext={len(ciphertext)} bytes, "
                f"shared_secret={len(shared_secret)} bytes"
            )
            return ciphertext, shared_secret
        except Exception as e:
            LOGGER.error(f"Failed to encapsulate with ML-KEM-768: {e}")
            raise

    def decapsulate_ml_kem_768(self, ciphertext: bytes, secret_key: bytes) -> bytes:
        """Decapsulate a shared secret using ML-KEM-768 (Kyber768).

        Args:
            ciphertext: Ciphertext from encapsulation
            secret_key: ML-KEM-768 secret key

        Returns:
            bytes: Shared secret
        """
        try:
            kem = self.oqs.KeyEncapsulation("ML-KEM-768", secret_key)
            shared_secret = kem.decap_secret(ciphertext)
            LOGGER.debug(f"ML-KEM-768 decapsulation: {len(shared_secret)} bytes")
            return shared_secret
        except Exception as e:
            LOGGER.error(f"Failed to decapsulate with ML-KEM-768: {e}")
            raise

    # Convenience aliases for pqc_didcomm_v1.py compatibility
    def kem_encapsulate(self, public_key: bytes) -> Tuple[bytes, bytes]:
        """Alias for encapsulate_ml_kem_768()."""
        return self.encapsulate_ml_kem_768(public_key)

    def kem_decapsulate(self, secret_key: bytes, ciphertext: bytes) -> bytes:
        """Alias for decapsulate_ml_kem_768()."""
        return self.decapsulate_ml_kem_768(ciphertext, secret_key)

    def ml_dsa_sign(self, secret_key: bytes, message: bytes) -> bytes:
        """Alias for sign_ml_dsa_65()."""
        return self.sign_ml_dsa_65(message, secret_key)

    def ml_dsa_verify(self, public_key: bytes, message: bytes, signature: bytes) -> bool:
        """Alias for verify_ml_dsa_65()."""
        return self.verify_ml_dsa_65(message, signature, public_key)

    @staticmethod
    def get_supported_algorithms() -> dict:
        """Get information about supported PQC algorithms.

        Returns:
            dict: Supported algorithms with their properties
        """
        return {
            "signature": {
                "ml-dsa-65": {
                    "oqs_name": "Dilithium3",
                    "public_key_size": 1952,  # bytes
                    "secret_key_size": 4000,  # bytes
                    "signature_size": 3293,  # bytes
                    "security_level": "NIST Level 3",
                },
            },
            "kem": {
                "ml-kem-768": {
                    "oqs_name": "Kyber768",
                    "public_key_size": 1184,  # bytes
                    "secret_key_size": 2400,  # bytes
                    "ciphertext_size": 1088,  # bytes
                    "shared_secret_size": 32,  # bytes
                    "security_level": "NIST Level 3",
                },
            },
        }


# Global instance
_liboqs_instance: Optional[LibOQSWrapper] = None


def get_liboqs() -> LibOQSWrapper:
    """Get or create the global LibOQS wrapper instance.

    Returns:
        LibOQSWrapper: Global LibOQS wrapper instance
    """
    global _liboqs_instance
    if _liboqs_instance is None:
        _liboqs_instance = LibOQSWrapper()
    return _liboqs_instance
\end{lstlisting}

\subsubsection{pqc\_peer4\_creator.py}

\refstepcounter{manualListingCounterA}
\label{lst:pqc_peer4_creator.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - pqc\_peer4\_creator.py}, numbers=left, frame=single]
"""Create did:peer:4 with ML-DSA-65 + ML-KEM-768."""

from typing import List, Optional, Sequence, Dict
from did_peer_4 import encode
from did_peer_4.input_doc import input_doc_from_keys_and_services, KeySpec

from acapy_agent.wallet.base import BaseWallet
from acapy_agent.wallet.did_info import DIDInfo
from acapy_agent.wallet.did_method import PEER4

from .key_types import ML_DSA_65, ML_KEM_768
from .pqc_multikey import key_info_to_multikey


async def create_pqc_peer4_did(
    wallet: BaseWallet,
    svc_endpoints: Optional[Sequence[str]] = None,
    routing_keys: Optional[List[str]] = None,
    metadata: Optional[Dict] = None,
) -> DIDInfo:
    """Create did:peer:4 with ML-DSA-65 (signature) + ML-KEM-768 (key agreement).

    This function creates a PQC-enabled did:peer:4 DID with two keys:
    1. ML-DSA-65 for authentication and assertion (digital signatures)
    2. ML-KEM-768 for key agreement (encryption/key encapsulation)

    Args:
        wallet: Wallet instance for key management
        svc_endpoints: Service endpoints for DIDComm messaging
        routing_keys: Routing keys for mediation (optional)
        metadata: Additional metadata to store with DID

    Returns:
        DIDInfo compatible with GET /wallet/did response format

    Example:
        >>> did_info = await create_pqc_peer4_did(
        ...     wallet=wallet,
        ...     svc_endpoints=["https://agent.example.com:8020"]
        ... )
        >>> print(did_info.did)
        did:peer:4:z6MNxxx...
    """

    # 1. Create ML-DSA-65 key (signature/authentication)
    sig_key = await wallet.create_key(ML_DSA_65)

    # 2. Create ML-KEM-768 key (key agreement/encryption)
    kem_key = await wallet.create_key(ML_KEM_768)

    # 3. Convert to multikeys
    sig_multikey = key_info_to_multikey(sig_key)  # --> z6MN... (ML-DSA-65)
    kem_multikey = key_info_to_multikey(kem_key)  # --> z6MK768... (ML-KEM-768)

    # 4. Create KeySpec objects for did:peer:4
    # NOTE: Order matters! did-peer-4 numbers from 0: key_specs[0] --> #key-0, key_specs[1] --> #key-1
    key_specs = [
        KeySpec(
            multikey=sig_multikey,
            relationships=["authentication", "assertionMethod"]  # --> #key-0
        ),
        KeySpec(
            multikey=kem_multikey,
            relationships=["keyAgreement"]  # --> #key-1 (used in recipientKeys!)
        ),
    ]

    # 5. Build DIDComm v1 services (compatible with DID Exchange 1.1)
    services = []
    for index, endpoint in enumerate(svc_endpoints or []):
        services.append({
            "id": f"#didcomm-{index}",
            "type": "did-communication",
            "recipientKeys": ["#key-1"],  # ML-KEM-768 (key agreement/encryption, key_specs[1] --> #key-1)
            "routingKeys": routing_keys or [],
            "serviceEndpoint": endpoint,
            "priority": index,
        })

    # 6. Generate did:peer:4 (long form)
    input_doc = input_doc_from_keys_and_services(
        keys=key_specs,
        services=services
    )
    did = encode(input_doc)

    # 7. Create metadata (compatible with existing /wallet/did format)
    did_metadata = metadata or {}
    did_metadata.update({
        "pqc_enabled": True,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": f"{did}#key-1",  # KEM key is key_specs[1] --> #key-1
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0",
    })

    # 8. Create DIDInfo (SAME structure as original did:peer:4!)
    did_info = DIDInfo(
        did=did,
        method=PEER4,
        verkey=sig_key.verkey,
        metadata=did_metadata,
        key_type=ML_DSA_65,
    )

    # 9. Store DID in wallet
    await wallet.store_did(did_info)

    # 10. Assign Key IDs - did-peer-4 numbers from 0!
    await wallet.assign_kid_to_key(sig_key.verkey, f"{did}#key-0")  # key_specs[0] --> #key-0
    await wallet.assign_kid_to_key(kem_key.verkey, f"{did}#key-1")  # key_specs[1] --> #key-1

    return did_info
\end{lstlisting}

\subsubsection{pqc\_peer4\_resolver.py}

\refstepcounter{manualListingCounterA}
\label{lst:pqc_peer4_resolver.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - pqc\_peer4\_resolver.py}, numbers=left, frame=single]
"""PQC-aware did:peer:4 resolver."""

from re import compile
from typing import Optional, Pattern, Sequence, Text

from did_peer_4 import LONG_PATTERN, SHORT_PATTERN, long_to_short, resolve, resolve_short

from acapy_agent.config.injection_context import InjectionContext
from acapy_agent.core.profile import Profile
from acapy_agent.resolver.base import BaseDIDResolver, DIDNotFound, ResolverType
from acapy_agent.storage.base import BaseStorage
from acapy_agent.storage.error import StorageNotFoundError
from acapy_agent.storage.record import StorageRecord


class PQCPeer4Resolver(BaseDIDResolver):
    """Resolver for PQC-enabled did:peer:4 DIDs.

    Uses external did-peer-4 library for resolution.
    PQC multikeys (ML-DSA-65, ML-KEM-768) are preserved in verification methods.

    This resolver is registered automatically by the pqc_didpeer4_fm plugin
    and handles all did:peer:4 DIDs, whether they use PQC or classical algorithms.
    """

    RECORD_TYPE = "long_peer_did_4_doc"

    def __init__(self):
        """Initialize PQC Peer4 Resolver."""
        super().__init__(ResolverType.NATIVE)

    async def setup(self, context: InjectionContext):
        """Setup resolver (no initialization needed)."""

    @property
    def supported_did_regex(self) -> Pattern:
        """Return supported_did_regex for did:peer:4."""
        return compile(f"{LONG_PATTERN.pattern}|{SHORT_PATTERN.pattern}")

    async def _resolve(
        self,
        profile: Profile,
        did: str,
        service_accept: Optional[Sequence[Text]] = None,
    ) -> dict:
        """Resolve did:peer:4 DID document.

        Args:
            profile: Profile context
            did: The did:peer:4 DID to resolve
            service_accept: Service types to accept (optional)

        Returns:
            DID Document as dict

        Raises:
            DIDNotFound: If DID resolution fails
        """
        if LONG_PATTERN.match(did):
            short_did_peer_4 = long_to_short(did)
            # resolve and save long form
            async with profile.session() as session:
                storage = session.inject(BaseStorage)
                try:
                    record = await storage.get_record(self.RECORD_TYPE, short_did_peer_4)
                except StorageNotFoundError:
                    record = StorageRecord(self.RECORD_TYPE, did, {}, short_did_peer_4)
                    await storage.add_record(record)
            document = resolve(did)

        elif SHORT_PATTERN.match(did):
            async with profile.session() as session:
                storage = session.inject(BaseStorage)
                try:
                    record = await storage.get_record(self.RECORD_TYPE, did)
                except StorageNotFoundError:
                    raise DIDNotFound(
                        f"short did:peer:4 does not correspond to a known long did:peer:4: {did}"
                    )
            document = resolve_short(record.value)
        else:
            raise ValueError(f"{did} did not match long or short form of did:peer:4")

        return document
\end{lstlisting}

\pagebreak

\subsubsection{pqc\_multicodec.py}

\refstepcounter{manualListingCounterA}
\label{lst:pqc_multicodec.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - pqc\_multicodec.py}, numbers=left, frame=single]
"""PQC Multicodec registry for ML-DSA and ML-KEM algorithms.

This module provides a standalone multicodec registry for PQC algorithms,
independent of ACA-Py's built-in multicodec system which uses a fixed Enum.
"""

# PQC Multicodec prefixes (provisional - based on W3C draft)
# https://w3c-ccg.github.io/multicodec/
PQC_MULTICODECS = {
    # ML-DSA (NIST FIPS-204) - Digital Signature Algorithm
    "ml-dsa-44-pub": b"\xd0\x44",
    "ml-dsa-65-pub": b"\xd0\x65",  # Primary signature algorithm
    "ml-dsa-87-pub": b"\xd0\x87",

    # ML-KEM (NIST FIPS-203) - Key Encapsulation Mechanism
    "ml-kem-512-pub": b"\xe0\x12",
    "ml-kem-768-pub": b"\xe0\x18",  # Primary key agreement algorithm
    "ml-kem-1024-pub": b"\xe0\x24",
}


def register_pqc_multicodecs():
    """Register PQC multicodecs.

    This is a no-op since the registry is already defined as a module-level
    dictionary. The function exists for API compatibility with the plugin setup.
    """
    pass


def wrap_pqc(codec_name: str, data: bytes) -> bytes:
    """Wrap data with PQC multicodec prefix.

    Args:
        codec_name: Multicodec name (e.g., "ml-dsa-65-pub")
        data: Raw key bytes to wrap

    Returns:
        Multicodec-prefixed bytes

    Raises:
        ValueError: If codec_name is not a known PQC codec
    """
    if codec_name not in PQC_MULTICODECS:
        raise ValueError(
            f"Unknown PQC codec: {codec_name}. "
            f"Supported: {list(PQC_MULTICODECS.keys())}"
        )
    return PQC_MULTICODECS[codec_name] + data


def unwrap_pqc(data: bytes) -> tuple[str, bytes]:
    """Unwrap PQC multicodec prefix from data.

    Args:
        data: Multicodec-prefixed key bytes

    Returns:
        Tuple of (codec_name, raw_key_bytes)

    Raises:
        ValueError: If data doesn't start with a known PQC multicodec prefix
    """
    for codec_name, prefix in PQC_MULTICODECS.items():
        if data.startswith(prefix):
            return codec_name, data[len(prefix):]

    raise ValueError(
        f"Unknown PQC multicodec prefix. "
        f"Data starts with: {data[:2].hex() if len(data) >= 2 else 'empty'}"
    )
\end{lstlisting}

\subsubsection{pqc\_multikey.py}

\refstepcounter{manualListingCounterA}
\label{lst:pqc_multikey.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - pqc\_multikey.py}, numbers=left, frame=single]
"""Convert PQC KeyInfo to Multikey format for did:peer:4."""

from base58 import b58decode
from multiformats import multibase
from acapy_agent.wallet.did_info import KeyInfo

from .key_types import ML_DSA_65, ML_KEM_768
from .pqc_multicodec import wrap_pqc, unwrap_pqc


# Mapping: KeyType --> Multicodec name
KEY_TYPE_TO_MULTICODEC = {
    ML_DSA_65.key_type: "ml-dsa-65-pub",
    ML_KEM_768.key_type: "ml-kem-768-pub",
}


def key_info_to_multikey(key_info: KeyInfo) -> str:
    """Convert PQC KeyInfo to multikey string.

    Args:
        key_info: KeyInfo with ML-DSA-65 or ML-KEM-768 key

    Returns:
        Multikey string (e.g., "z6MNx8r2..." for ML-DSA-65)

    Raises:
        ValueError: If key type is not supported

    Example:
        >>> key = await wallet.create_key(ML_DSA_65)
        >>> multikey = key_info_to_multikey(key)
        >>> print(multikey)
        z6MNxxx...  # ML-DSA-65 multikey (base58btc)
    """
    # Get multicodec name for this key type
    codec_name = KEY_TYPE_TO_MULTICODEC.get(key_info.key_type.key_type)
    if not codec_name:
        raise ValueError(
            f"Unsupported key type for PQC multikey: {key_info.key_type.key_type}. "
            f"Supported: {list(KEY_TYPE_TO_MULTICODEC.keys())}"
        )

    # Decode base58 verkey --> raw bytes
    raw_key = b58decode(key_info.verkey)

    # Wrap with PQC multicodec prefix
    multicodec_key = wrap_pqc(codec_name, raw_key)

    # Encode with multibase (base58btc --> starts with 'z')
    multikey = multibase.encode(multicodec_key, "base58btc")

    return multikey


def multikey_to_raw(multikey: str) -> tuple:
    """Decode multikey to (codec_name, raw_bytes).

    Used for verification and key recovery.

    Args:
        multikey: Multikey string (e.g., "z6MNxxx...")

    Returns:
        Tuple of (codec_name, raw_key_bytes)

    Example:
        >>> codec, raw = multikey_to_raw("z6MNxxx...")
        >>> print(codec)
        ml-dsa-65-pub
    """
    # Decode multibase
    decoded = multibase.decode(multikey)

    # Unwrap PQC multicodec
    codec_name, raw_key = unwrap_pqc(decoded)

    return codec_name, raw_key
\end{lstlisting}

\pagebreak

\subsubsection{pqc\_didcomm\_v1.py}

\refstepcounter{manualListingCounterA}
\label{lst:pqc_didcomm_v1.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - pqc\_didcomm\_v1.py}, numbers=left, frame=single]
"""PQC-aware DIDComm v1 envelope handling.

This module provides Post-Quantum Cryptography (PQC) support for DIDComm v1
pack/unpack operations while maintaining compatibility with classical ED25519/X25519.

Classical DIDComm v1:
    - Uses ED25519 (signatures) --> X25519 (ECDH) conversion
    - crypto_box (ECDH + XChaCha20-Poly1305) for key exchange
    - Not quantum-safe!

PQC-DIDComm v1:
    - Uses ML-DSA-65 (signatures) for authcrypt
    - ML-KEM-768 (KEM) for CEK encapsulation (replaces ECDH)
    - XChaCha20-Poly1305 for symmetric encryption (quantum-safe!)
    - JWE format: alg = "PQC-Authcrypt" or "PQC-Anoncrypt"

This implementation supports:
    - Pure PQC mode (ML-DSA-65 + ML-KEM-768)
    - Pure classical mode (ED25519 + X25519)
    - Hybrid mode (mixed PQC/classical recipients)
"""

import logging
import json
from collections import OrderedDict
from typing import Optional, Sequence, Tuple, Union

from aries_askar import Key, KeyAlg, Session
from aries_askar.bindings import key_get_secret_bytes

from .liboqs_wrapper import get_liboqs
from .key_types import ML_DSA_65, ML_KEM_768
from .askar_pqc_patch import PQCKey

LOGGER = logging.getLogger(__name__)


def _is_pqc_key(key: Union[Key, PQCKey]) -> bool:
    """Check if key is a PQC key (ML-DSA-65 or ML-KEM-768).

    Args:
        key: Key to check

    Returns:
        bool: True if PQC key, False if classical key
    """
    if isinstance(key, PQCKey):
        return True
    return False


def _detect_recipient_key_type(verkey: str) -> str:
    """Detect if a base58 verkey is PQC or classical based on length.

    ML-KEM-768 public keys: 1184 bytes --> ~1615 base58 chars
    ED25519 public keys: 32 bytes --> 44 base58 chars

    Args:
        verkey: Base58-encoded public key

    Returns:
        str: "pqc" or "classical"
    """
    if len(verkey) > 1000:  # Definitely PQC
        return "pqc"
    else:
        return "classical"


async def pack_message_pqc(
    session: Session,
    to_verkeys: Sequence[str],
    from_key: Optional[Union[Key, PQCKey]],
    message: str,
) -> bytes:
    """Pack a message with PQC support.

    This function automatically detects whether to use PQC or classical
    crypto based on the sender key type and recipient key types.

    Args:
        session: Askar session for key lookups
        to_verkeys: List of recipient verkeys (base58)
        from_key: Sender key (PQCKey or aries_askar.Key) or None for anoncrypt
        message: Message to pack (string or bytes)

    Returns:
        bytes: JWE-encoded packed message
    """
    # Detect crypto mode
    sender_is_pqc = _is_pqc_key(from_key) if from_key else False
    recipient_types = [_detect_recipient_key_type(vk) for vk in to_verkeys]

    # Check if any recipient is PQC
    any_pqc = sender_is_pqc or "pqc" in recipient_types
    all_pqc = sender_is_pqc and all(t == "pqc" for t in recipient_types)

    if any_pqc:
        LOGGER.info(f"Using PQC pack mode: sender_pqc={sender_is_pqc}, "
                   f"recipients={recipient_types}")
        return await _pack_pqc(session, to_verkeys, from_key, message)
    else:
        # Delegate to classical implementation
        LOGGER.debug("Using classical pack mode (ED25519/X25519)")
        from acapy_agent.askar.didcomm.v1 import pack_message as pack_classical

        # Convert message to bytes if needed
        message_bytes = message.encode("utf-8") if isinstance(message, str) else message
        return pack_classical(to_verkeys, from_key, message_bytes)


async def _pack_pqc(
    session: Session,
    to_verkeys: Sequence[str],
    from_key: Optional[Union[Key, PQCKey]],
    message: Union[str, bytes],
) -> bytes:
    """Pack a message using PQC algorithms (ML-KEM-768 + XChaCha20-Poly1305).

    Algorithm:
        1. Generate random CEK (Content Encryption Key) using ChaCha20-Poly1305
        2. For each recipient:
           - Fetch ML-KEM-768 public key
           - Encapsulate CEK using ML-KEM-768
           - Add JWE recipient with encrypted_key
        3. If authcrypt:
           - Sign CEK with ML-DSA-65
           - Add signature to protected header
        4. Encrypt message with CEK using ChaCha20-Poly1305
        5. Return JWE envelope

    Args:
        session: Askar session for key lookups
        to_verkeys: Recipient verkeys (base58)
        from_key: Sender key (PQCKey with ML-DSA-65 or ML-KEM-768) or None
        message: Message to encrypt

    Returns:
        bytes: JWE-encoded message
    """
    from acapy_agent.utils.jwe import JweEnvelope, JweRecipient, b64url
    from acapy_agent.wallet.util import bytes_to_b58, b58_to_bytes

    # Convert message to bytes
    message_bytes = message.encode("utf-8") if isinstance(message, str) else message

    # 1. Generate CEK (Content Encryption Key)
    cek = Key.generate(KeyAlg.C20P)  # ChaCha20-Poly1305
    cek_bytes = key_get_secret_bytes(cek._handle)

    # Get liboqs for KEM operations
    liboqs = get_liboqs()

    # 2. Prepare sender info for authcrypt
    sender_vk = None
    if from_key:
        if isinstance(from_key, PQCKey):
            sender_vk = bytes_to_b58(from_key.get_public_bytes()).encode("utf-8")
        else:
            sender_vk = bytes_to_b58(from_key.get_public_bytes()).encode("utf-8")

    # 3. Create JWE wrapper
    wrapper = JweEnvelope(with_protected_recipients=True, with_flatten_recipients=False)

    # 4. Encapsulate CEK for each recipient
    for target_vk in to_verkeys:
        # Decode recipient's public key from base58
        # Note: Unlike sender keys, recipient keys are NOT stored in our wallet!
        # The verkey is passed as a base58-encoded public key that we decode directly.

        # Detect key type based on length (same as pack_message_pqc)
        recipient_type = _detect_recipient_key_type(target_vk)

        if recipient_type == "pqc":
            # PQC path: Decode ML-KEM-768 public key from base58
            LOGGER.debug(f"Decoding ML-KEM-768 recipient key: {target_vk[:20]}...")
            target_public = b58_to_bytes(target_vk)

            if sender_vk:
                # Authcrypt mode: Encapsulate CEK + sign with sender
                ciphertext, shared_secret = liboqs.kem_encapsulate(target_public)

                # For authcrypt, we need to sign the ciphertext with sender's ML-DSA-65
                if isinstance(from_key, PQCKey) and from_key.algorithm == "ml-dsa-65":
                    signature = liboqs.ml_dsa_sign(
                        from_key.get_secret_bytes(),
                        ciphertext
                    )

                    wrapper.add_recipient(
                        JweRecipient(
                            encrypted_key=ciphertext,
                            header=OrderedDict([
                                ("kid", target_vk),
                                ("sender", b64url(sender_vk)),
                                ("sig", b64url(signature)),
                            ])
                        )
                    )
                else:
                    # Sender is not ML-DSA-65, can't do PQC authcrypt
                    raise ValueError("Authcrypt requires sender with ML-DSA-65 key")

                # Use shared_secret as CEK (for simplicity, in production use KDF)
                # Note: This is a simplified implementation!
                cek_bytes = shared_secret[:32]  # ChaCha20 needs 256-bit key
                cek = Key.from_secret_bytes(KeyAlg.C20P, cek_bytes)  # Recreate CEK from shared_secret!

            else:
                # Anoncrypt mode: Just encapsulate CEK
                ciphertext, shared_secret = liboqs.kem_encapsulate(target_public)

                wrapper.add_recipient(
                    JweRecipient(
                        encrypted_key=ciphertext,
                        header={"kid": target_vk}
                    )
                )

                # Use shared_secret as CEK
                cek_bytes = shared_secret[:32]
                cek = Key.from_secret_bytes(KeyAlg.C20P, cek_bytes)  # Recreate CEK from shared_secret!

        else:
            # Classical key (ED25519) - decode from base58 and convert to X25519
            LOGGER.debug(f"Decoding classical ED25519 recipient key: {target_vk[:20]}...")

            # Decode ED25519 public key from base58
            ed_public_bytes = b58_to_bytes(target_vk)

            # Convert ED25519 --> X25519 for ECDH
            from aries_askar import crypto_box
            target_xk = Key.from_public_bytes(KeyAlg.ED25519, ed_public_bytes).convert_key(KeyAlg.X25519)

            if sender_vk:
                # Classical authcrypt
                if isinstance(from_key, Key):
                    sender_xk = from_key.convert_key(KeyAlg.X25519)
                    enc_sender = crypto_box.crypto_box_seal(target_xk, sender_vk)
                    nonce = crypto_box.random_nonce()
                    enc_cek = crypto_box.crypto_box(target_xk, sender_xk, cek_bytes, nonce)

                    wrapper.add_recipient(
                        JweRecipient(
                            encrypted_key=enc_cek,
                            header=OrderedDict([
                                ("kid", target_vk),
                                ("sender", b64url(enc_sender)),
                                ("iv", b64url(nonce)),
                            ])
                        )
                    )
                else:
                    raise ValueError("Classical authcrypt requires classical sender key")
            else:
                # Classical anoncrypt
                enc_cek = crypto_box.crypto_box_seal(target_xk, cek_bytes)
                wrapper.add_recipient(
                    JweRecipient(
                        encrypted_key=enc_cek,
                        header={"kid": target_vk}
                    )
                )

    # 5. Set protected header
    alg = "PQC-Authcrypt" if from_key else "PQC-Anoncrypt"
    wrapper.set_protected(
        OrderedDict([
            ("enc", "xchacha20poly1305_ietf"),
            ("typ", "JWM/1.0"),
            ("alg", alg),
        ])
    )

    # 6. Encrypt message with CEK
    enc = cek.aead_encrypt(message_bytes, aad=wrapper.protected_bytes)
    ciphertext, tag, nonce = enc.parts
    wrapper.set_payload(ciphertext, nonce, tag)

    # 7. Serialize to JSON
    return wrapper.to_json().encode("utf-8")


async def unpack_message_pqc(
    session: Session,
    enc_message: bytes
) -> Tuple[str, str, str]:
    """Unpack a message with PQC support.

    This function automatically detects whether the message was packed with
    PQC or classical crypto based on the "alg" field in the JWE protected header.

    Args:
        session: Askar session for key lookups
        enc_message: JWE-encoded message

    Returns:
        Tuple[str, str, str]: (message, sender_verkey, recipient_verkey)
    """
    from acapy_agent.utils.jwe import JweEnvelope
    from acapy_agent.wallet.base import WalletError
    from marshmallow import ValidationError

    try:
        wrapper = JweEnvelope.from_json(enc_message)
    except ValidationError:
        raise WalletError("Invalid packed message")

    alg = wrapper.protected.get("alg")

    # Detect PQC mode
    if alg in ("PQC-Authcrypt", "PQC-Anoncrypt"):
        LOGGER.info(f"Using PQC unpack mode: alg={alg}")
        return await _unpack_pqc(session, wrapper)
    elif alg in ("Authcrypt", "Anoncrypt"):
        # Delegate to classical implementation
        LOGGER.debug(f"Using classical unpack mode: alg={alg}")
        from acapy_agent.askar.didcomm.v1 import unpack_message as unpack_classical
        return await unpack_classical(session, enc_message)
    else:
        raise WalletError(f"Unsupported pack algorithm: {alg}")


async def _unpack_pqc(
    session: Session,
    wrapper
) -> Tuple[str, str, str]:
    """Unpack a PQC-encrypted message.

    Algorithm:
        1. Extract recipients from JWE
        2. Try each recipient until we find one we can decrypt:
           - Fetch our ML-KEM-768 secret key
           - Decapsulate CEK using ML-KEM-768
           - If authcrypt, verify signature with sender's ML-DSA-65
        3. Decrypt message with CEK using ChaCha20-Poly1305
        4. Return (message, sender_vk, recipient_vk)

    Args:
        session: Askar session for key lookups
        wrapper: JweEnvelope object

    Returns:
        Tuple[str, str, str]: (message, sender_verkey, recipient_verkey)
    """
    from acapy_agent.wallet.base import WalletError
    from acapy_agent.utils.jwe import b64url

    alg = wrapper.protected.get("alg")
    is_authcrypt = alg == "PQC-Authcrypt"

    # Get liboqs for KEM operations
    liboqs = get_liboqs()

    # Parse recipients manually (don't use extract_pack_recipients - it's classical-only)
    # PQC format: {"kid": "...", "sender": "...", "sig": "..."}
    # Classical format: {"kid": "...", "sender": "...", "iv": "..."}
    # NOTE: wrapper.recipients returns JweRecipient objects, not dicts
    recips = {}
    for recipient in wrapper.recipients:
        kid = recipient.header.get("kid")
        if kid:
            recips[kid] = {
                "key": recipient.encrypted_key,
                "sender": recipient.header.get("sender"),
                "sig": recipient.header.get("sig"),  # PQC: ML-DSA-65 signature
                "iv": recipient.header.get("iv"),    # Classical: nonce
                "nonce": recipient.header.get("iv"), # Alias for compatibility
            }

    # Try to decrypt for each recipient we have a key for
    cek_bytes, sender_vk, recip_vk = None, None, None

    LOGGER.error("=" * 80)
    LOGGER.error("[PQC UNPACK DEBUG] Starting recipient decryption loop")
    LOGGER.error(f"[PQC UNPACK DEBUG] Found {len(recips)} recipients in JWE")
    LOGGER.error(f"[PQC UNPACK DEBUG] Recipient KIDs: {list(recips.keys())}")

    for recip_verkey, recip_data in recips.items():
        LOGGER.error(f"[PQC UNPACK DEBUG] Trying recipient: {recip_verkey[:60] if len(recip_verkey) > 60 else recip_verkey}...")

        # Try to fetch our secret key
        LOGGER.error(f"[PQC UNPACK DEBUG] Calling session.fetch_key({recip_verkey[:60]}...)")
        recip_key_entry = await session.fetch_key(recip_verkey)
        LOGGER.error(f"[PQC UNPACK DEBUG] fetch_key result: {recip_key_entry}")

        if recip_key_entry:
            # Handle both KeyEntry wrapper (classical) and direct PQCKey
            if isinstance(recip_key_entry, PQCKey):
                recip_key = recip_key_entry  # Already a PQCKey
            else:
                recip_key = recip_key_entry.key  # KeyEntry wrapper

            if isinstance(recip_key, PQCKey) and recip_key.algorithm == "ml-kem-768":
                # PQC path: ML-KEM-768 decapsulation
                secret_key = recip_key.get_secret_bytes()
                ciphertext = recip_data["key"]  # encrypted_key from JWE recipient

                try:
                    # Decapsulate to get shared secret (used as CEK)
                    shared_secret = liboqs.kem_decapsulate(secret_key, ciphertext)
                    cek_bytes = shared_secret[:32]  # ChaCha20 needs 256-bit key

                    # Check for authcrypt signature
                    if is_authcrypt:
                        sig = recip_data.get("sig")
                        sender_vk_b64 = recip_data.get("sender")

                        if not sig or not sender_vk_b64:
                            raise WalletError("Authcrypt message missing signature or sender")

                        # Decode sender verkey and signature
                        import base64
                        from acapy_agent.wallet.util import b58_to_bytes

                        sender_vk = base64.urlsafe_b64decode(sender_vk_b64 + "==").decode("utf-8")
                        signature = base64.urlsafe_b64decode(sig + "==")

                        # Extract sender's ML-DSA-65 public key directly from message
                        # (Like classical ED25519: don't fetch from wallet, use from message!)
                        sender_public = b58_to_bytes(sender_vk)

                        # Verify signature
                        LOGGER.error(f"[PQC UNPACK DEBUG] Verifying ML-DSA-65 signature...")
                        LOGGER.error(f"[PQC UNPACK DEBUG]   sender_vk (base58): {sender_vk[:60]}...")
                        LOGGER.error(f"[PQC UNPACK DEBUG]   sender_public (bytes): {len(sender_public)} bytes")
                        if not liboqs.ml_dsa_verify(sender_public, ciphertext, signature):
                            raise WalletError("Invalid ML-DSA-65 signature in authcrypt")
                        LOGGER.error(f"[PQC UNPACK DEBUG] Signature verified!")

                    recip_vk = recip_verkey
                    LOGGER.error(f"[PQC UNPACK DEBUG] SUCCESS! recip_vk set to: {recip_vk[:60] if len(recip_vk) > 60 else recip_vk}")
                    break  # Success!

                except Exception as e:
                    LOGGER.error(f"[PQC UNPACK DEBUG] Decryption failed: {e}")
                    LOGGER.debug(f"Failed to decrypt for recipient {recip_verkey[:20]}...: {e}")
                    continue

            else:
                # Classical key - should not happen in pure PQC mode
                LOGGER.warning(f"Recipient {recip_verkey[:20]}... is classical, "
                              f"trying classical crypto_box")

                # Fallback to classical decryption
                from acapy_agent.askar.didcomm.v1 import _extract_payload_key
                try:
                    cek_bytes, sender_vk = _extract_payload_key(recip_data, recip_key)
                    recip_vk = recip_verkey
                    break
                except Exception as e:
                    LOGGER.error(f"[PQC UNPACK DEBUG] Classical decryption failed: {e}")
                    LOGGER.debug(f"Classical decrypt failed for {recip_verkey[:20]}...: {e}")
                    continue
        else:
            LOGGER.error(f"[PQC UNPACK DEBUG] No key found for recipient {recip_verkey[:60] if len(recip_verkey) > 60 else recip_verkey}")

    LOGGER.error(f"[PQC UNPACK DEBUG] Loop finished. recip_vk = {recip_vk}")
    LOGGER.error("=" * 80)

    if not cek_bytes:
        raise WalletError(f"No corresponding recipient key found in {tuple(recips.keys())}")

    if not sender_vk and is_authcrypt:
        raise WalletError("Sender public key not provided for Authcrypt message")

    # Decrypt message with CEK
    cek = Key.from_secret_bytes(KeyAlg.C20P, cek_bytes)
    message = cek.aead_decrypt(
        wrapper.ciphertext,
        nonce=wrapper.iv,
        tag=wrapper.tag,
        aad=wrapper.protected_bytes,
    )

    # Return in ACA-Py format: (message, from_verkey, to_verkey)
    # from_verkey = sender, to_verkey = recipient
    return message.decode("utf-8"), sender_vk, recip_vk
\end{lstlisting}

\subsubsection{monkey\_patches.py}

\refstepcounter{manualListingCounterA}
\label{lst:monkey_patches.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - monkey\_patches.py}, numbers=left, frame=single]
"""Monkey patches for transparent PQC integration.

This module patches BaseConnectionManager and DIDExchangeManager to transparently
replace ED25519/X25519 with ML-DSA-65/ML-KEM-768 when creating did:peer:4 DIDs.

NO API changes needed - existing workflows continue to work!
"""

import logging
from typing import Optional, Sequence, List, Tuple

from acapy_agent.connections.base_manager import BaseConnectionManager
from acapy_agent.protocols.didexchange.v1_0.manager import DIDXManager
from acapy_agent.protocols.didexchange.v1_0.messages.request import DIDXRequest
from acapy_agent.protocols.didexchange.v1_0.messages.response import DIDXResponse
from acapy_agent.connections.models.conn_record import ConnRecord
from acapy_agent.messaging.decorators.attach_decorator import AttachDecorator
from acapy_agent.wallet.base import BaseWallet
from acapy_agent.wallet.did_method import PEER4
from acapy_agent.protocols.coordinate_mediation.v1_0.models.mediation_record import MediationRecord

from .key_types import ML_DSA_65, ML_KEM_768
from .base_manager_patch import (
    create_did_peer_4_pqc_complete,
    create_did_peer_4_conditional_pqc,
    _extract_key_material_in_base58_format_pqc,
    long_did_peer_4_to_short_pqc,
    long_did_peer_to_short_pqc,
    record_keys_for_resolvable_did_pqc,
    resolve_inbound_connection_pqc,
    find_connection_pqc,
)

LOGGER = logging.getLogger(__name__)


# Store original methods for potential fallback
_original_create_did_peer_4 = BaseConnectionManager.create_did_peer_4
_original_qualified_did_with_fallback = DIDXManager._qualified_did_with_fallback
_original_extract_key_material = BaseConnectionManager._extract_key_material_in_base58_format
_original_long_did_peer_4_to_short = BaseConnectionManager.long_did_peer_4_to_short
_original_long_did_peer_to_short = BaseConnectionManager.long_did_peer_to_short
_original_record_keys_for_resolvable_did = BaseConnectionManager.record_keys_for_resolvable_did
_original_find_connection = BaseConnectionManager.find_connection


async def _qualified_did_with_fallback_pqc(
    self,
    conn_rec: ConnRecord,
    my_endpoints: Sequence[str],
    mediation_records: List[MediationRecord],
    use_did_method: Optional[str] = None,
    signing_key: Optional[str] = None,
) -> Tuple[str, Optional[AttachDecorator]]:
    """Patched _qualified_did_with_fallback with PQC signature fix.

    CRITICAL FIX: For did:peer:4 with PQC, the `signing_key` parameter is often
    the `invitation_key` from the out-of-band invitation. However, this key is
    NOT the same as the authentication key in the did:peer:4 DID Document!

    For PQC did:peer:4:
    - The DID contains ML-DSA-65 authentication keys (1952 bytes)
    - The signature MUST use the authentication key from the DID
    - NOT the invitation_key (which might be a different key or from a different DID)

    This patch:
    1. Calls the original method
    2. If the DID is did:peer:4 and signing_key was provided, replaces it with
       the actual authentication key from the created DID
    3. Re-signs the attachment with the correct key

    This ensures DID Exchange signature verification succeeds!
    """
    from acapy_agent.protocols.didexchange.v1_0.manager import LegacyHandlingFallback, DIDPosture

    # Call the original method to get the DID and potentially signed attachment
    did, attach = await _original_qualified_did_with_fallback(
        self, conn_rec, my_endpoints, mediation_records, use_did_method, signing_key
    )

    # Check if we need to re-sign with the correct key
    if did and did.startswith("did:peer:4") and attach and signing_key:
        # The original method signed with signing_key (invitation_key)
        # But for did:peer:4, we should sign with the DID's authentication key!

        LOGGER.info(f"PQC Fix: Re-signing did:peer:4 attachment with DID authentication key")
        LOGGER.debug(f"  Original signing_key: {signing_key[:20]}...")

        # Get the DID's authentication key
        async with self.profile.session() as session:
            wallet = session.inject(BaseWallet)
            my_info = await wallet.get_local_did(conn_rec.my_did)

        LOGGER.debug(f"  DID authentication key (verkey): {my_info.verkey[:20]}...")
        LOGGER.debug(f"  Key type: {my_info.key_type}")

        # Only re-sign if the keys are different
        if my_info.verkey != signing_key:
            LOGGER.info(f"  Keys differ! Re-signing with correct authentication key")

            # Re-sign the attachment with the correct key
            async with self.profile.session() as session:
                wallet = session.inject(BaseWallet)
                await attach.data.sign(my_info.verkey, wallet)

            LOGGER.info("Attachment re-signed with did:peer:4 authentication key")
        else:
            LOGGER.debug("  Keys match, no re-signing needed")

    return did, attach


def apply_all_patches():
    """Apply all monkey patches for transparent PQC integration.

    This function:
    1. Patches BaseConnectionManager.create_did_peer_4 with PQC version
    2. Patches BaseConnectionManager._extract_key_material_in_base58_format for PQC
    3. Patches BaseConnectionManager.long_did_peer_4_to_short to preserve key_type
    4. Patches BaseConnectionManager.long_did_peer_to_short to handle short-form DIDs
    5. Patches DIDXManager._qualified_did_with_fallback for correct PQC signatures
    6. Patches BaseConnectionManager.record_keys_for_resolvable_did for PQC key storage
    7. Patches BaseConnectionManager.resolve_inbound_connection with DEBUG logging
    8. Patches BaseConnectionManager.find_connection to handle long/short form my_did
    9. Extends PEER4 DID method to support ML-DSA-65 and ML-KEM-768 key types

    Called during plugin setup.
    """

    # 1. Patch BaseConnectionManager.create_did_peer_4 with CRYPTO-AGILE wrapper
    # KRYPTO-AGILITÄT: Conditional wrapper that checks metadata["key_type"]
    # - If "ed25519" --> Delegates to _original_create_did_peer_4 (pre-plugin behavior)
    # - Otherwise --> Uses ML-DSA-65 + ML-KEM-768 (PQC)
    # This enables mixed environments where ED25519 and PQC can coexist!
    BaseConnectionManager.create_did_peer_4 = create_did_peer_4_conditional_pqc

    # 2. Patch BaseConnectionManager._extract_key_material_in_base58_format
    # CRITICAL: This allows out-of-band invitation creation to work with PQC keys
    # by accepting ml-dsa-65-pub and ml-kem-768-pub multicodecs
    BaseConnectionManager._extract_key_material_in_base58_format = staticmethod(
        _extract_key_material_in_base58_format_pqc
    )

    # 3. Patch BaseConnectionManager.long_did_peer_4_to_short
    # CRITICAL: This preserves PQC key_type when converting long-form to short-form,
    # eliminating ED25519 "ghost DIDs" in connections!
    BaseConnectionManager.long_did_peer_4_to_short = long_did_peer_4_to_short_pqc

    # 4. Patch BaseConnectionManager.long_did_peer_to_short
    # CRITICAL FIX: This prevents double-conversion of short-form DIDs during
    # connection lookup, which was causing "No connection found" errors during
    # credential offer handling!
    BaseConnectionManager.long_did_peer_to_short = long_did_peer_to_short_pqc

    # 5. Patch DIDXManager._qualified_did_with_fallback for correct PQC signatures
    # CRITICAL: This ensures did:peer:4 attachments are signed with the DID's
    # authentication key, not the invitation_key!
    DIDXManager._qualified_did_with_fallback = _qualified_did_with_fallback_pqc

    # 6. Patch BaseConnectionManager.record_keys_for_resolvable_did
    # CRITICAL FIX: Stores BOTH keys (ML-DSA-65 auth + ML-KEM-768 keyAgr) in DID_KEY records
    # This enables find_did_for_key(sender_verkey) to work during message handling!
    # Original only stored recipientKeys (ML-KEM-768), causing connection lookup to fail
    # when searching for sender's authentication key (ML-DSA-65)
    BaseConnectionManager.record_keys_for_resolvable_did = record_keys_for_resolvable_did_pqc

    # 7. Patch BaseConnectionManager.resolve_inbound_connection
    # DEBUG PATCH: Adds extensive logging to diagnose why recipient_did is None
    BaseConnectionManager.resolve_inbound_connection = resolve_inbound_connection_pqc

    # 8. Patch BaseConnectionManager.find_connection
    # CRITICAL FIX for credential offer handling: Handles both long and short forms of my_did
    # This fixes "No connection found" errors during credential issuance when:
    # - Connection stored with my_did = LONG-FORM (during DID Exchange)
    # - Credential offer uses my_did = SHORT-FORM (from wallet.get_local_did_for_verkey)
    BaseConnectionManager.find_connection = find_connection_pqc

    # 9. Extend PEER4 key types for validation (optional but recommended)
    if ML_DSA_65 not in PEER4._supported_key_types:
        PEER4._supported_key_types.extend([ML_DSA_65, ML_KEM_768])
\end{lstlisting}

\subsubsection{base\_manager\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:base_manager_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - base\_manager\_patch.py}, numbers=left, frame=single]
"""Patch for acapy_agent/connections/base_manager.py to use PQC keys.

This patch completely replaces BaseConnectionManager.create_did_peer_4 to use
ML-DSA-65 + ML-KEM-768 instead of ED25519, eliminating the creation of ED25519
"ghost DIDs" in the wallet.

NO modifications to acapy_agent source code needed - all changes are in the plugin!
"""

import logging
from typing import Dict, List, Optional, Sequence

from did_peer_4 import encode
from did_peer_4.input_doc import input_doc_from_keys_and_services, KeySpec as KeySpec_DP4

from acapy_agent.wallet.base import BaseWallet
from acapy_agent.wallet.did_info import DIDInfo
from acapy_agent.wallet.did_method import PEER4
from acapy_agent.protocols.coordinate_mediation.v1_0.models.mediation_record import MediationRecord
from acapy_agent.did.did_key import DIDKey
from acapy_agent.wallet.util import bytes_to_b58, b64_to_bytes
from acapy_agent.connections.base_manager import BaseConnectionManagerError
from acapy_agent.utils.multiformats import multibase, multicodec

from .key_types import ML_DSA_65, ML_KEM_768
from .pqc_multikey import key_info_to_multikey

LOGGER = logging.getLogger(__name__)


async def create_did_peer_4_pqc_complete(
    self,
    svc_endpoints: Optional[Sequence[str]] = None,
    mediation_records: Optional[List[MediationRecord]] = None,
    metadata: Optional[Dict] = None,
) -> DIDInfo:
    """Create a did:peer:4 DID for a connection using PQC keys.

    This is a COMPLETE replacement for BaseConnectionManager.create_did_peer_4
    that uses ML-DSA-65 + ML-KEM-768 instead of ED25519.

    CRITICAL DIFFERENCE from original:
    - Line 165 in original: `key = await wallet.create_key(ED25519)`
    - This version: Creates ML-DSA-65 (authentication) + ML-KEM-768 (key agreement)

    This eliminates the ED25519 "ghost DIDs" that were previously created!

    Args:
        self: BaseConnectionManager instance
        svc_endpoints: Custom endpoints for the DID Document
        mediation_records: Records for mediation that contain routing keys and endpoint
        metadata: Additional metadata for the DID

    Returns:
        DIDInfo: The new PQC did:peer:4 DID
    """
    # Extract routing keys from mediation (same as original)
    routing_keys: List[str] = []
    if mediation_records:
        for mediation_record in mediation_records:
            (
                mediator_routing_keys,
                endpoint,
            ) = await self._route_manager.routing_info(
                self._profile, mediation_record
            )
            routing_keys = [*routing_keys, *(mediator_routing_keys or [])]
            if endpoint:
                svc_endpoints = [endpoint]

    # Build DIDComm v1 services (same as original)
    services = []
    for index, endpoint in enumerate(svc_endpoints or []):
        services.append(
            {
                "id": f"#didcomm-{index}",
                "type": "did-communication",
                "recipientKeys": ["#key-1"],  # ML-KEM-768 key for encryption (key_specs[1] --> #key-1)
                "routingKeys": routing_keys,
                "serviceEndpoint": endpoint,
                "priority": index,
            }
        )

    async with self._profile.session() as session:
        wallet = session.inject(BaseWallet)

        # ============================================================
        # PQC MODIFICATION: Create TWO keys instead of one ED25519 key
        # ============================================================

        # 1. Create ML-DSA-65 key for authentication/assertion (signatures)
        sig_key = await wallet.create_key(ML_DSA_65)
        sig_multikey = key_info_to_multikey(sig_key)

        # 2. Create ML-KEM-768 key for key agreement (encryption)
        kem_key = await wallet.create_key(ML_KEM_768)
        kem_multikey = key_info_to_multikey(kem_key)

        LOGGER.info(f"Created PQC keys for did:peer:4:")
        LOGGER.info(f"  - ML-DSA-65 (auth): {sig_multikey[:40]}...")
        LOGGER.info(f"  - ML-KEM-768 (keyAgr): {kem_multikey[:40]}...")

        # 3. Create KeySpec objects for did:peer:4 (same as pqc_peer4_creator.py)
        # NOTE: Order matters! did-peer-4 numbers from 0: key_specs[0] --> #key-0, key_specs[1] --> #key-1
        key_specs = [
            KeySpec_DP4(
                multikey=sig_multikey,
                relationships=["authentication", "assertionMethod"],  # --> #key-0
            ),
            KeySpec_DP4(
                multikey=kem_multikey,
                relationships=["keyAgreement"],  # --> #key-1 (used in recipientKeys!)
            ),
        ]

        # 4. Generate did:peer:4 long form (same structure as original)
        input_doc = input_doc_from_keys_and_services(
            keys=key_specs,
            services=services
        )
        did = encode(input_doc)

        LOGGER.info(f"Generated PQC did:peer:4: {did[:80]}...")

        # 5. Create metadata with PQC markers
        did_metadata = metadata if metadata else {}
        did_metadata.update({
            "pqc_enabled": True,
            "signature_algorithm": "ml-dsa-65",
            "key_agreement_algorithm": "ml-kem-768",
            "kem_key_kid": f"{did}#key-1",  # KEM key is key_specs[1] --> #key-1
            "kem_verkey": kem_key.verkey,  # CRITICAL: Store KEM verkey for connection lookup!
            "plugin": "pqc_didpeer4_fm",
            "version": "0.1.0",
        })

        LOGGER.error(f"STORING PQC DID WITH KEM_VERKEY IN METADATA:")
        LOGGER.error(f"   DID: {did[:60]}...")
        LOGGER.error(f"   Primary verkey (ML-DSA-65): {sig_key.verkey[:30]}...")
        LOGGER.error(f"   KEM verkey (ML-KEM-768): {kem_key.verkey[:30]}...")
        LOGGER.error(f"   Metadata keys: {list(did_metadata.keys())}")

        # 6. Create DIDInfo with ML-DSA-65 as primary key type
        did_info = DIDInfo(
            did=did,
            method=PEER4,
            verkey=sig_key.verkey,  # ML-DSA-65 verkey
            metadata=did_metadata,
            key_type=ML_DSA_65,  # NOT ED25519!
        )

        # 7. Store DID in wallet
        await wallet.store_did(did_info)

        # 8. Assign Key IDs - did-peer-4 numbers from 0!
        await wallet.assign_kid_to_key(sig_key.verkey, f"{did}#key-0")  # key_specs[0] --> #key-0
        await wallet.assign_kid_to_key(kem_key.verkey, f"{did}#key-1")  # key_specs[1] --> #key-1

        LOGGER.info(f"Stored PQC did:peer:4 in wallet with key IDs assigned")

    return did_info

async def create_did_peer_4_conditional_pqc(
    self,
    svc_endpoints: Optional[Sequence[str]] = None,
    mediation_records: Optional[List[MediationRecord]] = None,
    metadata: Optional[Dict] = None,
) -> DIDInfo:
    """Conditional wrapper: Use PQC or original ED25519 based on metadata.
    """
    from acapy_agent.connections.base_manager import BaseConnectionManager

    # Check: Wurde ED25519 explizit gewünscht?
    if metadata and metadata.get("key_type") == "ed25519":
        LOGGER.info("Kryptoagilität: ED25519 gewünscht --> Nutze Original-ACA-Py (KEIN Plugin-Patch)")
        # Delegation an ursprüngliche ACA-Py-Implementierung ohne Plugin-Eingriff
        # Die Original-Funktion wurde gesichert bevor das Plugin sie überschrieben hat
        from . import monkey_patches
        if hasattr(monkey_patches, '_original_create_did_peer_4'):
            return await monkey_patches._original_create_did_peer_4(
                self, svc_endpoints, mediation_records, metadata
            )
        else:
            # Fallback: Rufe die aktuelle Implementierung auf (sollte nicht passieren)
            LOGGER.warning("Original create_did_peer_4 nicht gefunden, nutze aktuelle Implementierung")
            return await BaseConnectionManager.create_did_peer_4(
                self, svc_endpoints, mediation_records, metadata
            )

    # Default: Nutze PQC
    LOGGER.info("Kryptoagilität: PQC-Algorithmen (ML-DSA-65 + ML-KEM-768)")
    return await create_did_peer_4_pqc_complete(
        self, svc_endpoints, mediation_records, metadata
    )

def _extract_key_material_in_base58_format_pqc(method) -> str:
    """Patched version of BaseConnectionManager._extract_key_material_in_base58_format.

    This patch extends the original method to accept PQC multicodecs (ML-DSA-65 and ML-KEM-768)
    in addition to the classical ED25519 multicodec.

    CRITICAL: This method is called during out-of-band invitation creation when resolving
    recipient keys from did:peer:4 DID Documents. The original only accepts ed25519-pub,
    causing BaseConnectionManagerError when encountering PQC keys.

    This patched version:
    1. Maintains full backward compatibility with all existing key types
    2. Adds support for ml-dsa-65-pub and ml-kem-768-pub multicodecs
    3. Returns base58-encoded key material for all supported key types

    Args:
        method: VerificationMethod from DID Document

    Returns:
        str: Base58-encoded key material

    Raises:
        BaseConnectionManagerError: If key type or multicodec is not supported
    """
    from pydid.verification_method import (
        Ed25519VerificationKey2018,
        Ed25519VerificationKey2020,
        JsonWebKey2020,
        Multikey,
    )

    if isinstance(method, Ed25519VerificationKey2018):
        return method.material
    elif isinstance(method, Ed25519VerificationKey2020):
        raw_data = multibase.decode(method.material)
        if len(raw_data) == 32:  # No multicodec prefix
            return bytes_to_b58(raw_data)
        else:
            codec, key = multicodec.unwrap(raw_data)
            if codec == multicodec.multicodec("ed25519-pub"):
                return bytes_to_b58(key)
            else:
                raise BaseConnectionManagerError(
                    f"Key type {type(method).__name__} "
                    f"with multicodec value {codec} is not supported"
                )

    elif isinstance(method, JsonWebKey2020):
        if method.public_key_jwk.get("kty") == "OKP":
            return bytes_to_b58(b64_to_bytes(method.public_key_jwk.get("x"), True))
        else:
            raise BaseConnectionManagerError(
                f"Key type {type(method).__name__} "
                f"with kty {method.public_key_jwk.get('kty')} is not supported"
            )
    elif isinstance(method, Multikey):
        codec, key = multicodec.unwrap(multibase.decode(method.material))

        # PQC EXTENSION: Accept ML-DSA-65 and ML-KEM-768 in addition to ED25519
        accepted_codecs = [
            multicodec.multicodec("ed25519-pub"),      # Classical
            multicodec.multicodec("ml-dsa-65-pub"),    # PQC signature
            multicodec.multicodec("ml-kem-768-pub"),   # PQC key agreement
        ]

        if codec not in accepted_codecs:
            raise BaseConnectionManagerError(
                f"Expected ed25519-pub, ml-dsa-65-pub, or ml-kem-768-pub multicodec, got: {codec}"
            )

        LOGGER.debug(f"Extracting key material from Multikey with codec: {codec.name}")
        return bytes_to_b58(key)
    else:
        raise BaseConnectionManagerError(
            f"Key type {type(method).__name__} is not supported"
        )


async def long_did_peer_4_to_short_pqc(self, long_dp4: str) -> str:
    """Convert did:peer:4 long format to short format and store in wallet (PQC version).

    This is a patched version of BaseConnectionManager.long_did_peer_4_to_short that:
    1. Preserves the key_type from the original long-form DID (ML-DSA-65 instead of hardcoded ED25519)
    2. Preserves metadata from the original DID
    3. Ensures quantum-safe connections

    CRITICAL DIFFERENCE from original (base_manager.py:112):
    - Original: `key_type=ED25519` (hardcoded)
    - This version: `key_type=long_dp4_info.key_type` (preserved from long-form)

    This eliminates the ED25519 "ghost DIDs" in connections!

    Args:
        self: BaseConnectionManager instance
        long_dp4: Long-form did:peer:4 DID

    Returns:
        str: Short-form did:peer:4 DID
    """
    from did_peer_4 import long_to_short

    async with self._profile.session() as session:
        wallet = session.inject(BaseWallet)
        long_dp4_info = await wallet.get_local_did(long_dp4)

    short_did_peer_4 = long_to_short(long_dp4)

    # CRITICAL: Use key_type from original DID, NOT hardcoded ED25519!
    did_info = DIDInfo(
        did=short_did_peer_4,
        method=PEER4,
        verkey=long_dp4_info.verkey,
        metadata=long_dp4_info.metadata or {},  # Preserve metadata
        key_type=long_dp4_info.key_type,  # Preserve key_type (ML-DSA-65)
    )

    async with self._profile.session() as session:
        wallet = session.inject(BaseWallet)
        await wallet.store_did(did_info)

    LOGGER.info(f"Converted Long-Form to Short-Form (key_type preserved: {long_dp4_info.key_type})")
    LOGGER.debug(f"   Long:  {long_dp4[:80]}...")
    LOGGER.debug(f"   Short: {short_did_peer_4}")

    return did_info.did


async def record_keys_for_resolvable_did_pqc(self, did: str):
    """Store verkeys for lookup against a DID (PQC version that stores BOTH keys).

    This is an EXTENDED version of BaseConnectionManager.record_keys_for_resolvable_did
    that stores BOTH recipient keys (ML-KEM-768) AND authentication keys (ML-DSA-65).

    CRITICAL FIX for connection lookup during message handling:
    - Original (base_manager.py:536-543): Only stores recipientKeys from DIDComm service
    - For classical did:peer:4: recipientKeys = [ED25519] --> Stored
    - For PQC did:peer:4: recipientKeys = [ML-KEM-768 (#key-1)] --> Stored
      BUT authentication key (ML-DSA-65 #key-0) is NOT stored!

    Problem:
    - DIDComm messages have sender_verkey = sender's authentication key (ML-DSA-65)
    - `find_did_for_key(sender_verkey)` looks up this key in DID_KEY records
    - For PQC: ML-DSA-65 was never stored --> NOT FOUND! --> Connection lookup fails!

    This patch:
    1. Stores recipientKeys (original behavior - KEM key)
    2. ALSO stores authentication keys (ML-DSA-65 for PQC)
    3. Enables connection lookup for BOTH keys

    Args:
        self: BaseConnectionManager instance
        did: The DID for which to record keys
    """
    from acapy_agent.resolver.did_resolver import DIDResolver

    LOGGER.error(f"[PQC] ====== record_keys_for_resolvable_did_pqc CALLED ======")
    LOGGER.error(f"[PQC]   DID: {did[:80]}...")

    doc, didcomm_services = await self.resolve_didcomm_services(did)

    # 1. Store recipient keys from DIDComm services (original behavior)
    LOGGER.error(f"[PQC]   Found {len(didcomm_services)} DIDComm services")
    for service in didcomm_services:
        recips, _ = await self.verification_methods_for_service(doc, service)
        LOGGER.error(f"[PQC]   Service has {len(recips)} recipient keys")
        for recip in recips:
            recip_key = self._extract_key_material_in_base58_format(recip)
            LOGGER.error(f"[PQC]   Storing RECIPIENT key: {recip_key[:30]}... --> DID: {did[:60]}...")
            await self.add_key_for_did(did, recip_key)

    # 2. ALSO store authentication keys (NEW for PQC support!)
    #    This enables find_did_for_key(sender's ML-DSA-65) to work
    if doc.authentication:
        LOGGER.error(f"[PQC]   Found {len(doc.authentication)} authentication keys in DID Document")
        resolver = self._profile.inject(DIDResolver)
        for auth_ref in doc.authentication:
            try:
                auth_method = await resolver.dereference_verification_method(
                    self._profile, auth_ref, document=doc
                )
                auth_key = self._extract_key_material_in_base58_format(auth_method)
                LOGGER.error(f"[PQC] Storing AUTHENTICATION key: {auth_key[:30]}... --> DID: {did[:60]}...")
                await self.add_key_for_did(did, auth_key)
            except Exception as e:
                LOGGER.error(f"[PQC] Could not store authentication key: {e}")
                # Don't fail if we can't store auth key (not critical)
                pass
    else:
        LOGGER.error(f"[PQC] No authentication keys found in DID Document!")

    LOGGER.error(f"[PQC] ====== record_keys_for_resolvable_did_pqc DONE ======")


def long_did_peer_to_short_pqc(self, long_did: str) -> str:
    """Convert did:peer:4 long format to short format and return (PQC-safe version).

    This is a patched version of BaseConnectionManager.long_did_peer_to_short that
    handles BOTH long-form and short-form DIDs correctly.

    CRITICAL FIX for connection lookup during message handling:
    - Original method (base_manager.py:93-97) blindly converts any did:peer:4 to short
    - But after DID Exchange, DIDs are stored in SHORT-FORM in the wallet
    - When `find_connection` (line 877) looks up a connection, it assumes `their_did`
      is LONG-FORM and tries to convert it to short (line 900)
    - If `their_did` is ALREADY short-form, converting it again produces a WRONG hash
    - Connection lookup fails --> "No connection found" error in credential offer handler!

    This patch:
    1. Detects if the DID is already in short-form (no multibase-encoded DID Document suffix)
    2. Returns it unchanged if short-form
    3. Converts if long-form (original behavior)

    Short-form vs Long-form detection:
    - Short-form: did:peer:4zQmZMkY... (just the hash, no suffix after 4th ":")
    - Long-form:  did:peer:4zQmZMkY:z25g... (hash + ":" + multibase-encoded doc)
    - Count colons: SHORT has 2 colons ("did:peer:4..."), LONG has 3+ ("did:peer:4...:z...")

    Args:
        self: BaseConnectionManager instance
        long_did: did:peer:4 DID (can be long-form OR short-form)

    Returns:
        str: Short-form did:peer:4 DID
    """
    from did_peer_4 import long_to_short

    # Check if already short-form by counting colons
    # Short-form: "did:peer:4zQm..." --> 2 colons
    # Long-form:  "did:peer:4zQm...:z25g..." --> 3 colons
    colon_count = long_did.count(":")
    if colon_count == 2:
        LOGGER.debug(f"DID is already short-form (2 colons), returning unchanged: {long_did[:60]}...")
        return long_did

    # Long-form --> convert to short
    LOGGER.debug(f"Converting long-form to short-form ({colon_count} colons): {long_did[:60]}...")
    short_did_peer = long_to_short(long_did)
    LOGGER.debug(f"  Result: {short_did_peer}")
    return short_did_peer


async def resolve_inbound_connection_pqc(self, receipt):
    """PQC-aware version of resolve_inbound_connection with sender-based lookup.

    CRITICAL FIX for PQC multi-connection scenario:
    - Problem: wallet.get_local_did_for_verkey(recipient_verkey) can return WRONG DID
      when multiple connections exist (each with different ML-KEM-768 keys)
    - The wallet patch searches ALL DIDs and returns the FIRST match, which might be
      from a DIFFERENT connection (e.g., verifier connection instead of issuer connection)

    Solution: Prefer sender-based connection lookup over recipient-based:
    1. Resolve sender_did from sender_verkey (their authentication key)
    2. Find connection by their_did (sender_did) - this uniquely identifies the connection
    3. Only fall back to recipient_did lookup if sender-based lookup fails

    This matches ED25519 workflow where connections are resolved from sender identity.
    """
    from acapy_agent.core.error import BaseError
    from acapy_agent.storage.error import StorageNotFoundError
    from acapy_agent.wallet.error import WalletNotFoundError

    # InjectionError might be from config.injection_context
    try:
        from acapy_agent.config.injection_context import InjectionError
    except ImportError:
        # Fallback: use BaseError
        InjectionError = BaseError

    LOGGER.error("=" * 80)
    LOGGER.error("[PQC FIX] resolve_inbound_connection CALLED")
    LOGGER.error(f"[PQC FIX]   sender_verkey: {receipt.sender_verkey[:30] if receipt.sender_verkey else 'None'}...")
    LOGGER.error(f"[PQC FIX]   recipient_verkey: {receipt.recipient_verkey[:30] if receipt.recipient_verkey else 'None'}...")

    # Step 1: Resolve sender_did from sender_verkey (CRITICAL for PQC!)
    receipt.sender_did = None
    if receipt.sender_verkey:
        try:
            LOGGER.error(f"[PQC FIX] Looking up sender_did from sender_verkey (their auth key)...")
            receipt.sender_did = await self.find_did_for_key(receipt.sender_verkey)
            LOGGER.error(f"[PQC FIX] Found sender_did (their_did): {receipt.sender_did[:60]}...")
        except StorageNotFoundError as e:
            LOGGER.error(f"[PQC FIX] sender_did NOT found: {e}")
            pass

    # Step 2: Try sender-based connection lookup FIRST (NEW for PQC!)
    if receipt.sender_did:
        LOGGER.error(f"[PQC FIX] TRYING SENDER-BASED LOOKUP (their_did={receipt.sender_did[:60]}...)")
        try:
            connection = await self.find_connection(
                their_did=receipt.sender_did,
                my_did=None,  # Don't filter by my_did yet!
                parent_thread_id=receipt.parent_thread_id,
                auto_complete=True
            )
            if connection:
                LOGGER.error(f"[PQC FIX] Found connection via SENDER lookup!")
                LOGGER.error(f"[PQC FIX]   Connection: {connection.connection_id}")
                LOGGER.error(f"[PQC FIX]   their_did: {connection.their_did}")
                LOGGER.error(f"[PQC FIX]   my_did: {connection.my_did}")

                # Set recipient_did from the connection's my_did (correct one!)
                receipt.recipient_did = connection.my_did
                LOGGER.error(f"[PQC FIX]   Set recipient_did from connection: {receipt.recipient_did[:60]}...")
                LOGGER.error("=" * 80)
                return connection
        except Exception as e:
            LOGGER.error(f"[PQC FIX] Sender-based lookup failed: {e}")

    # Step 3: Fall back to recipient-based lookup (original behavior)
    LOGGER.error(f"[PQC FIX] Falling back to RECIPIENT-BASED lookup...")
    if receipt.recipient_verkey:
        try:
            LOGGER.error(f"[PQC FIX] Getting wallet and calling get_local_did_for_verkey...")
            async with self._profile.session() as session:
                wallet = session.inject(BaseWallet)
                my_info = await wallet.get_local_did_for_verkey(
                    receipt.recipient_verkey
                )
            LOGGER.error(f"[PQC FIX]   Got my_info.did: {my_info.did[:60]}...")
            receipt.recipient_did = my_info.did
            if "posted" in my_info.metadata and my_info.metadata["posted"] is True:
                receipt.recipient_did_public = True
                LOGGER.error(f"[PQC FIX]   Set recipient_did_public = True")
        except InjectionError as e:
            LOGGER.error(f"[PQC FIX] InjectionError: {e}")
            self._logger.warning(
                "Cannot resolve recipient verkey, no wallet defined by context: %s",
                receipt.recipient_verkey,
            )
        except WalletNotFoundError as e:
            LOGGER.error(f"[PQC FIX] WalletNotFoundError: {e}")
            self._logger.debug(
                "No corresponding DID found for recipient verkey: %s",
                receipt.recipient_verkey,
            )

    LOGGER.error(f"[PQC FIX]   Final sender_did: {receipt.sender_did[:60] if receipt.sender_did else 'None'}...")
    LOGGER.error(f"[PQC FIX]   Final recipient_did: {receipt.recipient_did[:60] if receipt.recipient_did else 'None'}...")
    LOGGER.error("[PQC FIX] Calling find_connection with both DIDs...")

    result = await self.find_connection(
        receipt.sender_did, receipt.recipient_did, receipt.parent_thread_id, True
    )

    LOGGER.error(f"[PQC FIX]   find_connection result: {result}")
    LOGGER.error("[PQC FIX] resolve_inbound_connection DONE")
    LOGGER.error("=" * 80)

    return result


async def find_connection_pqc(
    self,
    their_did: Optional[str],
    my_did: Optional[str] = None,
    parent_thread_id: Optional[str] = None,
    auto_complete=False,
):
    """PQC-aware version of find_connection that handles both long and short forms of my_did.

    CRITICAL FIX for credential offer handling:
    - Connection is stored with my_did = LONG-FORM during DID Exchange
    - Credential offer processing uses my_did = SHORT-FORM (from wallet.get_local_did_for_verkey)
    - Original find_connection only queries with the exact my_did provided --> NOT FOUND!

    This patch:
    1. Converts my_did to both long and short forms (if did:peer:4)
    2. Queries database with: (their_did IN (long, short)) AND (my_did IN (long, short))
    3. Returns the connection if found

    This matches the existing logic for their_did in base_manager.py:897-907.

    Args:
        self: BaseConnectionManager instance
        their_did: Their DID
        my_did: My DID (can be long or short form for did:peer:4)
        parent_thread_id: Parent thread ID
        auto_complete: Should this connection automatically be promoted to active

    Returns:
        The located `ConnRecord`, if any
    """
    from acapy_agent.storage.error import StorageNotFoundError
    from acapy_agent.connections.models.conn_record import ConnRecord
    from acapy_agent.protocols.discovery.v2_0.manager import V20DiscoveryMgr
    from did_peer_4 import long_to_short

    LOGGER.error("=" * 80)
    LOGGER.error("[PQC FIX] find_connection_pqc CALLED")
    LOGGER.error(f"[PQC FIX]   their_did: {their_did[:60] if their_did else 'None'}...")
    LOGGER.error(f"[PQC FIX]   my_did: {my_did[:60] if my_did else 'None'}...")
    LOGGER.error(f"[PQC FIX]   parent_thread_id: {parent_thread_id}")

    connection = None
    if their_did and their_did.startswith("did:peer:4"):
        # Handle their_did: Convert to both long and short forms
        their_long = their_did
        their_short = self.long_did_peer_to_short(their_did)
        LOGGER.error(f"[PQC FIX]   their_did_long: {their_long[:60]}...")
        LOGGER.error(f"[PQC FIX]   their_did_short: {their_short[:60]}...")

        # CRITICAL FIX: Also handle my_did in both forms!
        if my_did and my_did.startswith("did:peer:4"):
            # Check if my_did is already long-form (contains ":z")
            if ":z" in my_did:
                my_long = my_did
                my_short = long_to_short(my_did)
            else:
                # my_did is short-form, need to find long-form
                # For now, we'll search with just short form and also try to
                # reconstruct long form from wallet
                my_short = my_did
                my_long = None  # We'll handle this with $or query

                # Try to get long form from wallet
                try:
                    async with self._profile.session() as session:
                        wallet = session.inject(BaseWallet)
                        # Check if there's a long-form version stored
                        try:
                            my_did_info = await wallet.get_local_did(my_did)
                            # If successful, my_did exists (short form is valid)
                        except Exception:
                            pass
                except Exception as e:
                    LOGGER.debug(f"Could not retrieve long form for my_did: {e}")

            LOGGER.error(f"[PQC FIX]   my_did is did:peer:4")
            LOGGER.error(f"[PQC FIX]   my_did_short: {my_short[:60]}...")
            LOGGER.error(f"[PQC FIX]   my_did_long: {my_long[:60] if my_long else 'None (will search all)'}...")

            # Query with both forms of their_did AND my_did
            try:
                async with self._profile.session() as session:
                    # Build query that checks:
                    # (their_did = long OR their_did = short) AND (my_did = short OR my_did = long)
                    # Since we might not have my_long, we'll search for connections with
                    # matching their_did first, then filter by my_did
                    LOGGER.error(f"[PQC FIX]   Querying with retrieve_by_did_peer_4...")

                    # Try with short my_did first
                    try:
                        connection = await ConnRecord.retrieve_by_did_peer_4(
                            session, their_long, their_short, my_short
                        )
                        LOGGER.error(f"[PQC FIX] Found connection with my_did={my_short[:60]}...")
                    except StorageNotFoundError:
                        # If short form fails and we have long form, try long form
                        if my_long:
                            LOGGER.error(f"[PQC FIX] Short form failed, trying long form...")
                            try:
                                connection = await ConnRecord.retrieve_by_did_peer_4(
                                    session, their_long, their_short, my_long
                                )
                                LOGGER.error(f"[PQC FIX] Found connection with my_did={my_long[:60]}...")
                            except StorageNotFoundError:
                                LOGGER.error(f"[PQC FIX] Long form also failed")
                                LOGGER.error(f"[PQC FIX] Connection NOT FOUND (as expected - wrong my_did from wallet)")
            except StorageNotFoundError:
                LOGGER.error(f"[PQC FIX] No connection found (StorageNotFoundError)")
                pass
        else:
            # my_did is not did:peer:4, use original logic
            LOGGER.error(f"[PQC FIX]   my_did is not did:peer:4, using original query")
            try:
                async with self._profile.session() as session:
                    connection = await ConnRecord.retrieve_by_did_peer_4(
                        session, their_long, their_short, my_did
                    )
                LOGGER.error(f"[PQC FIX] Found connection")
            except StorageNotFoundError:
                LOGGER.error(f"[PQC FIX] No connection found")
                pass
    elif their_did:
        # their_did is not did:peer:4, use original logic
        LOGGER.error(f"[PQC FIX]   their_did is not did:peer:4, using original retrieve_by_did")
        try:
            async with self._profile.session() as session:
                connection = await ConnRecord.retrieve_by_did(
                    session, their_did, my_did
                )
            LOGGER.error(f"[PQC FIX] Found connection")
        except StorageNotFoundError:
            LOGGER.error(f"[PQC FIX] No connection found")
            pass

    # Handle auto_complete (from original base_manager.py:917-934)
    if (
        connection
        and ConnRecord.State.get(connection.state) is ConnRecord.State.RESPONSE
        and auto_complete
    ):
        LOGGER.error(f"[PQC FIX]   Auto-completing connection to COMPLETED state")
        connection.state = ConnRecord.State.COMPLETED.rfc160
        async with self._profile.session() as session:
            await connection.save(session, reason="Connection promoted to active")
            if session.settings.get("auto_disclose_features"):
                discovery_mgr = V20DiscoveryMgr(self._profile)
                await discovery_mgr.proactive_disclose_features(
                    connection_id=connection.connection_id
                )

    LOGGER.error(f"[PQC FIX]   Result: {'Found' if connection else 'None'}")
    if connection:
        LOGGER.error(f"[PQC FIX]     connection_id: {connection.connection_id}")
        LOGGER.error(f"[PQC FIX]     their_did: {connection.their_did[:60]}...")
        LOGGER.error(f"[PQC FIX]     my_did: {connection.my_did[:60]}...")
    LOGGER.error("[PQC FIX] find_connection_pqc DONE")
    LOGGER.error("=" * 80)

    return connection
\end{lstlisting}

\subsubsection{askar\_pqc\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:askar_pqc_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - askar\_pqc\_patch.py}, numbers=left, frame=single]
"""Monkey-patch aries-askar to support PQC key generation.

This module patches acapy_agent.wallet.askar._create_keypair() to handle
ML-DSA-65 and ML-KEM-768 key generation via liboqs-python while maintaining
compatibility with the aries-askar Key interface.
"""

import logging
from base58 import b58encode
from typing import Optional

from .liboqs_wrapper import get_liboqs
from .key_types import ML_DSA_65, ML_KEM_768

LOGGER = logging.getLogger(__name__)

# Store reference to original function
_original_create_keypair = None


class PQCKey:
    """PQC Key wrapper compatible with aries-askar Key interface.

    This class mimics the interface of aries_askar.Key to ensure compatibility
    with ACA-Py's wallet operations while using liboqs-python for PQC keys.
    """

    def __init__(
        self,
        algorithm: str,
        public_key: bytes,
        secret_key: bytes,
        key_type_obj,
    ):
        """Initialize PQC Key wrapper.

        Args:
            algorithm: Algorithm name (e.g., "ml-dsa-65", "ml-kem-768")
            public_key: Public key bytes
            secret_key: Secret key bytes
            key_type_obj: KeyType object (ML_DSA_65 or ML_KEM_768)
        """
        self.algorithm = algorithm
        self._public_key = public_key
        self._secret_key = secret_key
        self._key_type = key_type_obj
        LOGGER.debug(f"Created PQCKey: algorithm={algorithm}, public_key_len={len(public_key)}")

    def get_public_bytes(self) -> bytes:
        """Get public key bytes (compatible with aries-askar Key interface).

        Returns:
            bytes: Public key bytes
        """
        return self._public_key

    def get_secret_bytes(self) -> bytes:
        """Get secret key bytes.

        Returns:
            bytes: Secret key bytes
        """
        return self._secret_key

    @property
    def key_type(self):
        """Get KeyType object.

        Returns:
            KeyType: Key type object
        """
        return self._key_type

    def sign_message(self, message: bytes) -> bytes:
        """Sign a message using ML-DSA-65.

        Args:
            message: Message to sign

        Returns:
            bytes: Signature

        Raises:
            ValueError: If this is not a signature key (ML-DSA-65)
        """
        if self.algorithm != "ml-dsa-65":
            raise ValueError(f"Cannot sign with {self.algorithm} key, only ml-dsa-65 supports signing")

        # Debug logging for signing inputs
        LOGGER.info(f"ML-DSA-65 Signing Inputs:")
        LOGGER.info(f"  message length: {len(message)} bytes")
        LOGGER.info(f"  message (first 100 bytes hex): {message[:100].hex()}")
        LOGGER.info(f"  public_key length: {len(self._public_key)} bytes")
        LOGGER.info(f"  secret_key length: {len(self._secret_key)} bytes")

        liboqs = get_liboqs()
        signature = liboqs.sign_ml_dsa_65(message, self._secret_key)

        LOGGER.info(f"ML-DSA-65 signature created: length={len(signature)} bytes (expected 3293)")
        return signature

    def __str__(self) -> str:
        """String representation."""
        return f"PQCKey(algorithm={self.algorithm})"

    def __repr__(self) -> str:
        """Detailed representation."""
        return (
            f"PQCKey(algorithm={self.algorithm}, "
            f"public_key_len={len(self._public_key)}, "
            f"secret_key_len={len(self._secret_key)})"
        )


def _create_keypair_pqc(key_type, seed=None, metadata=None):
    """Patched _create_keypair function with PQC support.

    This function intercepts key generation requests and handles PQC key types
    (ML-DSA-65, ML-KEM-768) via liboqs-python, while delegating classical key
    types (ED25519, X25519, etc.) to the original aries-askar implementation.

    Args:
        key_type: KeyType object or string
        seed: Optional seed for key generation (not used for PQC)
        metadata: Optional metadata

    Returns:
        PQCKey or aries_askar.Key: Key object
    """
    # Handle KeyType object or string
    if hasattr(key_type, 'key_type'):
        key_type_str = key_type.key_type
        key_type_obj = key_type
    else:
        key_type_str = str(key_type)
        key_type_obj = None

    LOGGER.debug(f"_create_keypair_pqc called with key_type={key_type_str}")

    # Handle ML-DSA-65 (Dilithium3)
    if key_type_str == "ml-dsa-65" or (key_type_obj and key_type_obj == ML_DSA_65):
        LOGGER.info("Generating ML-DSA-65 keypair via liboqs-python")
        liboqs = get_liboqs()
        public_key, secret_key = liboqs.generate_ml_dsa_65_keypair()

        return PQCKey(
            algorithm="ml-dsa-65",
            public_key=public_key,
            secret_key=secret_key,
            key_type_obj=ML_DSA_65,
        )

    # Handle ML-KEM-768 (Kyber768)
    elif key_type_str == "ml-kem-768" or (key_type_obj and key_type_obj == ML_KEM_768):
        LOGGER.info("Generating ML-KEM-768 keypair via liboqs-python")
        liboqs = get_liboqs()
        public_key, secret_key = liboqs.generate_ml_kem_768_keypair()

        return PQCKey(
            algorithm="ml-kem-768",
            public_key=public_key,
            secret_key=secret_key,
            key_type_obj=ML_KEM_768,
        )

    # Delegate to original aries-askar implementation for classical key types
    else:
        LOGGER.debug(f"Delegating to original _create_keypair for {key_type_str}")
        if _original_create_keypair is None:
            raise RuntimeError("Original _create_keypair not saved before patching")
        return _original_create_keypair(key_type, seed, metadata)


def patch_askar_insert_key():
    """Patch aries_askar.Session.insert_key() für PQC-Key-Storage.

    PQC-Keys (PQCKey) werden via generische Storage gespeichert,
    klassische Keys (aries_askar.Key) nutzen native FFI insert_key().

    Sicherheit: Identisch! Session.insert() nutzt dieselbe Verschlüsselung.
    """
    import json
    from aries_askar import Session

    # Speichere Original-Methode
    original_insert_key = Session.insert_key

    async def insert_key_pqc(self, name, key, *, metadata=None, tags=None, expiry_ms=None):
        """Patched insert_key mit PQC-Support."""

        # Check: Ist es ein PQC-Key?
        if isinstance(key, PQCKey):
            LOGGER.info(f"Storing PQC key '{name}' via generic storage (encrypted)")

            # Serialize PQC key data
            value = json.dumps({
                "algorithm": key.algorithm,
                "public_key": key._public_key.hex(),
                "secret_key": key._secret_key.hex(),
                "key_type": key._key_type.key_type
            })

            # Speichere via generische Storage (VERSCHLÜSSELT von Askar!)
            await self.insert(
                category="pqc_key",
                name=name,
                value=value,
                tags=tags or {},
                expiry_ms=expiry_ms
            )

            LOGGER.debug(f"PQC key '{name}' stored successfully")
            return name  # Return key name wie Original

        else:
            # Klassischer Askar Key: Delegiere an Original
            return await original_insert_key(self, name, key,
                                            metadata=metadata,
                                            tags=tags,
                                            expiry_ms=expiry_ms)

    # Ersetze Methode
    Session.insert_key = insert_key_pqc
    LOGGER.info("Patched Session.insert_key() for PQC support")


def patch_askar_fetch_key():
    """Patch aries_askar.Session.fetch_key() für PQC-Key-Retrieval."""
    import json
    from aries_askar import Session
    from .key_types import ML_DSA_65, ML_KEM_768

    # Speichere Original-Methode
    original_fetch_key = Session.fetch_key

    async def fetch_key_pqc(self, name, *, for_update=False):
        """Patched fetch_key mit PQC-Support."""

        # Versuche zuerst Original (klassische Keys)
        result = await original_fetch_key(self, name, for_update=for_update)

        if result is not None:
            return result  # Found in native storage

        # Not found in native storage, try PQC storage
        LOGGER.debug(f"Key '{name}' not found in native storage, trying PQC storage...")

        entry = await self.fetch(category="pqc_key", name=name, for_update=for_update)

        if entry:
            data = json.loads(entry.value)

            # Lookup KeyType
            key_type_map = {
                "ml-dsa-65": ML_DSA_65,
                "ml-kem-768": ML_KEM_768
            }
            key_type_obj = key_type_map.get(data["key_type"])

            if not key_type_obj:
                raise ValueError(f"Unknown PQC key type: {data['key_type']}")

            # Rekonstruiere PQCKey
            pqc_key = PQCKey(
                algorithm=data["algorithm"],
                public_key=bytes.fromhex(data["public_key"]),
                secret_key=bytes.fromhex(data["secret_key"]),
                key_type_obj=key_type_obj
            )

            LOGGER.debug(f"PQC key '{name}' retrieved successfully")
            return pqc_key

        # Not found anywhere - return None (standard behavior)
        LOGGER.debug(f"Key '{name}' not found in native or PQC storage")
        return None

    # Ersetze Methode
    Session.fetch_key = fetch_key_pqc
    LOGGER.info("Patched Session.fetch_key() for PQC support")


def patch_askar_update_key():
    """Patch aries_askar.Session.update_key() für PQC-Key-Updates.

    PQC keys (PQCKey) werden via Session.replace() aktualisiert,
    klassische Keys nutzen native FFI update_key().

    Dies ermöglicht assign_kid_to_key() für PQC-Keys zu funktionieren.
    """
    import json
    from aries_askar import Session
    from .key_types import ML_DSA_65, ML_KEM_768

    # Speichere Original-Methode
    original_update_key = Session.update_key

    async def update_key_pqc(self, name, *, metadata=None, tags=None):
        """Patched update_key mit PQC-Support."""

        # Versuche zuerst PQC Storage (falls es ein PQC-Key ist)
        try:
            entry = await self.fetch(category="pqc_key", name=name, for_update=True)

            if entry:
                # Es ist ein PQC-Key! Update via replace()
                LOGGER.debug(f"Updating PQC key '{name}' metadata via generic storage")

                # Parse existing data
                data = json.loads(entry.value)

                # Store metadata directly in the PQC key data structure
                # (metadata is stored as part of tags, not in the JSON value)
                # Just need to update tags, value stays the same

                # Update via replace() - keep value, update tags
                await self.replace(
                    category="pqc_key",
                    name=name,
                    value=entry.value,  # Keep original value unchanged
                    tags=tags if tags is not None else entry.tags
                )

                LOGGER.debug(f"PQC key '{name}' metadata updated successfully")
                return name
        except Exception:
            # Nicht gefunden in PQC storage, könnte klassischer Key sein
            pass

        # Klassischer Askar Key: Delegiere an Original
        return await original_update_key(self, name, metadata=metadata, tags=tags)

    # Ersetze Methode
    Session.update_key = update_key_pqc
    LOGGER.info("Patched Session.update_key() for PQC support")


def patch_askar_assign_kid():
    """Patch acapy_agent.wallet.askar.AskarWallet.assign_kid_to_key() für PQC-Keys.

    KID (Key Identifier) ist essentiell für:
    - DIDComm: Richtigen Schlüssel für Entschlüsselung identifizieren
    - Signaturen: Verifikation weiß welchen Key nutzen
    - Multi-Key-DIDs: Unterscheidung zwischen #key-1 und #key-2

    PQC keys speichern KID in tags via Session.replace().
    """
    import json
    from acapy_agent.wallet.askar import AskarWallet

    # Speichere Original-Methode
    original_assign_kid = AskarWallet.assign_kid_to_key

    async def assign_kid_to_key_pqc(self, verkey: str, kid: str):
        """Patched assign_kid_to_key mit PQC-Support."""

        # Check ob es ein PQC-Key ist (via self._session.handle)
        try:
            entry = await self._session.handle.fetch(
                category="pqc_key",
                name=verkey,
                for_update=True
            )

            if entry:
                # Es ist ein PQC-Key! Speichere KID in tags
                LOGGER.info(f"Assigning KID '{kid}' to PQC key '{verkey[:20]}...'")

                # Update tags mit KID
                existing_tags = entry.tags or {}
                existing_tags["kid"] = kid

                # Update via replace()
                await self._session.handle.replace(
                    category="pqc_key",
                    name=verkey,
                    value=entry.value,  # Keep value unchanged
                    tags=existing_tags
                )

                LOGGER.debug(f"KID '{kid}' assigned to PQC key successfully")
                return  # Success!

        except Exception:
            # Nicht in PQC storage gefunden, könnte klassischer Key sein
            pass

        # Klassischer Askar Key: Delegiere an Original
        return await original_assign_kid(self, verkey, kid)

    # Ersetze Methode
    AskarWallet.assign_kid_to_key = assign_kid_to_key_pqc
    LOGGER.info("Patched AskarWallet.assign_kid_to_key() for PQC support")


def patch_askar_create_keypair():
    """Apply monkey patch to aries-askar's _create_keypair function.

    This function replaces acapy_agent.wallet.askar._create_keypair with our
    PQC-aware version that can generate ML-DSA-65 and ML-KEM-768 keys via
    liboqs-python while maintaining compatibility with classical key types.

    ALSO patches Session.insert_key() and Session.fetch_key() for PQC storage.
    """
    global _original_create_keypair

    try:
        import acapy_agent.wallet.askar as askar_module

        # Save original function if not already saved
        if _original_create_keypair is None:
            _original_create_keypair = askar_module._create_keypair
            LOGGER.info("Saved original _create_keypair function")

        # Replace with PQC-aware version
        askar_module._create_keypair = _create_keypair_pqc
        LOGGER.info("Successfully patched askar._create_keypair for PQC support")

        # Patch Storage-Layer für PQC-Keys
        patch_askar_insert_key()
        patch_askar_fetch_key()
        patch_askar_update_key()
        patch_askar_assign_kid()

    except ImportError as e:
        LOGGER.error(f"Failed to import acapy_agent.wallet.askar: {e}")
        raise
    except AttributeError as e:
        LOGGER.error(f"Failed to find _create_keypair in askar module: {e}")
        raise
    except Exception as e:
        LOGGER.error(f"Unexpected error while patching askar: {e}")
        raise


def patch_askar_pack_unpack():
    """Patch AskarWallet.pack_message() and unpack_message() for PQC support.

    DIDComm v1's default pack/unpack implementation is hardcoded for
    ED25519/X25519 crypto_box. This patch replaces them with PQC-aware
    versions that support ML-KEM-768 + ML-DSA-65.

    The patched methods automatically detect PQC vs classical keys and
    use the appropriate algorithm.
    """
    from acapy_agent.wallet.askar import AskarWallet
    from .pqc_didcomm_v1 import pack_message_pqc, unpack_message_pqc

    # Save original methods
    original_pack = AskarWallet.pack_message
    original_unpack = AskarWallet.unpack_message

    async def pack_message_patched(self, message: str, to_verkeys, from_verkey=None):
        """Patched pack_message with PQC support.

        Args:
            message: Message to pack
            to_verkeys: List of recipient verkeys
            from_verkey: Sender verkey (optional for anoncrypt)

        Returns:
            bytes: Packed JWE message
        """
        from acapy_agent.wallet.error import WalletError, WalletNotFoundError

        if message is None:
            raise WalletError("Message not provided")

        try:
            # Fetch sender key if provided
            from_key = None
            if from_verkey:
                # Try native storage first (for classical keys)
                from_key_entry = await self._session.handle.fetch_key(from_verkey)

                if from_key_entry:
                    # Handle both KeyEntry wrapper (classical) and direct PQCKey
                    if isinstance(from_key_entry, PQCKey):
                        from_key = from_key_entry  # Already a PQCKey
                    else:
                        from_key = from_key_entry.key  # KeyEntry wrapper
                else:
                    # Not found in native storage, try PQC storage
                    LOGGER.debug(f"Key not found in native storage, trying PQC storage: {from_verkey[:20]}...")

                    entry = await self._session.handle.fetch(
                        category="pqc_key",
                        name=from_verkey,
                        for_update=False
                    )

                    if entry:
                        # Reconstruct PQCKey from stored data
                        import json
                        from .key_types import ML_DSA_65, ML_KEM_768

                        data = json.loads(entry.value)

                        key_type_map = {
                            "ml-dsa-65": ML_DSA_65,
                            "ml-kem-768": ML_KEM_768
                        }
                        key_type_obj = key_type_map.get(data["key_type"])

                        if not key_type_obj:
                            raise ValueError(f"Unknown PQC key type: {data['key_type']}")

                        from_key = PQCKey(
                            algorithm=data["algorithm"],
                            public_key=bytes.fromhex(data["public_key"]),
                            secret_key=bytes.fromhex(data["secret_key"]),
                            key_type_obj=key_type_obj
                        )
                        LOGGER.info(f"Retrieved PQC key from storage: {data['algorithm']}")
                    else:
                        raise WalletNotFoundError("Missing key for pack operation")

            # Use PQC-aware pack (detects PQC vs classical automatically)
            return await pack_message_pqc(
                self._session.handle,
                to_verkeys,
                from_key,
                message
            )

        except Exception as err:
            LOGGER.error(f"Exception when packing message: {err}", exc_info=True)
            raise WalletError("Exception when packing message") from err

    async def unpack_message_patched(self, enc_message: bytes):
        """Patched unpack_message with PQC support.

        Args:
            enc_message: Packed JWE message

        Returns:
            Tuple[str, str, str]: (message, from_verkey, to_verkey)
        """
        from acapy_agent.wallet.error import WalletError

        if not enc_message:
            raise WalletError("Message not provided")

        try:
            # Use PQC-aware unpack (detects PQC vs classical automatically)
            return await unpack_message_pqc(
                self._session.handle,
                enc_message
            )

        except Exception as err:
            LOGGER.error(f"Exception when unpacking message: {err}", exc_info=True)
            raise WalletError("Exception when unpacking message") from err

    # Store original sign_message method
    original_sign_message = AskarWallet.sign_message

    async def sign_message_patched(self, message: bytes, from_verkey: str):
        """Patched sign_message with PQCKey handling.

        Args:
            message: Message to sign
            from_verkey: Verkey of signing key

        Returns:
            bytes: Signature
        """
        from acapy_agent.wallet.error import WalletError, WalletNotFoundError

        try:
            # Fetch key (may return KeyEntry wrapper or direct PQCKey)
            keypair = await self._session.handle.fetch_key(from_verkey)

            if not keypair:
                raise WalletNotFoundError(f"Key not found: {from_verkey}")

            # Handle both KeyEntry wrapper (classical) and direct PQCKey
            if isinstance(keypair, PQCKey):
                key = keypair  # Already a PQCKey
                LOGGER.debug("Using PQC key for signing")
            else:
                key = keypair.key  # KeyEntry wrapper
                LOGGER.debug("Using classical key for signing")

            # Use the key's sign method
            return key.sign_message(message)

        except WalletNotFoundError:
            raise
        except Exception as err:
            LOGGER.error(f"Exception when signing message: {err}", exc_info=True)
            raise WalletError("Exception when signing message") from err

    # Store original verify_message method
    original_verify_message = AskarWallet.verify_message

    async def verify_message_patched(
        self,
        message: bytes,
        signature: bytes,
        verkey: bytes,
        key_type=None,
        did=None,
    ):
        """Patched verify_message with PQC signature verification.

        Args:
            message: Original message
            signature: Signature to verify (may be base58-encoded or raw bytes)
            verkey: Public key (may be base58-encoded or raw bytes)
            key_type: Key type (optional)
            did: DID (optional)

        Returns:
            bool: True if signature is valid, False otherwise
        """
        from acapy_agent.wallet.error import WalletError

        try:
            # Handle both base58-encoded strings and raw bytes
            if isinstance(verkey, str):
                from base58 import b58decode
                verkey_bytes = b58decode(verkey)
                LOGGER.debug(f"Decoded base58 verkey: {len(verkey)} chars --> {len(verkey_bytes)} bytes")
            else:
                verkey_bytes = verkey

            verkey_len = len(verkey_bytes)

            # ML-DSA-65 public keys are 1952 bytes (raw)
            if verkey_len == 1952:
                LOGGER.info(f"Detected ML-DSA-65 key (verkey_len={verkey_len}), using PQC verification")

                # Decode signature if base58-encoded
                if isinstance(signature, str):
                    from base58 import b58decode
                    signature_bytes = b58decode(signature)
                    LOGGER.debug(f"Decoded base58 signature: {len(signature)} chars --> {len(signature_bytes)} bytes")
                else:
                    signature_bytes = signature

                # Debug logging for verification inputs
                LOGGER.info(f"ML-DSA-65 Verification Inputs:")
                LOGGER.info(f"  message length: {len(message)} bytes")
                LOGGER.info(f"  message (first 100 bytes hex): {message[:100].hex()}")
                LOGGER.info(f"  signature_bytes length: {len(signature_bytes)} bytes")
                LOGGER.info(f"  verkey_bytes length: {len(verkey_bytes)} bytes")
                LOGGER.info(f"  Expected ML-DSA-65 signature: 3293 bytes")

                liboqs = get_liboqs()
                is_valid = liboqs.verify_ml_dsa_65(message, signature_bytes, verkey_bytes)

                LOGGER.info(f"ML-DSA-65 signature verification result: {is_valid}")
                if not is_valid:
                    LOGGER.error(f"ML-DSA-65 signature verification FAILED!")
                    LOGGER.error(f"  This could indicate:")
                    LOGGER.error(f"    - Message was preprocessed differently during signing")
                    LOGGER.error(f"    - Signature format mismatch")
                    LOGGER.error(f"    - Wrong public key")

                return is_valid

            # Classical key (ED25519 = 32 bytes) - use original verification
            else:
                LOGGER.debug(f"Detected classical key (verkey_len={verkey_len}), using original verification")
                return await original_verify_message(self, message, signature, verkey, key_type)

        except Exception as err:
            LOGGER.error(f"Exception when verifying message signature: {err}", exc_info=True)
            raise WalletError("Exception when verifying message signature") from err

    # Apply patches
    AskarWallet.pack_message = pack_message_patched
    AskarWallet.unpack_message = unpack_message_patched
    AskarWallet.sign_message = sign_message_patched
    AskarWallet.verify_message = verify_message_patched

    LOGGER.info("Patched AskarWallet.pack_message() for PQC support")
    LOGGER.info("Patched AskarWallet.unpack_message() for PQC support")
    LOGGER.info("Patched AskarWallet.sign_message() for PQC support")
    LOGGER.info("Patched AskarWallet.verify_message() for PQC support")


def patch_attach_decorator_for_pqc():
    """Patch AttachDecoratorData to support PQC signatures in JWS format.

    The DID Exchange protocol uses AttachDecoratorData with JWS (JSON Web Signature)
    to sign DID documents. The original implementation is hardcoded for ED25519.
    This patch adds support for ML-DSA-65 while maintaining backward compatibility.
    """
    from acapy_agent.messaging.decorators.attach_decorator import (
        AttachDecoratorData,
        AttachDecoratorDataJWS,
        AttachDecoratorDataJWSHeader,
    )
    from acapy_agent.wallet.util import (
        b58_to_bytes,
        b64_to_bytes,
        b64_to_str,
        bytes_to_b58,
        bytes_to_b64,
        set_urlsafe_b64,
        str_to_b64,
        unpad,
    )
    from acapy_agent.wallet.key_type import ED25519
    import json

    # Save original methods
    original_sign = AttachDecoratorData.sign
    original_verify = AttachDecoratorData.verify

    async def sign_patched(self, verkeys, wallet):
        """Patched sign method supporting both ED25519 and ML-DSA-65.

        For ED25519 keys (32 bytes): Use original JWS format with Ed25519/OKP
        For ML-DSA-65 keys (1952 bytes): Use custom PQC JWS format
        """
        from acapy_agent.did.did_key import DIDKey

        # Helper to check if key is PQC based on length
        def is_pqc_key(verkey_b58: str) -> bool:
            """Check if a Base58 verkey is PQC (ML-DSA-65 = 1952 bytes)."""
            try:
                key_bytes = b58_to_bytes(verkey_b58)
                return len(key_bytes) == 1952  # ML-DSA-65 public key size
            except Exception:
                return False

        def build_protected_ed25519(verkey: str):
            """Build protected header for ED25519 (original format)."""
            return str_to_b64(
                json.dumps({
                    "alg": "EdDSA",
                    "jwk": {
                        "kty": "OKP",
                        "crv": "Ed25519",
                        "x": bytes_to_b64(
                            b58_to_bytes(verkey), urlsafe=True, pad=False
                        ),
                        "kid": DIDKey.from_public_key_b58(verkey, ED25519).did,
                    },
                }),
                urlsafe=True,
                pad=False,
            )

        def build_protected_pqc(verkey: str, kid: str):
            """Build protected header for ML-DSA-65 (PQC format).

            Args:
                verkey: Base58 encoded public key
                kid: Key identifier in DID URL format (e.g., did:peer:4zQm...#key-1)
            """
            return str_to_b64(
                json.dumps({
                    "alg": "ML-DSA-65",
                    "jwk": {
                        "kty": "OQP",  # Quantum-resistant OKP
                        "crv": "ML-DSA-65",
                        "x_pqc": bytes_to_b64(
                            b58_to_bytes(verkey), urlsafe=True, pad=False
                        ),
                        # Use proper DID URL format for kid
                        "kid": kid,
                    },
                }),
                urlsafe=True,
                pad=False,
            )

        assert self.base64
        b64_payload = unpad(set_urlsafe_b64(self.base64, True))

        # Handle single verkey or list with one element
        if isinstance(verkeys, str) or (isinstance(verkeys, list) and len(verkeys) == 1):
            verkey = verkeys if isinstance(verkeys, str) else verkeys[0]

            if is_pqc_key(verkey):
                LOGGER.info(f"Using PQC signature format for ML-DSA-65 key")

                # Retrieve KID from PQC storage tags (stored by assign_kid_to_key_pqc)
                try:
                    entry = await wallet._session.handle.fetch(
                        category="pqc_key",
                        name=verkey
                    )
                    if entry and entry.tags:
                        kid = entry.tags.get("kid", verkey)
                        LOGGER.debug(f"Retrieved KID from PQC storage: {kid}")
                    else:
                        kid = verkey
                        LOGGER.warning(f"No KID found in PQC storage tags, using verkey")
                except Exception as e:
                    LOGGER.warning(f"Could not retrieve KID from PQC storage: {e}, using verkey")
                    kid = verkey  # Fallback to verkey if lookup fails

                b64_protected = build_protected_pqc(verkey, kid)
            else:
                LOGGER.debug(f"Using ED25519 signature format for classical key")
                b64_protected = build_protected_ed25519(verkey)
                kid = DIDKey.from_public_key_b58(verkey, ED25519).did

            # Sign the message (wallet.sign_message handles both ED25519 and ML-DSA-65)
            signature_bytes = await wallet.sign_message(
                message=(b64_protected + "." + b64_payload).encode("ascii"),
                from_verkey=verkey,
            )

            b64_sig = bytes_to_b64(signature_bytes, urlsafe=True, pad=False)

            self.jws_ = AttachDecoratorDataJWS.deserialize({
                "header": AttachDecoratorDataJWSHeader(kid).serialize(),
                "protected": b64_protected,
                "signature": b64_sig,
            })
        else:
            # Multi-signature case
            jws = {"signatures": []}
            for verkey in verkeys:
                if is_pqc_key(verkey):
                    # Retrieve KID from PQC storage tags for each PQC key
                    try:
                        entry = await wallet._session.handle.fetch(
                            category="pqc_key",
                            name=verkey
                        )
                        if entry and entry.tags:
                            kid = entry.tags.get("kid", verkey)
                            LOGGER.debug(f"Retrieved KID from PQC storage: {kid}")
                        else:
                            kid = verkey
                            LOGGER.warning(f"No KID found in PQC storage tags, using verkey")
                    except Exception as e:
                        LOGGER.warning(f"Could not retrieve KID from PQC storage: {e}, using verkey")
                        kid = verkey

                    b64_protected = build_protected_pqc(verkey, kid)
                else:
                    b64_protected = build_protected_ed25519(verkey)
                    kid = DIDKey.from_public_key_b58(verkey, ED25519).did

                signature_bytes = await wallet.sign_message(
                    message=(b64_protected + "." + b64_payload).encode("ascii"),
                    from_verkey=verkey,
                )

                b64_sig = bytes_to_b64(signature_bytes, urlsafe=True, pad=False)

                jws["signatures"].append({
                    "protected": b64_protected,
                    "header": {"kid": kid},
                    "signature": b64_sig,
                })

            self.jws_ = AttachDecoratorDataJWS.deserialize(jws)

    async def verify_patched(self, wallet, signer_verkey=None):
        """Patched verify method supporting both ED25519 and ML-DSA-65.

        For ED25519: Use original verification with DIDKey resolution
        For ML-DSA-65: Extract raw verkey from JWK and verify directly
        """
        from acapy_agent.did.did_key import DIDKey

        assert self.jws

        b64_payload = unpad(set_urlsafe_b64(self.base64, True))
        verkey_to_check = []

        for sig in [self.jws] if self.signatures == 1 else self.jws.signatures:
            b64_protected = sig.protected
            b64_sig = sig.signature
            protected = json.loads(b64_to_str(b64_protected, urlsafe=True))

            sign_input = (b64_protected + "." + b64_payload).encode("ascii")
            b_sig = b64_to_bytes(b64_sig, urlsafe=True)

            # Check if this is a PQC signature
            jwk = protected.get("jwk", {})
            alg = protected.get("alg")

            if alg == "ML-DSA-65" and jwk.get("kty") == "OQP":
                # PQC verification path
                LOGGER.info("Verifying ML-DSA-65 signature from AttachDecorator")

                # Extract PQC public key from x_pqc field
                x_pqc_b64 = jwk.get("x_pqc")
                if not x_pqc_b64:
                    LOGGER.error("PQC signature missing x_pqc field in JWK")
                    return False

                verkey_bytes = b64_to_bytes(x_pqc_b64, urlsafe=True)
                verkey_b58 = bytes_to_b58(verkey_bytes)

                LOGGER.debug(f"Extracted ML-DSA-65 verkey: {len(verkey_bytes)} bytes")

                # Verify using wallet (which will auto-detect ML-DSA-65 from key length)
                if not await wallet.verify_message(
                    sign_input, b_sig, verkey_b58, None  # None = auto-detect
                ):
                    LOGGER.error("ML-DSA-65 signature verification failed")
                    return False

                # Track verkey for signer check
                verkey_to_check.append(verkey_b58)

            elif jwk.get("kty") == "OKP" and jwk.get("crv") == "Ed25519":
                # Classical ED25519 verification path (original logic)
                LOGGER.debug("Verifying ED25519 signature from AttachDecorator")

                verkey = bytes_to_b58(b64_to_bytes(jwk["x"], urlsafe=True))

                if not await wallet.verify_message(sign_input, b_sig, verkey, ED25519):
                    return False

                # Also verify using DIDKey if kid is present
                if "kid" in jwk:
                    encoded_pk = DIDKey.from_did(jwk["kid"]).public_key_b58
                    verkey_to_check.append(encoded_pk)
                    if not await wallet.verify_message(
                        sign_input, b_sig, encoded_pk, ED25519
                    ):
                        return False
            else:
                LOGGER.error(f"Unknown signature algorithm: {alg}, kty={jwk.get('kty')}")
                return False

        # Check if expected signer matches
        if signer_verkey and signer_verkey not in verkey_to_check:
            # CRITICAL FIX for did:peer:4 with PQC:
            # The signer_verkey parameter is often the invitation_key, which is different
            # from the did:peer:4 authentication key. We need to check if the signature
            # was created by a key from the DID Document.

            # Try to resolve the kid from the JWS header to get the DID
            kid = None
            for sig in [self.jws] if self.signatures == 1 else self.jws.signatures:
                b64_protected = sig.protected
                protected = json.loads(b64_to_str(b64_protected, urlsafe=True))
                jwk = protected.get("jwk", {})
                if "kid" in jwk:
                    kid = jwk["kid"]
                    break
                # Also check header.kid
                if hasattr(sig, 'header') and sig.header and hasattr(sig.header, 'kid'):
                    kid = sig.header.kid
                    break

            if kid and kid.startswith("did:peer:4"):
                # Extract the DID from the kid (remove fragment)
                did = kid.split("#")[0]
                LOGGER.info(f"PQC Fix: Signature from did:peer:4, resolving DID to get authentication key")
                LOGGER.debug(f"  kid: {kid}")
                LOGGER.debug(f"  DID: {did}")
                LOGGER.debug(f"  invitation_key (signer_verkey): {signer_verkey[:20]}...")
                LOGGER.debug(f"  Actual signer from JWS: {verkey_to_check[0][:20] if verkey_to_check else 'None'}...")

                # For did:peer:4, the verkey in verkey_to_check is the authentication key
                # from the DID Document, which is the CORRECT key to check.
                # The invitation_key might be from the out-of-band invitation and could
                # be different. We should trust the signature verification result.
                LOGGER.info("Accepting did:peer:4 signature (cryptographic verification passed)")
                return True

            LOGGER.warning(f"Signer verkey {signer_verkey} not in verified keys: {verkey_to_check}")
            return False

        return True

    # Apply patches
    AttachDecoratorData.sign = sign_patched
    AttachDecoratorData.verify = verify_patched

    LOGGER.info("Patched AttachDecoratorData.sign() for PQC support")
    LOGGER.info("Patched AttachDecoratorData.verify() for PQC support")


def unpatch_askar_create_keypair():
    """Remove monkey patch and restore original _create_keypair function.

    This function is provided for testing purposes or if the plugin needs to be
    unloaded. In normal operation, the patch should remain active.
    """
    global _original_create_keypair

    if _original_create_keypair is None:
        LOGGER.warning("No original _create_keypair to restore")
        return

    try:
        import acapy_agent.wallet.askar as askar_module
        askar_module._create_keypair = _original_create_keypair
        LOGGER.info("Restored original _create_keypair function")
        _original_create_keypair = None
    except Exception as e:
        LOGGER.error(f"Failed to unpatch askar: {e}")
        raise
\end{lstlisting}

\subsubsection{wallet\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:wallet_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - wallet\_patch.py}, numbers=left, frame=single]
"""Patch for acapy_agent/wallet/askar.py to support PQC key lookup.

This patch extends AskarWallet.get_local_did_for_verkey to handle did:peer:4 DIDs
with multiple keys (ML-DSA-65 for authentication, ML-KEM-768 for key agreement).

CRITICAL FIX for connection lookup during message handling:
- DIDs are stored with verkey=ML-DSA-65 (authentication key, #key-0)
- DIDComm encrypts messages with ML-KEM-768 (keyAgreement key, #key-1)
- When credential offer arrives, receipt.recipient_verkey = ML-KEM-768
- Original wallet.get_local_did_for_verkey(ML-KEM-768) throws WalletNotFoundError
- Connection lookup fails --> "No connection found" error!

This patch:
1. Tries the original lookup (searches for verkey in tags)
2. On WalletNotFoundError: Searches for DIDs where metadata.kem_verkey == verkey
3. Returns the DID if found

This enables connection lookup for incoming DIDComm messages encrypted with ML-KEM-768!
"""

import logging
from typing import TYPE_CHECKING

from acapy_agent.wallet.error import WalletNotFoundError
from acapy_agent.wallet.did_info import DIDInfo
from acapy_agent.wallet.key_type import KeyType

if TYPE_CHECKING:
    from acapy_agent.wallet.askar import AskarWallet

LOGGER = logging.getLogger(__name__)


async def get_local_did_for_verkey_pqc(self: "AskarWallet", verkey: str) -> DIDInfo:
    """Resolve a local DID from a verkey (PQC-aware version).

    This is a patched version of AskarWallet.get_local_did_for_verkey that handles
    PQC did:peer:4 DIDs with multiple keys.

    CRITICAL DIFFERENCE from original (askar.py:462-491):
    - Original: Only searches for DIDs with verkey tag matching the input
    - This version: ALSO searches for DIDs with metadata.kem_verkey matching the input

    This is necessary because:
    - did:peer:4 has TWO keys: ML-DSA-65 (auth) and ML-KEM-768 (keyAgr)
    - DIDs are stored with verkey=ML-DSA-65 tag (only one verkey field in DIDInfo!)
    - DIDComm encrypts with ML-KEM-768 (recipient key for decryption)
    - When message arrives: receipt.recipient_verkey = ML-KEM-768
    - Original method: fetch_all(CATEGORY_DID, {"verkey": ML-KEM-768}) --> NOT FOUND!
    - This patch: fetch_all by kem_verkey in metadata --> FOUND!

    Args:
        self: AskarWallet instance
        verkey: The verkey for which to get the local DID (can be ML-DSA-65 OR ML-KEM-768)

    Returns:
        A `DIDInfo` instance representing the found DID

    Raises:
        WalletNotFoundError: If the verkey is not found (neither as primary nor as KEM key)
    """
    from aries_askar import AskarError

    LOGGER.error(f"[PQC Patch] ====== WALLET PATCH CALLED ====== verkey: {verkey[:30]}...")

    # Step 1: Try the original lookup (search by verkey tag)
    # This handles classical keys (ED25519) and PQC authentication keys (ML-DSA-65)
    try:
        LOGGER.error(f"[PQC Patch] Trying primary verkey lookup...")
        dids = await self._session.handle.fetch_all("did", {"verkey": verkey})
    except AskarError as err:
        from acapy_agent.wallet.error import WalletError
        raise WalletError("Error when fetching local DID for verkey") from err

    if dids:
        LOGGER.error(f"[PQC Patch] Found {len(dids)} DID(s) with primary verkey --> returning first")
        ret_did = dids[0]
        ret_did_info = ret_did.value_json
        # Handle long/short form preference (original logic from askar.py:483-489)
        if len(dids) > 1 and ret_did_info["did"].startswith("did:peer:4"):
            other_did = dids[1]
            other_did_info = other_did.value_json
            if len(other_did_info["did"]) < len(ret_did_info["did"]):
                ret_did = other_did
                ret_did_info = other_did.value_json
        return self._load_did_entry(ret_did)

    # Step 2: Primary verkey not found - check if it's a KEM verkey for a PQC DID
    LOGGER.error(f"[PQC Patch] Primary verkey NOT FOUND --> Checking for KEM verkey in metadata...")

    try:
        # Fetch ALL DIDs (we need to check metadata, which isn't indexed)
        all_dids = await self._session.handle.fetch_all("did")
    except AskarError as err:
        from acapy_agent.wallet.error import WalletError
        raise WalletError("Error when fetching DIDs for KEM verkey lookup") from err

    # Search for DIDs with kem_verkey in metadata matching our verkey
    for did_entry in all_dids:
        did_info_json = did_entry.value_json
        # metadata is stored INSIDE value_json (not as separate Entry attribute!)
        metadata = did_info_json.get("metadata", {})

        # Check if this DID has a kem_verkey that matches our search
        if metadata.get("kem_verkey") == verkey:
            LOGGER.error(f"[PQC Patch] FOUND DID WITH KEM VERKEY")
            LOGGER.error(f"[PQC Patch]    DID: {did_info_json['did'][:60]}...")
            LOGGER.error(f"[PQC Patch]    Primary verkey (ML-DSA-65): {did_info_json['verkey'][:30]}...")
            LOGGER.error(f"[PQC Patch]    KEM verkey (ML-KEM-768): {verkey[:30]}...")
            return self._load_did_entry(did_entry)

    # Step 3: Neither primary nor KEM verkey found
    LOGGER.error(f"[PQC Patch] NO DID FOUND (neither primary nor KEM)")
    LOGGER.error(f"[PQC Patch] Searched {len(all_dids)} DIDs in wallet - none matched!")
    raise WalletNotFoundError(f"No DID defined for verkey: {verkey}")
\end{lstlisting}

\subsubsection{connection\_target\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:connection_target_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - connection\_target\_patch.py}, numbers=left, frame=single]
"""Patch for ConnectionTarget schema validation to accept PQC keys.

This patch extends the ConnectionTargetSchema to accept both classical ED25519 keys
and Post-Quantum keys (ML-DSA-65, ML-KEM-768).

CRITICAL FIX for credential issuance:
- Original schema validates recipient_keys, routing_keys, and sender_key as ED25519
- Validation pattern: ^[1-9A-HJ-NP-Za-km-z]{43,44}$ (43-44 base58 chars)
- PQC keys have different byte lengths and base58 representations:
  - ML-DSA-65: 1952 bytes --> ~2650 base58 chars
  - ML-KEM-768: 1184 bytes --> ~1600 base58 chars
- Original validation fails with: "Value X is not a raw Ed25519VerificationKey2018 key"

This patch:
1. Creates a new validator that accepts both ED25519 and PQC key formats
2. Patches the ConnectionTargetSchema fields to use the new validator
3. Preserves backward compatibility with ED25519 workflows

Applied during plugin initialization in __init__.py setup().
"""

import logging
import re
from marshmallow.validate import Regexp
from base58 import alphabet

LOGGER = logging.getLogger(__name__)

# Base58 character set (from acapy_agent/messaging/valid.py:16)
B58 = alphabet if isinstance(alphabet, str) else alphabet.decode("ascii")


class RawPublicKeyAnyAlgorithm(Regexp):
    """Validate value against raw public key (any supported algorithm).

    This validator accepts:
    1. ED25519 keys (43-44 base58 characters)
    2. ML-DSA-65 keys (~2650 base58 characters)
    3. ML-KEM-768 keys (~1600 base58 characters)

    Pattern: Any non-empty base58 string (minimum 32 chars to prevent trivial inputs)
    """

    EXAMPLE = "H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV"  # ED25519 example
    # Accept any base58 string with reasonable length (32+ chars to prevent abuse)
    PATTERN = rf"^[{B58}]{{32,}}$"

    def __init__(self):
        """Initialize the instance."""
        super().__init__(
            RawPublicKeyAnyAlgorithm.PATTERN,
            error="Value {input} is not a valid raw public key (must be base58, 32+ chars)",
        )


# Create validator instance
RAW_PUBLIC_KEY_ANY_ALGORITHM_VALIDATE = RawPublicKeyAnyAlgorithm()


def patch_connection_target_schema():
    """Patch ConnectionTargetSchema to accept PQC keys.

    This function:
    1. Imports the ConnectionTargetSchema class
    2. Replaces the validators for recipient_keys, routing_keys, and sender_key
    3. Uses the new RawPublicKeyAnyAlgorithm validator that accepts both ED25519 and PQC

    IMPORTANT: This must be called AFTER acapy_agent is imported but BEFORE
    any ConnectionTarget objects are deserialized.
    """
    from acapy_agent.connections.models.connection_target import ConnectionTargetSchema
    from marshmallow import fields

    LOGGER.info("Patching ConnectionTargetSchema to accept PQC keys...")

    # Store original field definitions for logging
    original_recipient_keys = ConnectionTargetSchema._declared_fields.get('recipient_keys')
    original_routing_keys = ConnectionTargetSchema._declared_fields.get('routing_keys')
    original_sender_key = ConnectionTargetSchema._declared_fields.get('sender_key')

    LOGGER.debug(f"  Original recipient_keys validator: {original_recipient_keys}")
    LOGGER.debug(f"  Original routing_keys validator: {original_routing_keys}")
    LOGGER.debug(f"  Original sender_key validator: {original_sender_key}")

    # Patch recipient_keys field
    ConnectionTargetSchema._declared_fields['recipient_keys'] = fields.List(
        fields.Str(
            validate=RAW_PUBLIC_KEY_ANY_ALGORITHM_VALIDATE,
            metadata={
                "description": "Recipient public key (ED25519, ML-DSA-65, or ML-KEM-768)",
                "example": RawPublicKeyAnyAlgorithm.EXAMPLE,
            },
        ),
        required=False,
        metadata={"description": "List of recipient keys"},
    )

    # Patch routing_keys field
    ConnectionTargetSchema._declared_fields['routing_keys'] = fields.List(
        fields.Str(
            validate=RAW_PUBLIC_KEY_ANY_ALGORITHM_VALIDATE,
            metadata={
                "description": "Routing key (ED25519, ML-DSA-65, or ML-KEM-768)",
                "example": RawPublicKeyAnyAlgorithm.EXAMPLE,
            },
        ),
        data_key="routingKeys",
        required=False,
        metadata={"description": "List of routing keys"},
    )

    # Patch sender_key field
    ConnectionTargetSchema._declared_fields['sender_key'] = fields.Str(
        required=False,
        validate=RAW_PUBLIC_KEY_ANY_ALGORITHM_VALIDATE,
        metadata={
            "description": "Sender public key (ED25519, ML-DSA-65, or ML-KEM-768)",
            "example": RawPublicKeyAnyAlgorithm.EXAMPLE,
        },
    )

    LOGGER.info("recipient_keys field patched to accept PQC keys")
    LOGGER.info("routing_keys field patched to accept PQC keys")
    LOGGER.info("sender_key field patched to accept PQC keys")
    LOGGER.info("     New pattern: base58 strings with 32+ characters")
    LOGGER.info("     Accepts: ED25519 (43-44 chars), ML-DSA-65 (~2650 chars), ML-KEM-768 (~1600 chars)")
\end{lstlisting}

\subsubsection{validator\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:validator_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - validator\_patch.py}, numbers=left, frame=single]
"""Patch JWSHeaderKid validator to accept did:peer:4 format.

This patch extends the JWSHeaderKid validator in ACA-Py to accept did:peer:4
DID URLs with fragments (e.g., did:peer:4zQm...#key-1), which are used in
JWS header 'kid' fields during DID Exchange protocol.

The original validator only accepts:
- did:key:z[base58]+
- did:sov:[base58]{21-22}#...

The patched validator also accepts:
- did:peer:2...#key-X (existing peer method)
- did:peer:3...#key-X (existing peer method)
- did:peer:4...#key-X (new peer method with PQC support)

This patch is necessary because did:peer:4 DIDs can be very long when they
contain Post-Quantum Cryptography keys (ML-DSA-65 public keys are 1952 bytes,
resulting in did:peer:4 DIDs that are thousands of characters long when
multibase-encoded).
"""

import logging
from acapy_agent.messaging.valid import JWSHeaderKid, B58
import acapy_agent.messaging.valid as valid_module

LOGGER = logging.getLogger(__name__)


def patch_jws_header_kid_for_peer4():
    """Patch JWSHeaderKid validator to accept did:peer:4 DIDs.

    This function monkey-patches the JWSHeaderKid validator by:
    1. Updating the JWSHeaderKid.PATTERN class variable
    2. Creating a NEW validator instance with the updated pattern
    3. Replacing the global JWS_HEADER_KID_VALIDATE instance
    4. Patching all modules that cached the old validator instance

    The regex pattern is compiled in __init__, so we MUST create a new instance
    after changing the pattern, otherwise the old compiled regex will be used.

    CRITICAL: Python's import mechanism caches references to objects. Modules
    that imported JWS_HEADER_KID_VALIDATE before this patch still have references
    to the OLD validator instance. We must patch those cached references too!

    The new pattern maintains backward compatibility with existing did:key
    and did:sov formats while adding support for did:peer methods.

    Pattern breakdown:
    - ^did:(?:                         # Start with 'did:' prefix
    -   key:z[{B58}]+                  # did:key format (W3C)
    -   |sov:[{B58}]{{21,22}}          # did:sov format (Indy)
    -     (;.*)?(\?.*)?                # Optional parameters for did:sov
    -   |peer:[2-4].+                  # did:peer:2/3/4 format (NEW!)
    - )#.+$                            # Fragment identifier (e.g., #key-1)

    Examples of accepted DIDs:
    - did:key:z6MkpTHR8VNsBxYAAWHut2Geadd9jSwuBV8xRoAnwWsdvktH
    - did:sov:Q4zqM7aXqm7gDQkUVLng9h#keys-1
    - did:peer:2.Ez6LSbysY2xFMRpGMhb7tFTLMpeuPRaqaWM1yECx2AtzE3KCc.Vz6Mk...#key-1
    - did:peer:4zQmZDCy...z25gYmQo...#key-1 (PQC long form, thousands of chars)
    - did:peer:4zEYJrM...#key-1 (short form)
    """
    # Store original pattern for debugging
    original_pattern = JWSHeaderKid.PATTERN

    # Step 1: Update the class variable pattern
    JWSHeaderKid.PATTERN = rf"^did:(?:key:z[{B58}]+|sov:[{B58}]{{21,22}}(;.*)?(\?.*)?|peer:[2-4].+)#.+$"

    # Step 2: Update the example to show did:peer:4 format
    JWSHeaderKid.EXAMPLE = "did:peer:4zQmZDCy8xgzL1ZskYJ3Wk92mBRT1yzmJZCJkaARmZHLCuK#key-1"

    # Step 3: Create NEW validator instance with updated pattern
    # This is CRITICAL! The regex is compiled in __init__(), so we need a fresh instance
    new_validator = JWSHeaderKid()

    # Step 4: Replace the global instance in valid module
    valid_module.JWS_HEADER_KID_VALIDATE = new_validator
    valid_module.JWS_HEADER_KID_EXAMPLE = JWSHeaderKid.EXAMPLE

    # Step 5: Patch cached references in modules that already imported the validator
    # CRITICAL: attach_decorator imports JWS_HEADER_KID_VALIDATE at module load time
    # If it was already imported (e.g., by askar_pqc_patch), it has a cached reference
    # to the OLD validator instance. We MUST replace that cached reference!
    import sys
    patched_modules = []

    if 'acapy_agent.messaging.decorators.attach_decorator' in sys.modules:
        attach_decorator_module = sys.modules['acapy_agent.messaging.decorators.attach_decorator']
        attach_decorator_module.JWS_HEADER_KID_VALIDATE = new_validator
        patched_modules.append("attach_decorator (module-level)")

        # CRITICAL: Marshmallow schema classes are defined at module load time with
        # the validator baked into the field definition. We must patch the schema
        # class field directly!
        # Line 72 of attach_decorator.py:
        #   kid = fields.Str(validate=JWS_HEADER_KID_VALIDATE, ...)
        # The validator reference is stored in the schema field's metadata.
        schema_class = attach_decorator_module.AttachDecoratorDataJWSHeaderSchema
        if hasattr(schema_class, '_declared_fields') and 'kid' in schema_class._declared_fields:
            # Patch the validator in the declared field
            schema_class._declared_fields['kid'].validators = [new_validator]
            patched_modules.append("AttachDecoratorDataJWSHeaderSchema.kid field")

        LOGGER.info(f"Patched validator in: {', '.join(patched_modules)}")

    LOGGER.info("JWSHeaderKid validator patched for did:peer:4 support")
    LOGGER.debug(f"   Original pattern: {original_pattern}")
    LOGGER.debug(f"   New pattern: {JWSHeaderKid.PATTERN}")
    LOGGER.debug(f"   Validator instance recreated: {new_validator}")
    LOGGER.debug(f"   Patched {len(patched_modules)} locations")
\end{lstlisting}

\subsubsection{key\_types.py}

\refstepcounter{manualListingCounterA}
\label{lst:key_types.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - key\_types.py}, numbers=left, frame=single]
"""PQC Key Types for pqc_didpeer4_fm plugin.

This module defines KeyType objects for ML-DSA-65 and ML-KEM-768 without
depending on the pqcrypto_fm plugin. These are standalone definitions.
"""

from acapy_agent.wallet.key_type import KeyType


# ML-DSA-65 (Dilithium3) - NIST FIPS-204 Digital Signature Algorithm
ML_DSA_65 = KeyType(
    key_type="ml-dsa-65",
    multicodec_name="ml-dsa-65-pub",
    multicodec_prefix=b"\xd0\x65",
    jws_alg="PQC-DSA",
)

# ML-KEM-768 (Kyber768) - NIST FIPS-203 Key Encapsulation Mechanism
ML_KEM_768 = KeyType(
    key_type="ml-kem-768",
    multicodec_name="ml-kem-768-pub",
    multicodec_prefix=b"\xe0\x18",
    jws_alg=None,  # KEM algorithms don't have JWS algorithms
)
\end{lstlisting}

\subsubsection{key\_type\_patches.py}

\refstepcounter{manualListingCounterA}
\label{lst:key_type_patches.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - key\_type\_patches.py}, numbers=left, frame=single]
"""Patches for KeyTypes Registry and API Schema Validation.

This module extends ACA-Py's KeyTypes registry and API schemas to support
PQC key types (ML-DSA-65, ML-KEM-768).
"""

import logging

LOGGER = logging.getLogger(__name__)


def register_pqc_key_types(context):
    """Register PQC KeyTypes in ACA-Py's global KeyTypes registry.

    Args:
        context: InjectionContext to inject KeyTypes singleton

    This allows all ACA-Py components to look up PQC key types via:
    - key_types.from_key_type("ml-dsa-65")
    - key_types.from_multicodec_name("ml-dsa-65-pub")
    - key_types.from_multicodec_prefix(b"\xd0\x65")
    """
    from acapy_agent.wallet.key_type import KeyTypes

    from .key_types import ML_DSA_65, ML_KEM_768

    try:
        key_types = context.inject(KeyTypes)

        # Register PQC key types
        key_types.register(ML_DSA_65)
        key_types.register(ML_KEM_768)

        LOGGER.info(
            "Registered PQC key types in KeyTypes registry: ml-dsa-65, ml-kem-768"
        )
        print("Registered ML-DSA-65 and ML-KEM-768 in KeyTypes registry")

    except Exception as e:
        LOGGER.error(f"Failed to register PQC key types: {e}")
        raise


def patch_api_key_type_schemas():
    """Patch Marshmallow Schema __init__ to inject PQC validators at runtime.

    Patches 3 schema classes in acapy_agent/wallet/routes.py:
    1. DIDSchema.key_type (line 135) - Response schema
    2. DIDListQueryStringSchema.key_type (line 335) - Query parameter
    3. DIDCreateOptionsSchema.key_type (line 355) - Request body

    This allows:
    - POST /wallet/did/create with {"options": {"key_type": "ml-dsa-65"}}
    - GET /wallet/did?key_type=ml-dsa-65

    NOTE: We patch __init__() because aiohttp-apispec creates schema instances
    AFTER plugin setup. Patching _declared_fields doesn't work because
    Marshmallow copies them during instantiation.
    """
    from marshmallow import validate

    from acapy_agent.wallet.routes import (
        DIDCreateOptionsSchema,
        DIDListQueryStringSchema,
        DIDSchema,
    )

    try:
        # New validator including PQC key types
        pqc_validator = validate.OneOf(
            [
                "ed25519",
                "bls12381g2",
                "p256",
                "ml-dsa-65",
                "ml-kem-768",
            ]
        )

        # Patch 1: DIDSchema (response schema for /wallet/did, /wallet/did/create)
        original_did_init = DIDSchema.__init__

        def patched_did_init(self, *args, **kwargs):
            original_did_init(self, *args, **kwargs)
            if "key_type" in self.fields:
                self.fields["key_type"].validators = [pqc_validator]

        DIDSchema.__init__ = patched_did_init

        # Patch 2: DIDListQueryStringSchema (query params for GET /wallet/did)
        original_list_init = DIDListQueryStringSchema.__init__

        def patched_list_init(self, *args, **kwargs):
            original_list_init(self, *args, **kwargs)
            if "key_type" in self.fields:
                self.fields["key_type"].validators = [pqc_validator]

        DIDListQueryStringSchema.__init__ = patched_list_init

        # Patch 3: DIDCreateOptionsSchema (CRITICAL: request body for POST /wallet/did/create)
        original_create_init = DIDCreateOptionsSchema.__init__

        def patched_create_init(self, *args, **kwargs):
            original_create_init(self, *args, **kwargs)
            if "key_type" in self.fields:
                self.fields["key_type"].validators = [pqc_validator]

        DIDCreateOptionsSchema.__init__ = patched_create_init

        LOGGER.info("Patched API schemas __init__ to accept PQC key_types at runtime")
        print("Patched API schemas for PQC key_types (ml-dsa-65, ml-kem-768)")

    except Exception as e:
        LOGGER.error(f"Failed to patch API schemas: {e}")
        raise


def patch_did_peer4_supported_key_types(context):
    """Extend PEER4 DIDMethod to support PQC key types.

    By default, PEER4 only supports [ED25519, X25519].
    This patch adds [ML_DSA_65, ML_KEM_768] to make did:peer:4 PQC-capable.

    This is required because wallet_create_did() checks:
        if not method.supports_key_type(key_type):
            raise HTTPForbidden(...)

    Args:
        context: InjectionContext (unused, for API consistency)
    """
    from acapy_agent.wallet.did_method import PEER4

    from .key_types import ML_DSA_65, ML_KEM_768

    try:
        # Extend PEER4's supported key types
        # Original: [ED25519, X25519]
        # After patch: [ED25519, X25519, ML_DSA_65, ML_KEM_768]
        PEER4._supported_key_types.extend([ML_DSA_65, ML_KEM_768])

        supported_types = [kt.key_type for kt in PEER4.supported_key_types]
        LOGGER.info(f"Extended PEER4 to support PQC key types: {supported_types}")
        print(f"PEER4 now supports: {supported_types}")

    except Exception as e:
        LOGGER.error(f"Failed to patch PEER4 supported key types: {e}")
        raise


def patch_alg_mappings_for_pqc():
    """Extend ALG_MAPPINGS in wallet/keys/manager.py for PQC multikey support.

    ALG_MAPPINGS is used by verkey_to_multikey() and multikey_to_verkey()
    to convert between base58 verkeys and multibase multikeys.

    Without this patch, wallet.create_key(ML_DSA_65) fails with:
        KeyError: 'ml-dsa-65' in ALG_MAPPINGS[alg]["prefix_hex"]
    """
    from acapy_agent.wallet.keys.manager import ALG_MAPPINGS

    from .key_types import ML_DSA_65, ML_KEM_768

    try:
        # Add PQC algorithms to ALG_MAPPINGS
        # Based on multicodec prefixes from pqc_multicodec.py
        ALG_MAPPINGS["ml-dsa-65"] = {
            "key_type": ML_DSA_65,
            "multikey_prefix": "z6MN",  # Will be different for ML-DSA, but use z6MN for now
            "prefix_hex": "d065",  # From PQC_MULTICODECS: b"\xd0\x65"
            "prefix_length": 2,
        }

        ALG_MAPPINGS["ml-kem-768"] = {
            "key_type": ML_KEM_768,
            "multikey_prefix": "z6Ls",  # Will be different for ML-KEM
            "prefix_hex": "e018",  # From PQC_MULTICODECS: b"\xe0\x18"
            "prefix_length": 2,
        }

        LOGGER.info("Extended ALG_MAPPINGS for PQC multikey conversion")
        print("ALG_MAPPINGS extended for PQC (ml-dsa-65, ml-kem-768)")

    except Exception as e:
        LOGGER.error(f"Failed to patch ALG_MAPPINGS: {e}")
        raise

\end{lstlisting}

\subsubsection{multicodec\_patch.py}

\refstepcounter{manualListingCounterA}
\label{lst:multicodec_patch.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - multicodec\_patch.py}, numbers=left, frame=single]
"""Patch ACA-Py's SupportedCodecs to recognize PQC multicodecs.

This module monkey-patches acapy_agent.utils.multiformats.multicodec.SupportedCodecs
to support ML-DSA-65 and ML-KEM-768 multicodec prefixes used in PQC multikeys.

Without this patch, ACA-Py's multicodec.unwrap() throws:
    ValueError: Unsupported multicodec
when encountering PQC keys in did:peer:4 documents.
"""

import logging
from typing import Optional

LOGGER = logging.getLogger(__name__)


def patch_supported_codecs():
    """Monkey-patch SupportedCodecs to support PQC multicodecs.

    This patches two methods:
    1. SupportedCodecs.for_data() - Used when unwrapping multicodec-prefixed keys
    2. SupportedCodecs.by_name() - Used when looking up codecs by name

    Both methods fall back to PQC_MULTICODECS registry if classical codecs
    don't match.
    """
    try:
        from acapy_agent.utils.multiformats.multicodec import (
            SupportedCodecs,
            Multicodec,
        )
        from .pqc_multicodec import PQC_MULTICODECS

        # Save original methods
        original_for_data = SupportedCodecs.for_data
        original_by_name = SupportedCodecs.by_name

        @classmethod
        def for_data_pqc(cls, data: bytes) -> Multicodec:
            """Enhanced for_data() that supports PQC multicodecs.

            Args:
                data: Multicodec-prefixed key bytes

            Returns:
                Multicodec object

            Raises:
                ValueError: If multicodec prefix is unknown
            """
            # Try classical codecs first (ED25519, X25519, etc.)
            try:
                return original_for_data(data)
            except ValueError:
                # Classical codec not found, try PQC registry
                for codec_name, prefix in PQC_MULTICODECS.items():
                    if data.startswith(prefix):
                        LOGGER.debug(f"Matched PQC multicodec: {codec_name}")
                        return Multicodec(codec_name, prefix)

                # Neither classical nor PQC codec matched
                prefix_hex = data[:2].hex() if len(data) >= 2 else "empty"
                raise ValueError(
                    f"Unsupported multicodec (prefix: 0x{prefix_hex}). "
                    f"Supported PQC: {list(PQC_MULTICODECS.keys())}"
                )

        @classmethod
        def by_name_pqc(cls, name: str) -> Multicodec:
            """Enhanced by_name() that supports PQC multicodecs.

            Args:
                name: Multicodec name (e.g., "ml-dsa-65-pub")

            Returns:
                Multicodec object

            Raises:
                ValueError: If multicodec name is unknown
            """
            # Try classical codecs first
            try:
                return original_by_name(name)
            except ValueError:
                # Classical codec not found, try PQC registry
                if name in PQC_MULTICODECS:
                    LOGGER.debug(f"Matched PQC multicodec by name: {name}")
                    return Multicodec(name, PQC_MULTICODECS[name])

                # Neither classical nor PQC codec matched
                raise ValueError(
                    f"Unsupported multicodec: {name}. "
                    f"Supported PQC: {list(PQC_MULTICODECS.keys())}"
                )

        # Apply monkey patches
        SupportedCodecs.for_data = for_data_pqc
        SupportedCodecs.by_name = by_name_pqc

        LOGGER.info("Patched SupportedCodecs.for_data() for PQC support")
        LOGGER.info("Patched SupportedCodecs.by_name() for PQC support")

    except ImportError as e:
        LOGGER.error(f"Failed to import multicodec module: {e}")
        raise
    except Exception as e:
        LOGGER.error(f"Unexpected error while patching SupportedCodecs: {e}")
        raise


def unpatch_supported_codecs():
    """Remove monkey patches and restore original SupportedCodecs methods.

    This function is provided for testing purposes. In normal operation,
    the patches should remain active.
    """
    # Note: Restoration would require storing original methods globally
    # For now, this is a placeholder for API consistency
    LOGGER.warning("unpatch_supported_codecs() not implemented - patches are permanent")
\end{lstlisting}

\subsubsection{setup.py}

\refstepcounter{manualListingCounterA}
\label{lst:setup.py}
\begin{lstlisting}[language=python, caption={pqc\_didpeer4\_fm - setup.py}, numbers=left, frame=single]
"""Setup for pqc_didpeer4_fm plugin."""

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="pqc_didpeer4_fm",
    version="0.1.0",
    author="Ferris Menzel",
    author_email="admin@example.com",
    description="Post-Quantum did:peer:4 plugin with ML-DSA-65 + ML-KEM-768 for ACA-Py",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="",
    packages=find_packages(),
    install_requires=[
        # Note: aries-cloudagent is NOT listed here because this plugin
        # runs inside an existing ACA-Py installation which provides all
        # core dependencies. Only list additional dependencies needed by
        # this plugin that are not part of standard ACA-Py.
        "did-peer-4>=0.1.4",
        "pydid>=0.4.0",
        "multiformats>=0.3.0",
        "base58>=2.1.0",
        "liboqs-python>=0.10.0",
    ],
    entry_points={
        "aries_cloudagent.plugins": [
            "pqc_didpeer4_fm = pqc_didpeer4_fm:setup"
        ]
    },
    python_requires=">=3.9",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Topic :: Security :: Cryptography",
        "License :: OSI Approved :: Apache Software License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Operating System :: OS Independent",
    ],
    keywords="pqc post-quantum cryptography did peer ml-dsa ml-kem acapy aries ssi",
)
\end{lstlisting}

\subsubsection{README.md}

\refstepcounter{manualListingCounterA}
\label{lst:README.md}
\begin{lstlisting}[language=bash, caption={pqc\_didpeer4\_fm - README.md}, numbers=left, frame=single]
# pqc_didpeer4_fm

**Post-Quantum did:peer:4 Plugin for ACA-Py**

Transparently replaces ED25519/X25519 with **ML-DSA-65** (NIST FIPS-204) and **ML-KEM-768** (NIST FIPS-203) in did:peer:4 DIDs.

## Features

- **Post-Quantum Security**: ML-DSA-65 for signatures, ML-KEM-768 for key agreement
- **Transparent Integration**: NO API changes needed - works with existing workflows
- **Zero Code Changes**: Existing notebooks/scripts continue to work unchanged
- **Standards Compliant**: NIST FIPS-203/204, W3C Multicodec (provisional)
- **DIDComm v2 Ready**: Full encryption and authentication support

## Installation

```bash
cd acapy-plugins/pqc_didpeer4_fm
pip install -e .
```

## Usage

### 1. Load Plugin in ACA-Py

```bash
aca-py start \
  --plugin pqc_didpeer4_fm \
  --endpoint https://agent.example.com:8020 \
  --admin 0.0.0.0 8021 \
  ...
```

### 2. docker-compose.yml

```yaml
services:
  issuer:
    command: >
      start
      --plugin pqc_didpeer4_fm  # <-- Add this line
      --endpoint https://host.docker.internal:8020
      ...
```

### 3. Create Connection (Unchanged!)

```python
# Existing code continues to work!
invitation_data = {
    "use_did_method": "did:peer:4",  # <-- Plugin makes this PQC automatically
    "handshake_protocols": ["https://didcomm.org/didexchange/1.1"],
    "my_label": "My Agent"
}

response = requests.post(
    "http://localhost:8021/out-of-band/create-invitation",
    json=invitation_data
)

# Plugin creates did:peer:4 with ML-DSA-65 + ML-KEM-768 automatically!
```

### 4. View DIDs

```python
# GET /wallet/did now shows PQC keys
response = requests.get("http://localhost:8021/wallet/did")

# Response:
# {
#   "results": [{
#     "did": "did:peer:4:z6MNxxx...",
#     "key_type": "ml-dsa-65",  # <-- Shows PQC!
#     "method": "did:peer:4",
#     "metadata": {
#       "pqc_enabled": true,
#       "signature_algorithm": "ml-dsa-65",
#       "key_agreement_algorithm": "ml-kem-768",
#       "plugin": "pqc_didpeer4_fm"
#     }
#   }]
# }
```

## How It Works

### Transparent Monkey-Patching

The plugin patches `BaseConnectionManager.create_did_peer_4()` at runtime:

```
Workflow: POST /out-of-band/create-invitation {"use_did_method": "did:peer:4"}
             |
             V
         Out-of-Band Manager
             |
             V
         BaseConnectionManager.create_did_peer_4()
             |
             V
         PLUGIN INTERCEPTS HERE
             |
             V
         Creates ML-DSA-65 + ML-KEM-768 keys instead of ED25519 + X25519
             |
             V
         did:peer:4:z6MNxxx... (PQC enabled)
```

### Two Keys, Two Purposes

| Key Type | Purpose | DID Document Relationship |
|----------|---------|---------------------------|
| **ML-DSA-65** | Digital Signatures | `authentication`, `assertionMethod` |
| **ML-KEM-768** | Key Agreement (Encryption) | `keyAgreement` |

### Why Both?

- **ML-DSA-65** proves authenticity ("I am really Alice")
- **ML-KEM-768** enables encryption ("Only Bob can read this")
- **Both are required** for secure DIDComm messaging

## Technical Details

### Multicodec Prefixes (Provisional)

- ML-DSA-65: `0xd065` --> Multikey prefix `z6MN`
- ML-KEM-768: `0xe018` --> Multikey prefix `z6MK768`

### Dependencies

- `aries-cloudagent~=1.0.0`
- `did-peer-4>=0.1.4`
- `pydid>=0.4.0`
- `multiformats>=0.3.0`
- `base58>=2.1.0`
- `pqcrypto_fm` (PQC crypto plugin - must be installed separately)

### Workflow Integration

**NO CODE CHANGES NEEDED!**

Existing workflows (e.g., `SSI_Complete_Workflow.ipynb` Cells 13-16) continue to work:

```python
# Cell 13 - Connection Creation (unchanged)
invitation_response = api_post(
    ISSUER_ADMIN_URL,
    "/out-of-band/create-invitation",
    {"use_did_method": "did:peer:4", ...}
)
# Plugin automatically uses ML-DSA-65 + ML-KEM-768

# Cell 16 - DID Overview (unchanged)
dids = api_get(ISSUER_ADMIN_URL, "/wallet/did")
# Shows "key_type": "ml-dsa-65" automatically
```

## Important Notes

1. **All agents must have the plugin**: If one agent has PQC enabled, all connected agents must also have it
2. **Not backward compatible**: Classic did:peer:4 (ED25519) and PQC did:peer:4 (ML-DSA-65) cannot interoperate
3. **Experimental**: PQC multicodec prefixes are provisional (W3C draft)
4. **Requires OpenSSL 3.5+**: For native ML-DSA/ML-KEM support

## References

- [NIST FIPS-203 (ML-KEM)](https://csrc.nist.gov/pubs/fips/203/final)
- [NIST FIPS-204 (ML-DSA)](https://csrc.nist.gov/pubs/fips/204/final)
- [did:peer:4 Specification](https://identity.foundation/peer-did-method-spec/)
- [W3C Multicodec Registry](https://w3c-ccg.github.io/multicodec/)

## License

Apache License 2.0

## Author

Ferris Menzel
\end{lstlisting}

\subsection{Dockerfile: acapy-base-pqc}

\refstepcounter{manualListingCounterA}
\label{lst:Dockerfile-acapy-base-pqc}
\begin{lstlisting}[language=bash, caption={Dockerfile - acapy-base-pqc}, numbers=left, frame=single]
# =============================================================================
# Stage 1: Build OpenSSL 3.5.4 with native PQC support (ML-KEM, ML-DSA)
# =============================================================================
FROM debian:bookworm-slim AS openssl-builder

ARG OPENSSL_VERSION=3.5.4
ARG OPENSSL_PREFIX=/usr/local/ssl

# Install build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    curl \
    libssl-dev \
    perl \
    && rm -rf /var/lib/apt/lists/*

# Download and extract OpenSSL 3.5.4
WORKDIR /tmp
RUN curl -fsSL "https://www.openssl.org/source/openssl-${OPENSSL_VERSION}.tar.gz" -o openssl.tar.gz && \
    tar -xzf openssl.tar.gz && \
    rm openssl.tar.gz

# Build OpenSSL with native PQC support
WORKDIR /tmp/openssl-${OPENSSL_VERSION}
RUN ./Configure \
    --prefix=${OPENSSL_PREFIX} \
    --openssldir=${OPENSSL_PREFIX}/ssl \
    shared \
    enable-fips \
    linux-x86_64 && \
    make -j"$(nproc)" && \
    make install_sw install_ssldirs

# Verify OpenSSL 3.5.4 was built successfully
# RUN LD_LIBRARY_PATH=${OPENSSL_PREFIX}/lib64 ${OPENSSL_PREFIX}/bin/openssl version && \
#     ${OPENSSL_PREFIX}/bin/openssl list -kem-algorithms | grep -i ml-kem && \
#     ${OPENSSL_PREFIX}/bin/openssl list -signature-algorithms | grep -i ml-dsa

RUN LD_LIBRARY_PATH=${OPENSSL_PREFIX}/lib64 ${OPENSSL_PREFIX}/bin/openssl version

# =============================================================================
# Stage 2: Build liboqs for PQC support
# =============================================================================
FROM debian:bookworm-slim AS liboqs-builder

ARG LIBOQS_VERSION=0.14.0
ARG LIBOQS_PREFIX=/usr/local

# Install build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    cmake \
    curl \
    git \
    libssl-dev \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Download and build liboqs
WORKDIR /tmp
RUN curl -fsSL "https://github.com/open-quantum-safe/liboqs/archive/refs/tags/${LIBOQS_VERSION}.tar.gz" -o liboqs.tar.gz && \
    tar -xzf liboqs.tar.gz && \
    rm liboqs.tar.gz

WORKDIR /tmp/liboqs-${LIBOQS_VERSION}
RUN mkdir build && cd build && \
    cmake -GNinja \
    -DCMAKE_INSTALL_PREFIX=${LIBOQS_PREFIX} \
    -DBUILD_SHARED_LIBS=ON \
    .. && \
    ninja && \
    ninja install

# Verify liboqs installation
RUN ldconfig && \
    ls -la ${LIBOQS_PREFIX}/lib/liboqs.* && \
    echo "liboqs ${LIBOQS_VERSION} built successfully"

# =============================================================================
# Stage 3: ACA-Py Build Stage (Poetry)
# =============================================================================
FROM python:3.12-slim-bookworm AS build
ARG python_version=3.12

RUN pip install --no-cache-dir poetry==2.1.1

WORKDIR /src

COPY ./pyproject.toml ./poetry.lock ./
RUN poetry install --no-root

COPY ./acapy_agent ./acapy_agent
COPY ./README.md /src
RUN poetry build

# =============================================================================
# Stage 4: Main Runtime Image with OpenSSL 3.5.4 PQC + liboqs
# =============================================================================
FROM python:3.12-slim-bookworm AS main

ARG python_version=3.12
ARG uid=1001
ARG user=aries
ARG acapy_name="acapy-agent"
ARG acapy_version
ARG acapy_reqs=[didcommv2]
ARG OPENSSL_PREFIX=/usr/local/ssl

ENV HOME="/home/$user" \
    APP_ROOT="/home/$user" \
    LC_ALL=C.UTF-8 \
    LANG=C.UTF-8 \
    PIP_NO_CACHE_DIR=off \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    RUST_LOG=warn \
    SHELL=/bin/bash \
    SUMMARY="$acapy_name image with PQC support" \
    DESCRIPTION="$acapy_name provides a base image for running acapy agents with Post-Quantum Cryptography support via OpenSSL 3.5.4. \
    Supports ML-KEM-768, ML-DSA-65, and X25519MLKEM768 hybrid algorithms. Based on Debian Bookworm."

LABEL summary="$SUMMARY" \
    description="$DESCRIPTION" \
    io.k8s.description="$DESCRIPTION" \
    io.k8s.display-name="$acapy_name $acapy_version" \
    name=$acapy_name \
    acapy.version="$acapy_version" \
    maintainer=""

# Copy OpenSSL 3.5.4 from builder stage
COPY --from=openssl-builder ${OPENSSL_PREFIX} ${OPENSSL_PREFIX}

# Copy liboqs from builder stage
COPY --from=liboqs-builder /usr/local/lib/liboqs.* /usr/local/lib/
COPY --from=liboqs-builder /usr/local/include/oqs /usr/local/include/oqs

# Copy PQC Root CA certificate
COPY hopE/pqc_sidecarproxy_nginx/certs/rootCA.crt /usr/local/share/ca-certificates/pqc-root-ca.crt

# Add aries user
RUN useradd -U -ms /bin/bash -u $uid $user

# Install environment (without openssl, we use custom build)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    apt-transport-https \
    ca-certificates \
    curl \
    git \
    libffi-dev \
    libgmp10 \
    libncurses5 \
    libncursesw5 \
    sqlite3 \
    zlib1g && \
    apt-get autopurge -y && \
    apt-get clean -y && \
    rm -rf /var/lib/apt/lists/* /usr/share/doc/*

# Import PQC Root CA into system trust store
RUN update-ca-certificates && \
    echo "PQC Root CA imported successfully"

# Replace system OpenSSL with PQC-enabled version and configure liboqs
RUN ln -sf ${OPENSSL_PREFIX}/bin/openssl /usr/bin/openssl && \
    ln -sf ${OPENSSL_PREFIX}/lib64/libssl.so.3 /usr/lib/x86_64-linux-gnu/libssl.so.3 && \
    ln -sf ${OPENSSL_PREFIX}/lib64/libcrypto.so.3 /usr/lib/x86_64-linux-gnu/libcrypto.so.3 && \
    echo "${OPENSSL_PREFIX}/lib64" > /etc/ld.so.conf.d/openssl-pqc.conf && \
    echo "/usr/local/lib" > /etc/ld.so.conf.d/liboqs.conf && \
    ldconfig

# Link OpenSSL's cert directory to system certs (for Python ssl module)
# This ensures Python's ssl module finds the PQC Root CA
RUN rm -rf ${OPENSSL_PREFIX}/ssl/certs && \
    ln -s /etc/ssl/certs ${OPENSSL_PREFIX}/ssl/certs && \
    ln -s /etc/ssl/certs/ca-certificates.crt ${OPENSSL_PREFIX}/ssl/cert.pem

# Set SSL environment variables for Python ssl module
ENV SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt \
    SSL_CERT_DIR=/etc/ssl/certs \
    REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt \
    CURL_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt

# Verify OpenSSL installation and PQC support
RUN openssl version && \
    openssl list -kem-algorithms | grep -i ml-kem && \
    echo "OpenSSL 3.5.4 with PQC support successfully installed"

WORKDIR $HOME

# Add local binaries and aliases to path
ENV PATH="$HOME/.local/bin:$PATH"

# - In order to drop the root user, we have to make some directories writable
#   to the root group as OpenShift default security model is to run the container
#   under random UID.
RUN usermod -a -G 0 $user

# Create standard directories to allow volume mounting and set permissions
# Note: PIP_NO_CACHE_DIR environment variable should be cleared to allow caching
RUN mkdir -p \
    $HOME/.acapy_agent \
    $HOME/.cache/pip/http \
    $HOME/.indy_client \
    $HOME/ledger/sandbox/data \
    $HOME/log

# The root group needs access the directories under $HOME/.indy_client and $HOME/.acapy_agent for the container to function in OpenShift.
RUN chown -R $user:root $HOME/.indy_client $HOME/.acapy_agent && \
    chmod -R ug+rw $HOME/log $HOME/ledger $HOME/.acapy_agent $HOME/.cache $HOME/.indy_client

# Create /home/indy and symlink .indy_client folder for backwards compatibility with artifacts created on older indy-based images.
RUN mkdir -p /home/indy
RUN ln -s /home/aries/.indy_client /home/indy/.indy_client

# Install ACA-py from the wheel as $user,
# and ensure the permissions on the python 'site-packages' and $HOME/.local folders are set correctly.
USER $user
COPY --from=build /src/dist/acapy_agent*.whl .
RUN acapy_agent_package=$(find ./ -name "acapy_agent*.whl" | head -n 1) && \
    echo "Installing ${acapy_agent_package} ..." && \
    pip install --no-cache-dir --find-links=. ${acapy_agent_package}${acapy_reqs} && \
    rm acapy_agent*.whl && \
    chmod +rx $(python -m site --user-site) $HOME/.local

# Install pqc_didpeer4_fm plugin
COPY --chown=$user:root acapy-plugins/pqc_didpeer4_fm /tmp/pqc_didpeer4_fm
RUN pip install --no-cache-dir /tmp/pqc_didpeer4_fm && \
    rm -rf /tmp/pqc_didpeer4_fm && \
    echo "pqc_didpeer4_fm plugin installed successfully"

ENTRYPOINT ["aca-py"]
\end{lstlisting}

\subsection{docker-compose.yml: SSI Agenten mit acapy-base-pqc \& Plugin}

\refstepcounter{manualListingCounterA}
\label{lst:docker-compose.yml-SSI-Agenten-mit-acapy-base-pqc-und-plugin}
\begin{lstlisting}[language=python, caption={multicodec\_patch.py}, numbers=left, frame=single]
version: '3.8'

services:
  # unter command: > --label XY ==>  --plugin pqc_didpeer4_fm ergänzen für PQC
  issuer:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base-pqc
    image: acapy-base-pqc
    container_name: issuer-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
      - TAILS_FILE_COUNT=100
    networks:
      - hope-issuer
    volumes:
      - issuer-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Issuer Agent"
      --plugin pqc_didpeer4_fm
      --inbound-transport http 0.0.0.0 8020
      --outbound-transport http
      --endpoint https://host.docker.internal:8020
      --admin 0.0.0.0 8021
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name issuer_wallet
      --wallet-key issuer_wallet_key_000000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-respond-credential-proposal
      --auto-respond-credential-offer
      --auto-respond-credential-request
      --auto-verify-presentation
      --public-invites
      --preserve-exchange-records
      --tails-server-base-url https://host.docker.internal:6543
      --notify-revocation
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8021/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # unter command: > --label XY ==>  --plugin pqc_didpeer4_fm ergänzen für PQC
  holder:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base-pqc
    image: acapy-base-pqc
    container_name: holder-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
    networks:
      - hope-holder
    volumes:
      - holder-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Holder Agent"
      --plugin pqc_didpeer4_fm
      --inbound-transport http 0.0.0.0 8030
      --outbound-transport http
      --endpoint https://host.docker.internal:8030
      --admin 0.0.0.0 8031
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name holder_wallet
      --wallet-key holder_wallet_key_000000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-respond-credential-offer
      --auto-store-credential
      --public-invites
      --tails-server-base-url https://host.docker.internal:6543
      --preserve-exchange-records
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8031/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # unter command: > --label XY ==>  --plugin pqc_didpeer4_fm ergänzen für PQC
  verifier:
    build:
      context: ..
      dockerfile: hopE/Dockerfile.acapy-base-pqc
    image: acapy-base-pqc
    container_name: verifier-agent
    environment:
      - DOCKERHOST=${DOCKERHOST:-host.docker.internal}
      - GENESIS_URL=${GENESIS_URL:-https://host.docker.internal:8000/genesis}
      - LEDGER_URL=${LEDGER_URL:-https://host.docker.internal:8000}
      - PUBLIC_TAILS_URL=https://host.docker.internal:6543
    networks:
      - hope-verifier
    volumes:
      - verifier-data:/home/aries/.acapy_agent
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      start
      --label "Verifier Agent"
      --plugin pqc_didpeer4_fm
      --inbound-transport http 0.0.0.0 8040
      --outbound-transport http
      --endpoint https://host.docker.internal:8040
      --admin 0.0.0.0 8041
      --admin-insecure-mode
      --auto-provision
      --wallet-type askar
      --wallet-name verifier_wallet
      --wallet-key verifier_wallet_key_00000000000
      --genesis-url https://host.docker.internal:8000/genesis
      --log-level info
      --auto-accept-invites
      --auto-accept-requests
      --auto-ping-connection
      --auto-verify-presentation
      --public-invites
      --tails-server-base-url https://host.docker.internal:6543
      --preserve-exchange-records
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8041/status/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Post-Quantum Nginx Reverse Proxy für Issuer Agent
  pqc-sidecarproxy-issuer:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-issuer
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-issuer
    ports:
      - "8020:8020"  # Issuer Inbound Transport HTTPS (ML-KEM-768)
      - "8021:8021"  # Issuer Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_issuer.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8021/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

 # Post-Quantum Nginx Reverse Proxy für Holder Agent
  pqc-sidecarproxy-holder:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-holder
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-holder
    ports:
      - "8030:8030"  # Holder Inbound Transport HTTPS (ML-KEM-768)
      - "8031:8031"  # Holder Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_holder.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8031/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

 # Post-Quantum Nginx Reverse Proxy für Verifier Agent
  pqc-sidecarproxy-verifier:
    build:
      context: ./pqc_sidecarproxy_nginx
      dockerfile: Dockerfile
      args:
        OPENSSL_TAG: openssl-3.5.4
        LIBOQS_TAG: 0.13.0
        OQSPROVIDER_TAG: 0.9.0
        NGINX_VERSION: 1.28.0
        SIG_ALG: mldsa65
        DEFAULT_GROUPS: X25519MLKEM768:mlkem768:x25519:mlkem1024
    container_name: pqc-sidecarproxy-verifier
    environment:
      # OpenSSL Configuration
      - OPENSSL_CONF=/opt/openssl/.openssl/ssl/openssl.cnf
      # Post-Quantum Key Exchange Groups
      - DEFAULT_GROUPS=X25519MLKEM768:mlkem768:x25519:mlkem1024
    networks:
      - von_sidecarproxy
      - hope-verifier
    ports:
      - "8040:8040"  # Verifier Inbound Transport HTTPS (ML-KEM-768)
      - "8041:8041"  # Verifier Admin API HTTPS (ML-KEM-768)
    volumes:
      # Custom nginx configuration for reverse proxy
      - ./pqc_sidecarproxy_nginx/nginx-conf/nginx_verifier.conf:/opt/nginx/nginx-conf/nginx.conf:ro
      # Custom ML-DSA-65 certificates
      - ./pqc_sidecarproxy_nginx/certs:/opt/nginx/certs:ro
      # Logs
      - nginx-logs:/opt/nginx/logs
    depends_on:
      - issuer
      - holder
      - verifier
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:8041/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

networks:
  hope-issuer:
  hope-holder:
  hope-verifier:
  von_sidecarproxy:
    external: true

volumes:
  issuer-data:
  holder-data:
  verifier-data:
  nginx-logs:
\end{lstlisting}

\subsection{Issuer Agent mit PQC-Plugin Boot Log}

\refstepcounter{manualListingCounterA}
\label{lst:Issuer-Agent-Boot-Logs-mit-PQC-Plugin}
\begin{lstlisting}[language=bash, caption={Issuer Agent mit PQC-Plugin Boot Log}, numbers=left, frame=single]
Executing task in folder ferris: docker logs --tail 1000 -f 767d8558623044fb63e556e0bd1fdce0912ed8be74c26003b449f425d349fa3e 

2025-12-03 23:40:34,654 acapy_agent.config.default_context INFO Registering default plugins
2025-12-03 23:40:34,772 acapy_agent.config.default_context INFO Registering askar plugins
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Saved original _create_keypair function
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Successfully patched askar._create_keypair for PQC support
============================================================
   pqc_didpeer4_fm Plugin v0.1.0
   Post-Quantum did:peer:4 with ML-DSA-65 + ML-KEM-768
   by Ferris Menzel
============================================================
   Askar patched for PQC key generation (liboqs-python)
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched Session.insert_key() for PQC support
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched Session.fetch_key() for PQC support
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched Session.update_key() for PQC support
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AskarWallet.assign_kid_to_key() for PQC support
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.key_type_patches INFO Registered PQC key types in KeyTypes registry: ml-dsa-65, ml-kem-768
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.key_type_patches INFO Patched API schemas __init__ to accept PQC key_types at runtime
   Registered ML-DSA-65 and ML-KEM-768 in KeyTypes registry
   Patched API schemas for PQC key_types (ml-dsa-65, ml-kem-768)
   PEER4 now supports: ['ed25519', 'x25519', 'ml-dsa-65', 'ml-kem-768']
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.key_type_patches INFO Extended PEER4 to support PQC key types: ['ed25519', 'x25519', 'ml-dsa-65', 'ml-kem-768']
   ALG_MAPPINGS extended for PQC (ml-dsa-65, ml-kem-768)
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.key_type_patches INFO Extended ALG_MAPPINGS for PQC multikey conversion
   PQC Multicodecs registered (ML-DSA-65, ML-KEM-768)
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.multicodec_patch INFO Patched SupportedCodecs.for_data() for PQC support
2025-12-03 23:40:34,841 pqc_didpeer4_fm.v1_0.multicodec_patch INFO Patched SupportedCodecs.by_name() for PQC support
   SupportedCodecs patched for PQC multicodec decoding
   DIDComm v1 pack/unpack patched for ML-KEM-768
   AttachDecorator patched for ML-DSA-65 JWS signatures
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AskarWallet.pack_message() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AskarWallet.unpack_message() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AskarWallet.sign_message() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AskarWallet.verify_message() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AttachDecoratorData.sign() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.askar_pqc_patch INFO Patched AttachDecoratorData.verify() for PQC support
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.validator_patch INFO    Patched validator in: attach_decorator (module-level), AttachDecoratorDataJWSHeaderSchema.kid field
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.validator_patch INFO JWSHeaderKid validator patched for did:peer:4 support
   JWSHeaderKid validator patched for did:peer:4
   AskarWallet patched for ML-KEM-768 verkey lookup
2025-12-03 23:40:34,842 pqc_didpeer4_fm ERROR WALLET PATCH APPLIED SUCCESSFULLY AT PLUGIN LOAD
2025-12-03 23:40:34,842 pqc_didpeer4_fm ERROR    Original: <function AskarWallet.get_local_did_for_verkey at 0x73ec177a6f20>
2025-12-03 23:40:34,842 pqc_didpeer4_fm ERROR    Patched:  <function get_local_did_for_verkey_pqc at 0x73ec172174c0>
2025-12-03 23:40:34,842 pqc_didpeer4_fm.v1_0.connection_target_patch INFO Patching ConnectionTargetSchema to accept PQC keys...
2025-12-03 23:40:34,843 pqc_didpeer4_fm.v1_0.connection_target_patch INFO   recipient_keys field patched to accept PQC keys
2025-12-03 23:40:34,843 pqc_didpeer4_fm.v1_0.connection_target_patch INFO   routing_keys field patched to accept PQC keys
2025-12-03 23:40:34,843 pqc_didpeer4_fm.v1_0.connection_target_patch INFO   sender_key field patched to accept PQC keys
2025-12-03 23:40:34,843 pqc_didpeer4_fm.v1_0.connection_target_patch INFO      New pattern: base58 strings with 32+ characters
   ConnectionTarget schema patched for PQC key validation
2025-12-03 23:40:34,843 pqc_didpeer4_fm.v1_0.connection_target_patch INFO      Accepts: ED25519 (43-44 chars), ML-DSA-65 (~2650 chars), ML-KEM-768 (~1600 chars)
   Monkey patches applied to BaseConnectionManager
      - create_did_peer_4 --> PQC version (eliminates ED25519)
      - _extract_key_material_in_base58_format --> PQC support
      - long_did_peer_4_to_short --> Preserves PQC key_type
      - long_did_peer_to_short --> Handles short-form DIDs correctly
      - record_keys_for_resolvable_did --> Stores BOTH PQC keys
      - resolve_inbound_connection --> DEBUG logging for diagnostics
      - find_connection --> Handles long/short form my_did (credential fix)
   PQC Peer4 Resolver registered
============================================================
   pqc_didpeer4_fm loaded successfully!
   did:peer:4 now uses ML-DSA-65 + ML-KEM-768
============================================================

2025-12-03 23:40:34,923 acapy_agent.config.ledger INFO Fetching genesis transactions from: https://host.docker.internal:8000/genesis
2025-12-03 23:40:34,931 acapy_agent.core.profile INFO Create profile manager: askar
2025-12-03 23:40:35,512 acapy_agent.config.wallet INFO Created new profile - Profile name: issuer_wallet, backend: askar
2025-12-03 23:40:35,515 acapy_agent.config.wallet INFO No public DID created
2025-12-03 23:40:35,575 acapy_agent.config.ledger INFO Ledger configuration complete
2025-12-03 23:40:35,575 acapy_agent.core.conductor INFO Ledger configured successfully.
2025-12-03 23:40:35,584 acapy_agent.core.conductor INFO Wallet type record not found.
2025-12-03 23:40:35,585 acapy_agent.core.conductor INFO New agent. Setting wallet type to askar.
2025-12-03 23:40:35,753 acapy_agent.config.banner INFO 
::::::::::::::::::::::::::::::::::::::::::::::
::               Issuer Agent               ::
::                                          ::
::                                          ::
:: Inbound Transports:                      ::
::                                          ::
::   - http://0.0.0.0:8020                  ::
::                                          ::
:: Outbound Transports:                     ::
::                                          ::
::   - http                                 ::
::   - https                                ::
::                                          ::
:: Administration API:                      ::
::                                          ::
::   - http://0.0.0.0:8021                  ::
::                                          ::
::                               ver: 1.3.2 ::
::::::::::::::::::::::::::::::::::::::::::::::

2025-12-03 23:40:35,753 acapy_agent.config.banner INFO 
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
::                               DEPRECATION NOTICE:                                ::
:: -------------------------------------------------------------------------------- ::
:: Receiving a core DIDComm protocol with the `did:sov:BzCbsNYhMrjHiqZDTUASHg;spec` ::
:: prefix is deprecated. All parties sending this prefix should be notified that    ::
:: support for receiving such messages will be removed in a future release. Use     ::
:: https://didcomm.org/ instead.                                                    ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0160: Connection Protocol is deprecated and support will be removed in ::
:: a future release; use RFC 0023: DID Exchange instead.                            ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0036: Issue Credential 1.0 is deprecated and support will be removed   ::
:: in a future release; use RFC 0453: Issue Credential 2.0 instead.                 ::
:: -------------------------------------------------------------------------------- ::
:: Aries RFC 0037: Present Proof 1.0 is deprecated and support will be removed in a ::
:: future release; use RFC 0454: Present Proof 2.0 instead.                         ::
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

2025-12-03 23:40:35,754 acapy_agent.core.conductor INFO Wallet version storage record not found.
2025-12-03 23:40:35,754 acapy_agent.core.conductor INFO No upgrade from version was found from wallet or via --from-version startup argument. Defaulting to v0.7.5.
2025-12-03 23:40:35,755 acapy_agent.core.conductor INFO Upgrade configurations available. Initiating upgrade.
2025-12-03 23:40:35,757 acapy_agent.commands.upgrade INFO No ACA-Py version found in wallet storage.
2025-12-03 23:40:35,757 acapy_agent.commands.upgrade INFO Selecting v0.7.5 as --from-version from the config.
2025-12-03 23:40:35,757 acapy_agent.commands.upgrade INFO Running upgrade process for v0.8.1
2025-12-03 23:40:35,758 acapy_agent.commands.upgrade INFO No records of <class 'acapy_agent.connections.models.conn_record.ConnRecord'> found
2025-12-03 23:40:35,766 acapy_agent.commands.upgrade INFO acapy_version storage record set to v1.3.2
2025-12-03 23:40:35,769 acapy_agent.core.conductor INFO Listening...
2025-12-03 23:40:38,661 aiohttp.access INFO 127.0.0.1 [03/Dec/2025:23:40:38 +0000] "GET /status/ready HTTP/1.1" 200 138 "-" "curl/7.88.1"
2025-12-03 23:41:08,696 aiohttp.access INFO 127.0.0.1 [03/Dec/2025:23:41:08 +0000] "GET /status/ready HTTP/1.1" 200 138 "-" "curl/7.88.1"
\end{lstlisting}


\newpage
\section{Summative Evaluation}
\label{sec:Anhang_Summative Evaluation}

\subsection{KRITIS Szenario}
\label{sec:Anhang_KRITIS Szenario}

% 1. ISSUANCE
%    - Betreiber (Issuer) stellt "Notfall-Wartungszertifikat" aus
%    - Techniker (Holder) empfängt VC mit PQC-Signatur
   
% 2. VERIFICATION
%    - Techniker präsentiert Credential am Umspannwerk-Zugang
%    - Verifier (Zutrittssystem) verifiziert ZKP-Proof
%    - Zugang wird gewährt
   
% 3. REVOCATION (optional, aber impactful)
%    - Einsatz beendet → Credential wird revoked
%    - Techniker versucht sich erneut zu authentifizieren
%    - Zugang wird nicht mehr gewährt



%  Verifizierte Attribute (REVEALED):
%  - Zertifikatstyp: Notfall-Wartungsberechtigung
%  - Anlage: Umspannwerk Nord-Ost
%  - Sicherheitsstufe: Stufe-3-Kritisch
%  - Gültigkeitszeiträume: 1765026000 bis 1765033200
%  - Rolle: Notfalltechniker

%  PER ZKP VERIFIZIERT ABER DURCH DATENSCHUTZ GESCHÜTZT (UNREVEALED):
% -  Vorname: NICHT offengelegt (Zero-Knowledge-Proof)
%  - Nachname: NICHT offengelegt(Zero-Knowledge-Proof)
%  - Organisation: NICHT offengelegt (Zero-Knowledge-Proof)

Dieses Jupyter Notebook demonstriert einen vollständigen \ac{SSI}-Workflow für ein \ac{KRITIS}-Unternehmen im Energiesektor \parencite[§2]{bundesministeriumdesinnern_VerordnungZurBestimmungKritischerInfrastrukturennachBSIGesetzBSIKritisverordnung_2016}. Im Fokus stehen die durchgängigen Prozesse der Ausstellung, Verwaltung und Verifikation von Notfall-Wartungszertifikaten für den Zugang zu einem Umspannwerk des Unternehmens auf, welches die Transformation von 
Hochspannungsenergie in andere Spannungsebenen regelt, Messung und Überwachung 
des Stromnetzes durchführt sowie die Steuerung der Stromverteilung regelt \parencite[S. 166]{syahputra_PowerTransformerLoadingAnalysisOrderImproveReliabilitySubstation_2017}.

Das zugrundeliegende Szenario basiert auf folgendem Anwendungsfall: Ein Energienetzbetreiber stellt einem Notfalltechniker ein zeitlich begrenztes Zertifikat aus, das den Zugang zu kritischer Infrastruktur, konkret einem Umspannwerk Nord-Ost, gewährt. Das Zutrittssystem verifiziert die Berechtigung des Technikers mittels Zero-Knowledge-Proofs, ohne dabei die Identität des Technikers offenzulegen. Dieser Ansatz realisiert das Datenschutzprinzip \enquote{Privacy by Design} gemäß Art. 25 der \ac{DSGVO}. Die dezentralisierte Natur von \ac{SSI}-Systemen ermöglicht dabei eine Entkopplung von Identitätsverwaltung und Zutrittskontrolle, wodurch einzelne Kontrollpunkte und zentralisierte Datenbanken als potenzielle Angriffsvektoren reduziert werden.

% 1. ISSUANCE \\
%    - Betreiber (Issuer) stellt "Notfall-Wartungszertifikat" aus \\
%    - Techniker (Holder) empfängt VC mit PQC-Signatur
   
% 2. VERIFICATION \\
%    - Techniker präsentiert Credential am Umspannwerk-Zugang \\
%    - Verifier (Zutrittssystem) verifiziert ZKP-Proof \\
%    - Zugang wird gewährt
   
% 3. REVOCATION (optional, aber impactful)
%    - Einsatz beendet --> Credential wird revoked
%    - Techniker versucht sich erneut zu authentifizieren
%    - Zugang wird nicht mehr gewährt

\subsubsection{Teil 1: Setup \& Verbindungstests}
\label{sec:Anhang_Teil1-Setup-Verbindungstests}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-1}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 1}, numbers=left, frame=single]
# Cell 1: Imports und Konfiguration
import requests
import json
import time
import pandas as pd
import tabulate
from IPython.display import display, Markdown, HTML
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# SSL Warnung für self-signed Zertifikate unterdrücken (PQC Proxy)
# import urllib3
# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Agent URLs
ISSUER_ADMIN_URL = "https://localhost:8021"
HOLDER_ADMIN_URL = "https://localhost:8031"
VERIFIER_ADMIN_URL = "https://localhost:8041"
VON_NETWORK_URL = "https://localhost:8000"  # PQC HTTPS Reverse Proxy

# Farben für Visualisierungen
ISSUER_COLOR = "#3498db"  # Blau
HOLDER_COLOR = "#2ecc71"  # Grün
VERIFIER_COLOR = "#e74c3c"  # Rot

print("Imports erfolgreich")
print(f"Issuer:   {ISSUER_ADMIN_URL}")
print(f"Holder:   {HOLDER_ADMIN_URL}")
print(f"Verifier: {VERIFIER_ADMIN_URL}")
print(f"Ledger:   {VON_NETWORK_URL}")

# ========================================
# Tails-Server Konfiguration (für Revocation)
# ========================================
TAILS_SERVER_URL = "https://localhost:6543"
TAILS_FILE_COUNT = 100  # Max Credentials per Registry

print(f"Tails:    {TAILS_SERVER_URL}")
print(f"   Max Credentials per Registry: {TAILS_FILE_COUNT}")

# ========================================
# Helper Functions
# ========================================

def api_get(url, path):
    """GET Request an ACA-Py Admin API"""
    try:
        response = requests.get(f"{url}{path}")
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"GET Fehler: {e}")
        return None

def api_post(url, path, data=None):
    """POST Request an ACA-Py Admin API"""
    try:
        response = requests.post(
            f"{url}{path}",
            json=data,
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"POST Fehler: {e}")
        if hasattr(e, 'response') and e.response:
            print(f"   Response: {e.response.text}")
        return None

def api_delete(url, path):
      """DELETE Request an ACA-Py Admin API"""
      try:
          response = requests.delete(
              f"{url}{path}",
              headers={"Content-Type": "application/json"}
          )
          response.raise_for_status()

          # DELETE kann leeren Body zurückgeben (204 No Content)
          if response.status_code == 204:
              return {}  # Success, aber kein Content

          # Versuche JSON zu parsen, falls vorhanden
          try:
              return response.json()
          except:
              return {}  # Success, aber kein JSON

      except Exception as e:
          print(f"DELETE Fehler: {e}")
          if hasattr(e, 'response') and e.response:
              print(f"   Response: {e.response.text}")
          return None

def pretty_print(data, title=""):
    """Formatierte JSON-Ausgabe"""
    if title:
        print(f"\n{'='*60}")
        print(f"  {title}")
        print(f"{'='*60}")
    print(json.dumps(data, indent=2))

def wait_for_status(url, path, field, expected_value, timeout=30, interval=1):
    """Wartet bis ein Feld einen bestimmten Wert hat"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        response = api_get(url, path)
        if response and response.get(field) == expected_value:
            return response
        time.sleep(interval)
    print(f"Timeout: {field} erreichte nicht '{expected_value}' nach {timeout}s")
    return None

def status_badge(status, label="Status"):
    """HTML Badge für Status"""
    colors = {
        "active": "green",
        "completed": "green",
        "done": "green",
        "ready": "green",
        "pending": "orange",
        "failed": "red",
        "error": "red"
    }
    color = colors.get(status.lower(), "gray")
    return f'<span style="background-color:{color};color:white;padding:3px 8px;border-radius:3px;font-weight:bold">{label}: {status}</span>'

print("\nSetup komplett!")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-1-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 1 Output}, numbers=left, frame=single]
Imports erfolgreich
Issuer:   https://localhost:8021
Holder:   https://localhost:8031
Verifier: https://localhost:8041
Ledger:   https://localhost:8000
Tails:    https://localhost:6543
Max Credentials per Registry: 100

Setup komplett!
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-2}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 2}, numbers=left, frame=single]
# Cell 2: Infrastruktur-Status prüfen (Health Check)

def check_agent_status(name, url):
    """Prüft ob ein Agent bereit ist"""
    try:
        response = api_get(url, "/status/ready")
        if response and response.get("ready"):
            return f"{name}: Bereit"
        else:
            return f"{name}: Nicht bereit"
    except:
        return f"{name}: Nicht erreichbar"

print("Agenten-Status Check...\n")
print(check_agent_status("Issuer", ISSUER_ADMIN_URL))
print(check_agent_status("Holder", HOLDER_ADMIN_URL))
print(check_agent_status("Verifier", VERIFIER_ADMIN_URL))

# Detaillierte Informationen mit Conductor-Statistiken
print("\nDetaillierte Agent-Informationen:\n")
agents_info = []
for name, url in [("Issuer", ISSUER_ADMIN_URL), ("Holder", HOLDER_ADMIN_URL), ("Verifier", VERIFIER_ADMIN_URL)]:
    status = api_get(url, "/status")
    if status:
        conductor = status.get("conductor", {})
        agents_info.append({
            "Agent": name,
            "Label": status.get("label", "N/A"),
            "Version": status.get("version", "N/A"),
            "In Sessions": conductor.get("in_sessions", 0),
            "Out Encode": conductor.get("out_encode", 0),
            "Out Deliver": conductor.get("out_deliver", 0),
            "Active": conductor.get("task_active", 0),
            "Done": conductor.get("task_done", 0),
            "Failed": conductor.get("task_failed", 0),
            "Pending": conductor.get("task_pending", 0)
        })

df = pd.DataFrame(agents_info)
print(df.to_json(orient='records', indent=2))

print("\nLedger-Verbindung testen...\n")

try:
    # Ledger-Status abrufen
    status_response = requests.get(f"{VON_NETWORK_URL}/status")
    if status_response.status_code == 200:
        print("Ledger-Status abrufbar")

    # Genesis-Datei abrufen
    genesis_response = requests.get(f"{VON_NETWORK_URL}/genesis")
    if genesis_response.status_code == 200:
        print(f"Ledger Genesis-Datei ({len(genesis_response.text)} Bytes) erreichbar\n")

except Exception as e:
    print(f"Ledger nicht erreichbar: {e}")
    print("   Bitte starte VON-Network: cd ../von-network && ./manage start --wait\n")

print(f"Tails Server Verbindung testen...\n")

try:
    # Tails Server Root-Endpoint testen (erwartet 404, da kein Root-Handler)
    tails_response = requests.get(f"{TAILS_SERVER_URL}")

    if tails_response.status_code == 404:
        print("Tails Server erreichbar (404 = Normal, kein Root-Endpoint)")
    elif tails_response.status_code == 200:
        print(f"Tails Server erreichbar (200 OK)")
    else:
        print(f"Tails Server antwortet mit Status {tails_response.status_code}")

    # Alternativer Test: Hash-Endpoint (sollte auch 404 geben ohne gültigen Hash)
    test_hash = "0" * 64  # Dummy-Hash für Test
    hash_response = requests.get(f"{TAILS_SERVER_URL}/{test_hash}")

    if hash_response.status_code in [404, 400]:
        print(f"Tails Server Hash-Endpoint antwortet (Status {hash_response.status_code})")

except requests.exceptions.ConnectionError as e:
    print(f"Tails Server nicht erreichbar: {e}")
    print("   Bitte starte Tails Server:")
    print("   cd ../indy-tails-server/docker && ./manage start")
except Exception as e:
    print(f"Fehler beim Tails Server Test: {e}")

# ========================================
# Zeige Ledger-Transaktionen
# ========================================
print("\n" + "="*60)
print("LEDGER-TRANSAKTIONEN")
print("="*60)

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])

        # Filtere nach seqNo 1-5
        target_seqnos = [1,2,3,4,5]
        found_txns = []

        for txn in ledger_txns:
            txn_metadata = txn.get('txnMetadata', {})
            seqno = txn_metadata.get('seqNo')

            if seqno in target_seqnos:
               found_txns.append(txn)

        # Sortiere nach seqNo aufsteigend
        found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

        if found_txns:
            print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

            for txn in found_txns:
                txn_metadata = txn.get('txnMetadata', {})
                txn_data = txn.get('txn', {})
                seqno = txn_metadata.get('seqNo')
                txn_type = txn_data.get('type')
                txn_time = txn_metadata.get('txnTime', 'N/A')

                # Typ-Mapping
                type_names = {
                    '0' : 'NODE (Validator Node Registration)',
                    '1' : 'NYM (DID Registration)',
                    '4' : 'TXN_AUTHOR_AGREEMENT',
                    '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                    '100': 'ATTRIB',
                    '101': 'SCHEMA',
                    '102': 'CRED_DEF',
                    '112': 'CHANGE_KEY',
                    '113': 'REVOC_REG_DEF',
                    '114': 'REVOC_REG_ENTRY',
                    '120': 'AUTH_RULE'
                }
                type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                print(f"============================================================")
                print(f"Seq No: {seqno} | Time: {txn_time}")
                print(f"Transaction Type: {type_name}")
                print(f"============================================================\n")

                # Zeige vollständige Transaction
                pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                print()

        else:
            print(f"\nNYM-Transaktion für DID {did_short} nicht gefunden")
            print(f"   (Möglicherweise noch nicht im Cache)")
    else:
        print(f"\nKonnte Ledger nicht abrufen: {ledger_response.status_code}")

except Exception as e:
    print(f"\nFehler beim Abrufen der Ledger-Transaktion: {e}")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-2-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 2 Output}, numbers=left, frame=single]
Agenten-Status Check...

Issuer: Bereit
Holder: Bereit
Verifier: Bereit

Detaillierte Agent-Informationen:

[
  {
    "Agent":"Issuer",
    "Label":"Issuer Agent",
    "Version":"1.3.2",
    "In Sessions":0,
    "Out Encode":0,
    "Out Deliver":0,
    "Active":1,
    "Done":352,
    "Failed":0,
    "Pending":0
  },
  {
    "Agent":"Holder",
    "Label":"Holder Agent",
    "Version":"1.3.2",
    "In Sessions":0,
    "Out Encode":0,
    "Out Deliver":0,
    "Active":1,
    "Done":400,
    "Failed":1,
    "Pending":0
  },
  {
    "Agent":"Verifier",
    "Label":"Verifier Agent",
    "Version":"1.3.2",
    "In Sessions":0,
    "Out Encode":0,
    "Out Deliver":0,
    "Active":1,
    "Done":333,
    "Failed":0,
    "Pending":0
  }
]

Ledger-Verbindung testen...

Ledger-Status abrufbar
Ledger Genesis-Datei (3099 Bytes) erreichbar

Tails Server Verbindung testen...

Tails Server erreichbar (404 = Normal, kein Root-Endpoint)
Tails Server Hash-Endpoint antwortet (Status 404)

============================================================
LEDGER-TRANSAKTIONEN
============================================================

5 Transaktionen gefunden:

============================================================
Seq No: 1 | Time: N/A
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 1)
============================================================
{
  "auditPath": [
    "3XtSyZ8CQPJUYbc5mFKvUendLZSt4ybG2Y4zRtJEewSL",
    "96irBGYpWrTvrVATexGGvktPrT3WicixwT8BtoZTtkYX",
    "Hf2vXibDGJUFB2sMyhEPZZNKEPEiY4iLFxaxckeXwgKx"
  ],
  "ledgerSize": 5,
  "reqSignature": {},
  "rootHash": "DJLzEifT7n9DbiCHqQ5KWrWUPFBiZt349popapDfzo5p",
  "txn": {
    "data": {
      "dest": "V4SGRU86Z58d6TV7PBUe6f",
      "role": "0",
      "verkey": "~CoRER63DVYnWZtK8uAzNbx"
    },
    "metadata": {},
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 1
  },
  "ver": "1"
}

============================================================
Seq No: 2 | Time: N/A
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 2)
============================================================
{
  "auditPath": [
    "JBMXqyYxQrtkq9AJjHueEiAtpGiNzVYXkgWqHkZo5zUi",
    "96irBGYpWrTvrVATexGGvktPrT3WicixwT8BtoZTtkYX",
    "Hf2vXibDGJUFB2sMyhEPZZNKEPEiY4iLFxaxckeXwgKx"
  ],
  "ledgerSize": 5,
  "reqSignature": {},
  "rootHash": "DJLzEifT7n9DbiCHqQ5KWrWUPFBiZt349popapDfzo5p",
  "txn": {
    "data": {
      "dest": "Th7MpTaRZVRYnPiabds81Y",
      "role": "2",
      "verkey": "~7TYfekw4GUagBnBVCqPjiC"
    },
    "metadata": {
      "from": "V4SGRU86Z58d6TV7PBUe6f"
    },
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 2
  },
  "ver": "1"
}

============================================================
Seq No: 3 | Time: N/A
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 3)
============================================================
{
  "auditPath": [
    "3CXeV5MsdAEeU4GLqwGhqvBFuSGAUVqn1xr77rfhWR4e",
    "D47RQhSxcBgWB1oBZwFeKom3Fvi343NTfpAdKb6uNLuw",
    "Hf2vXibDGJUFB2sMyhEPZZNKEPEiY4iLFxaxckeXwgKx"
  ],
  "ledgerSize": 5,
  "reqSignature": {},
  "rootHash": "DJLzEifT7n9DbiCHqQ5KWrWUPFBiZt349popapDfzo5p",
  "txn": {
    "data": {
      "dest": "EbP4aYNeTHL6q385GuVpRV",
      "role": "2",
      "verkey": "~RHGNtfvkgPEUQzQNtNxLNu"
    },
    "metadata": {
      "from": "V4SGRU86Z58d6TV7PBUe6f"
    },
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 3
  },
  "ver": "1"
}

============================================================
Seq No: 4 | Time: N/A
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 4)
============================================================
{
  "auditPath": [
    "51ENLADqdvbecXzFQGM5YDk6F9S995TyZfdwFnefKdEb",
    "D47RQhSxcBgWB1oBZwFeKom3Fvi343NTfpAdKb6uNLuw",
    "Hf2vXibDGJUFB2sMyhEPZZNKEPEiY4iLFxaxckeXwgKx"
  ],
  "ledgerSize": 5,
  "reqSignature": {},
  "rootHash": "DJLzEifT7n9DbiCHqQ5KWrWUPFBiZt349popapDfzo5p",
  "txn": {
    "data": {
      "dest": "4cU41vWW82ArfxJxHkzXPG",
      "role": "2",
      "verkey": "~EMoPA6HrpiExVihsVfxD3H"
    },
    "metadata": {
      "from": "V4SGRU86Z58d6TV7PBUe6f"
    },
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 4
  },
  "ver": "1"
}

============================================================
Seq No: 5 | Time: N/A
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 5)
============================================================
{
  "auditPath": [
    "D52hsZf4iH4Kp4x4eEp18FicbPNdirG9TL2cvat1eKvL"
  ],
  "ledgerSize": 5,
  "reqSignature": {},
  "rootHash": "DJLzEifT7n9DbiCHqQ5KWrWUPFBiZt349popapDfzo5p",
  "txn": {
    "data": {
      "dest": "TWwCRQRZ2ZHMJFn9TzLp7W",
      "role": "2",
      "verkey": "~UhP7K35SAXbix1kCQV4Upx"
    },
    "metadata": {
      "from": "V4SGRU86Z58d6TV7PBUe6f"
    },
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 5
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\subsubsection{Teil 2: DID Setup \& Ledger Registration - KRITIS-Identitäten}
\label{sec:Anhang_Teil2-DID-Setup-Ledger-Registration-KRITIS-Identitäten}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-3}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 3}, numbers=left, frame=single]
# Cell 3: Issuer - DID erstellen

print("Issuer: DID erstellen...\n")

# DID-Erstellungs-Parameter für did:indy
# API: POST /did/indy/create
did_params = {
    "options": {
        "key_type": "ed25519",  # Kryptografischer Key-Type (EdDSA)
        "seed": "000000000000000000000000Issuer01"  # 32 Zeichen für reproduzierbare DID
    }
}

print(f"DID-Parameter:")
print(f"   Method:   indy")
print(f"   Endpoint: /did/indy/create")
print(f"   Key Type: {did_params['options']['key_type']}")
print(f"   Seed:     {'(zufällig)' if 'seed' not in did_params['options'] else did_params['options']['seed']}")
print(f"   Format:   did:indy:<identifier>\n")

# DID erstellen mit korrektem Endpunkt
issuer_did_response = api_post(
    ISSUER_ADMIN_URL,
    "/did/indy/create",  # Korrekter Endpunkt für did:indy
    did_params
)

if issuer_did_response is not None:
    # Response-Format: {"did": "did:indy:...", "verkey": "..."}
    issuer_did = issuer_did_response.get("did")
    issuer_verkey = issuer_did_response.get("verkey")
    
    print(f"Issuer DID erstellt:")
    print(f"   DID:    {issuer_did}")
    print(f"   Verkey: {issuer_verkey}")
    
    # Speichern für spätere Verwendung
    issuer_info = {
        "did": issuer_did,
        "verkey": issuer_verkey,
        "method": "indy",
        "key_type": did_params["options"]["key_type"],
        "role": "ENDORSER"
    }
    
    print(f"\nFormat: {issuer_did} (vollständige qualified DID)")
else:
    print("Fehler beim Erstellen der Issuer DID")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-3-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 3 Output}, numbers=left, frame=single]
Issuer: DID erstellen...

DID-Parameter:
   Method:   indy
   Endpoint: /did/indy/create
   Key Type: ed25519
   Seed:     000000000000000000000000Issuer01
   Format:   did:indy:<identifier>

Issuer DID erstellt:
   DID:    did:indy:9pbXiFBZZGwXKp61HQBz3J
   Verkey: 2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm

Format: did:indy:9pbXiFBZZGwXKp61HQBz3J (vollständige qualified DID)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-4}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 4}, numbers=left, frame=single]
# Cell 4: Issuer DID auf Ledger registrieren

print("Issuer DID auf Ledger registrieren (via VON-Network)...\n")

# Registrierungs-Daten für VON-Network (DID-Based Registration)
# Nutzt DID+Verkey aus Cell 3 statt Seed
register_data = {
    "did": issuer_did,        # DID aus Cell 3 (ACA-Py Wallet)
    "verkey": issuer_verkey,  # Verkey aus Cell 3 (ACA-Py Wallet)
    "alias": "Energienetzbetreiber (Issuer)",
    "role": "ENDORSER"  # TRUST_ANCHOR oder ENDORSER für Issuer-Rechte
}

print(f"Registrierungs-Parameter:")
print(f"   Ledger URL: {VON_NETWORK_URL}/register")
print(f"   DID:        {register_data['did']}")
print(f"   Verkey:     {register_data['verkey'][:20]}...")
print(f"   Alias:      {register_data['alias']}")
print(f"   Role:       {register_data['role']}\n")

# Registriere DID auf VON-Network Ledger
print(f"POST {VON_NETWORK_URL}/register")

try:
    response = requests.post(
        f"{VON_NETWORK_URL}/register",
        json=register_data,
        timeout=10
    )

    response.raise_for_status()

    # Response format: {"did": "did:indy:...", "verkey": "..."}
    nym_info = response.json()

    issuer_did = nym_info["did"]
    issuer_verkey = nym_info["verkey"]

    print(f"Issuer DID auf Ledger registriert")
    print(f"   DID:    {issuer_did}")
    print(f"   Verkey: {issuer_verkey}")

    # Zeige vollständige Ledger-Response
    pretty_print(nym_info, "VON-Network Registration Response (Issuer)")

    # JETZT: Setze DID als Public DID im ACA-Py Wallet
    print(f"\nSetze DID als Public DID im Wallet...")
    public_did_response = api_post(
        ISSUER_ADMIN_URL,
        f"/wallet/did/public?did={issuer_did}"
    )

    if public_did_response is not None:
        print(f"Issuer DID als Public DID gesetzt")
    else:
        print(f"Warnung: Public DID konnte nicht gesetzt werden")

    # Speichern für spätere Verwendung
    issuer_info = {
        "did": issuer_did,
        "verkey": issuer_verkey,
        "method": "indy",
        "key_type": "ed25519",
        "role": "ENDORSER"
    }

    # Hole NYM Role vom Ledger zur Verifikation
    print(f"\nNYM Role vom Ledger verifizieren...")
    nym_role_response = api_get(
        ISSUER_ADMIN_URL,
        f"/ledger/get-nym-role?did={issuer_did}"
    )

    if nym_role_response is not None:
        print(f"NYM Role vom Ledger abgerufen")
        #pretty_print(nym_role_response, "Ledger NYM Role (Issuer DID)")

        # Extrahiere Role
        role = nym_role_response.get('role', 'N/A')
        print(f"\nVerifizierte Rolle: {role}")
    else:
        print(f"NYM Role konnte nicht abgerufen werden")

    # ========================================
    # Zeige Ledger-Transaktionen
    # ========================================
    print("\n" + "="*60)
    print("LEDGER-TRANSAKTIONEN")
    print("="*60)

    try:
        # Hole Domain Ledger Transaktionen
        ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

        if ledger_response.status_code == 200:
            ledger_data = ledger_response.json()
            ledger_txns = ledger_data.get('results', [])

            # Filtere nach seqNo 6 und 7
            target_seqnos = [6, 7]
            found_txns = []

            for txn in ledger_txns:
                txn_metadata = txn.get('txnMetadata', {})
                seqno = txn_metadata.get('seqNo')

                if seqno in target_seqnos:
                    found_txns.append(txn)

            # Sortiere nach seqNo aufsteigend
            found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

            if found_txns:
                print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

                for txn in found_txns:
                    txn_metadata = txn.get('txnMetadata', {})
                    txn_data = txn.get('txn', {})
                    seqno = txn_metadata.get('seqNo')
                    txn_type = txn_data.get('type')
                    txn_time = txn_metadata.get('txnTime', 'N/A')

                    # Typ-Mapping
                    type_names = {
                        '0' : 'NODE (Validator Node Registration)',
                        '1' : 'NYM (DID Registration)',
                        '4' : 'TXN_AUTHOR_AGREEMENT',
                        '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                        '100': 'ATTRIB',
                        '101': 'SCHEMA',
                        '102': 'CRED_DEF',
                        '112': 'CHANGE_KEY',
                        '113': 'REVOC_REG_DEF',
                        '114': 'REVOC_REG_ENTRY',
                        '120': 'AUTH_RULE'
                    }
                    type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                    print(f"============================================================")
                    print(f"Seq No: {seqno} | Time: {txn_time}")
                    print(f"Transaction Type: {type_name}")
                    print(f"============================================================\n")

                    # Zeige vollständige Transaction
                    pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                    print()

            else:
                print(f"\nNYM-Transaktion für DID {did_short} nicht gefunden")
                print(f"   (Möglicherweise noch nicht im Cache)")
        else:
            print(f"\nKonnte Ledger nicht abrufen: {ledger_response.status_code}")

    except Exception as e:
        print(f"\nFehler beim Abrufen der Ledger-Transaktion: {e}")

    print("="*60)

except requests.exceptions.RequestException as e:
    print(f" POST Fehler: {e}")
    print(f"   Stelle sicher dass VON-Network läuft: cd von-network && ./manage start")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-4-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 4 Output}, numbers=left, frame=single]
Issuer DID auf Ledger registrieren (via VON-Network)...

Registrierungs-Parameter:
   Ledger URL: https://localhost:8000/register
   DID:        did:indy:9pbXiFBZZGwXKp61HQBz3J
   Verkey:     2zoa6G7aMfX8GnUEpDxx...
   Alias:      Energienetzbetreiber (Issuer)
   Role:       ENDORSER

POST https://localhost:8000/register
Issuer DID auf Ledger registriert
   DID:    did:indy:9pbXiFBZZGwXKp61HQBz3J
   Verkey: 2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm

============================================================
  VON-Network Registration Response (Issuer)
============================================================
{
  "did": "did:indy:9pbXiFBZZGwXKp61HQBz3J",
  "seed": null,
  "verkey": "2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm"
}

Setze DID als Public DID im Wallet...
Issuer DID als Public DID gesetzt

NYM Role vom Ledger verifizieren...
NYM Role vom Ledger abgerufen

Verifizierte Rolle: ENDORSER

============================================================
LEDGER-TRANSAKTIONEN
============================================================

2 Transaktionen gefunden:

============================================================
Seq No: 6 | Time: 1765198429
Transaction Type: NYM (DID Registration)
============================================================


============================================================
  Ledger Transaction (seqNo 6)
============================================================
{
  "auditPath": [
    "Hf2vXibDGJUFB2sMyhEPZZNKEPEiY4iLFxaxckeXwgKx",
    "FRQ6sxzHrGmFgEcVDRGZ55wAMJ85fbHBusjMUMrHsJJ8",
    "D52hsZf4iH4Kp4x4eEp18FicbPNdirG9TL2cvat1eKvL"
  ],
  "ledgerSize": 7,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "V4SGRU86Z58d6TV7PBUe6f",
        "value": "CKhwb29Mt4pwW5Cd9jD8gSt3uCCWreuAEm4ypb9iFkh
        5i8puaHpsoa585fVReDXNP6DzDF5xUF7U4yj4oeSEpjw"
      }
    ]
  },
  "rootHash": "8mjvdSyKXsKYC39AZEKvEqqdHq87L5a1ebcPgXUi5jj6",
  "txn": {
    "data": {
      "alias": "Energienetzbetreiber (Issuer)",
      "dest": "9pbXiFBZZGwXKp61HQBz3J",
      "role": "101",
      "verkey": "2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm"
    },
    "metadata": {
      "digest": "6543a880c8311e2a231d588ac1b6a0d53044c51d5c70d15c12b8534af7483815",
      "from": "V4SGRU86Z58d6TV7PBUe6f",
      "payloadDigest": "6c05072098125c2479788ca79f7ecf6458efeaf557e31c699d736c7f501e5d5c",
      "reqId": 1765198429962886624
    },
    "protocolVersion": 2,
    "type": "1"
  },
  "txnMetadata": {
    "seqNo": 6,
    "txnId": "fd0d9a9b20213eef708dd54b3129d19875de67c8003f28b7c496eef9786cc176",
    "txnTime": 1765198429
  },
  "ver": "1"
}

============================================================
Seq No: 7 | Time: 1765198432
Transaction Type: ATTRIB
============================================================


============================================================
  Ledger Transaction (seqNo 7)
============================================================
{
  "auditPath": [
    "BXtA9jxNKoyR4nxwq7VQn6L5CFLFGiAHQHEbCfAMCHHW",
    "D52hsZf4iH4Kp4x4eEp18FicbPNdirG9TL2cvat1eKvL"
  ],
  "ledgerSize": 7,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "5Yv6acNxZzTpxudAFoU5NrcxWWHy1YEm2Y7myLXFs
        fJxRpDfyPaRjnXoVjdw255BLwJFBJLhzYGUFrcCmKmkmx48"
      }
    ]
  },
  "rootHash": "8mjvdSyKXsKYC39AZEKvEqqdHq87L5a1ebcPgXUi5jj6",
  "txn": {
    "data": {
      "dest": "9pbXiFBZZGwXKp61HQBz3J",
      "raw": "{\"endpoint\":{\"endpoint\":\"https://host.docker.internal:8020\",\"routingKeys\":[]}}"
    },
    "metadata": {
      "digest": "2268552566b77234a02fd74963edf4fe9146b005c21fc5e04d393b07b2e9511b",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "aa3cb2db276d6d30b8104d631be4a4bac47b8bb2679f0e97105178ec0d828796",
      "reqId": 1765198430095718777
    },
    "protocolVersion": 2,
    "type": "100"
  },
  "txnMetadata": {
    "seqNo": 7,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:1:b6bf7bc8d96f3ea9d132c83b3da8e7760e420138485657372db4d6a981d3fd9e",
    "txnTime": 1765198432
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-5}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 5}, numbers=left, frame=single]
# Cell 5: Wallet DID Übersicht anzeigen

print("Wallet-Übersicht - Alle DIDs:\n")

# Alle DIDs von allen Agenten abrufen
issuer_dids = api_get(ISSUER_ADMIN_URL, "/wallet/did")
holder_dids = api_get(HOLDER_ADMIN_URL, "/wallet/did")
verifier_dids = api_get(VERIFIER_ADMIN_URL, "/wallet/did")

#Zeige originale Responses
print("Originale /wallet/did Responses:\n")
if issuer_dids:
    pretty_print(issuer_dids, "Issuer Wallet DIDs")
if holder_dids:
    pretty_print(holder_dids, "Holder Wallet DIDs")
if verifier_dids:
    pretty_print(verifier_dids, "Verifier Wallet DIDs")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-5-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 5 Output}, numbers=left, frame=single]
Wallet-Übersicht - Alle DIDs:

Originale /wallet/did Responses:


============================================================
  Issuer Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:indy:9pbXiFBZZGwXKp61HQBz3J",
      "verkey": "2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm",
      "posture": "posted",
      "key_type": "ed25519",
      "method": "indy",
      "metadata": {
        "posted": true,
        "endpoint": "https://host.docker.internal:8020"
      }
    }
  ]
}

============================================================
  Holder Wallet DIDs
============================================================
{
  "results": []
}

============================================================
  Verifier Wallet DIDs
============================================================
{
  "results": []
}
\end{lstlisting}

\subsubsection{Teil 3: Schema \& Credential Definition - Notfall-Wartungszertifikat}
\label{sec:Anhang_Teil3-Schema-Credential-Definition-Notfall-Wartungszertifikat}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-6}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 6}, numbers=left, frame=single]
# Cell 6: Schema erstellen (Indy)

print("Schema erstellen...\n")

start_time = time.time()

# Schema-Daten für KRITIS-Notfall-Wartungszertifikat (Indy API)
# API: POST /schemas
# Format: {"schema_name": "...", "schema_version": "...", "attributes": [...]}
schema_data = {
    "schema_name": "kritis_emergency_maintenance_cert",
    "schema_version": "1.1",  # <-- WICHTIG: Version muss hier angegeben werden!
    "attributes": [
        "first_name",
        "name",
        "organisation",
        "role",
        "cert_type",
        "facility_type",
        "epoch_valid_from",
        "epoch_valid_until",
        "security_clearance_level"
    ]
}

# Schema auf Ledger registrieren (Indy endpoint)
schema_response = api_post(
    ISSUER_ADMIN_URL,
    "/schemas",  # Indy endpoint
    schema_data
)

if schema_response is not None:
    # Response Format: {"schema_id": "...", "schema": {...}}
    schema_id = schema_response.get("schema_id")

    # Korrekte Werte aus Response extrahieren
    schema_obj = schema_response.get("schema", {})
    schema_name = schema_obj.get("name", schema_data["schema_name"])
    schema_version = schema_obj.get("version", schema_data["schema_version"])
    schema_attributes = schema_obj.get("attrNames", schema_data["attributes"])

    print(f"Schema 'kritis_emergency_maintenance_cert' erstellt (Indy)")
    print(f"   Schema ID: {schema_id}")
    print(f"   Endpoint:  /schemas")
    print(f"   Name:      {schema_name}")
    print(f"   Version:   {schema_version}")
    print(f"   Attribute: {', '.join(schema_attributes)}\n")

else:
    print("Fehler beim Erstellen des Schemas")

# ========================================
# Zeige Ledger-Transaktionen
# ========================================
print("\n" + "="*60)
print("LEDGER-TRANSAKTIONEN")
print("="*60)

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])

        # Filtere nach seqNo 8
        target_seqnos = [8]
        found_txns = []

        for txn in ledger_txns:
            txn_metadata = txn.get('txnMetadata', {})
            seqno = txn_metadata.get('seqNo')

            if seqno in target_seqnos:
               found_txns.append(txn)

        # Sortiere nach seqNo aufsteigend
        found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

        if found_txns:
            print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

            for txn in found_txns:
                txn_metadata = txn.get('txnMetadata', {})
                txn_data = txn.get('txn', {})
                seqno = txn_metadata.get('seqNo')
                txn_type = txn_data.get('type')
                txn_time = txn_metadata.get('txnTime', 'N/A')

                # Typ-Mapping
                type_names = {
                    '0' : 'NODE (Validator Node Registration)',
                    '1' : 'NYM (DID Registration)',
                    '4' : 'TXN_AUTHOR_AGREEMENT',
                    '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                    '100': 'ATTRIB',
                    '101': 'SCHEMA',
                    '102': 'CRED_DEF',
                    '112': 'CHANGE_KEY',
                    '113': 'REVOC_REG_DEF',
                    '114': 'REVOC_REG_ENTRY',
                    '120': 'AUTH_RULE'
                }
                type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                print(f"============================================================")
                print(f"Seq No: {seqno} | Time: {txn_time}")
                print(f"Transaction Type: {type_name}")
                print(f"============================================================\n")

                # Zeige vollständige Transaction
                pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                print()

        else:
            print(f"\nNYM-Transaktion für DID {did_short} nicht gefunden")
            print(f"   (Möglicherweise noch nicht im Cache)")
    else:
        print(f"\nKonnte Ledger nicht abrufen: {ledger_response.status_code}")

except Exception as e:
    print(f"\nFehler beim Abrufen der Ledger-Transaktion: {e}")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-6-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 6 Output}, numbers=left, frame=single]
Schema erstellen...

Schema 'kritis_emergency_maintenance_cert' erstellt (Indy)
   Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
   Endpoint:  /schemas
   Name:      kritis_emergency_maintenance_cert
   Version:   1.1
   Attribute: security_clearance_level, epoch_valid_from, organisation, facility_type, first_name, epoch_valid_until, name, cert_type, role


============================================================
LEDGER-TRANSAKTIONEN
============================================================

1 Transaktionen gefunden:

============================================================
Seq No: 8 | Time: 1765198929
Transaction Type: SCHEMA
============================================================


============================================================
  Ledger Transaction (seqNo 8)
============================================================
{
  "auditPath": [
    "FRQ6sxzHrGmFgEcVDRGZ55wAMJ85fbHBusjMUMrHsJJ8",
    "BXtA9jxNKoyR4nxwq7VQn6L5CFLFGiAHQHEbCfAMCHHW",
    "D52hsZf4iH4Kp4x4eEp18FicbPNdirG9TL2cvat1eKvL"
  ],
  "ledgerSize": 8,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "559SjbmTJSn4f3J46jN4cP9kcWLLWyGpminVfms
        1JF9TCkx4JPkByFdaKqxKPiTxHV8dXtoUemNfPnQdYPMNjTE9"
      }
    ]
  },
  "rootHash": "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP",
  "txn": {
    "data": {
      "data": {
        "attr_names": [
          "cert_type",
          "first_name",
          "organisation",
          "epoch_valid_until",
          "role",
          "epoch_valid_from",
          "name",
          "security_clearance_level",
          "facility_type"
        ],
        "name": "kritis_emergency_maintenance_cert",
        "version": "1.1"
      }
    },
    "metadata": {
      "digest": "4d343a477a3ca559213e5f0b4305db42067ce07bf9f4a1845ae2298874e8b7e7",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "55cbeb77839b6c473e9036784b0b40f0ab7ab0e923b78a60fa441f9224a89322",
      "reqId": 1765198929055445431
    },
    "protocolVersion": 2,
    "type": "101"
  },
  "txnMetadata": {
    "seqNo": 8,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1",
    "txnTime": 1765198929
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-7}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 7}, numbers=left, frame=single]
# Cell 7: Credential Definition erstellen (Indy)

print("Credential Definition erstellen...\n")

start_time = time.time()

# Credential Definition Data (Indy API)
# API: POST /credential-definitions
# Format: {"schema_id": "...", "tag": "...", "support_revocation": bool, ...}
cred_def_data = {
    "schema_id": schema_id,  # from Cell 11
    "tag": "default",
    "support_revocation": True,  # Enable revocation
    "revocation_registry_size": TAILS_FILE_COUNT  # from config
}

print(f"Credential Definition Parameter (Indy):")
print(f"   Endpoint:            /credential-definitions")
print(f"   Schema ID:           {cred_def_data['schema_id']}")
print(f"   Tag:                 {cred_def_data['tag']}")
print(f"   Support Revocation:  {cred_def_data['support_revocation']}")
print(f"   RevReg Size:         {cred_def_data['revocation_registry_size']}\n")

# Create Credential Definition on Ledger (Indy endpoint)
cred_def_response = api_post(
    ISSUER_ADMIN_URL,
    "/credential-definitions",  # Indy endpoint
    cred_def_data
)

if cred_def_response is not None:
    # Response Format: {"credential_definition_id": "..."}
    cred_def_id = cred_def_response.get("credential_definition_id")

    print(f"Credential Definition erstellt (Indy)")
    print(f"   Cred Def ID: {cred_def_id}")
    print(f"   für Schema 'kritis_emergency_maintenance_cert'")

    # Show full Cred Def Response
    # pretty_print(cred_def_response, "Indy Credential Definition Response (KRITIS)")

else:
    print("Fehler beim Erstellen der Credential Definition")

# ========================================
# Zeige Ledger-Transaktionen
# ========================================
print("\n" + "="*60)
print("LEDGER-TRANSAKTIONEN")
print("="*60)

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])

        # Filtere nach seqNo 9-13
        target_seqnos = [9,10,11,12,13]
        found_txns = []

        for txn in ledger_txns:
            txn_metadata = txn.get('txnMetadata', {})
            seqno = txn_metadata.get('seqNo')

            if seqno in target_seqnos:
               found_txns.append(txn)

        # Sortiere nach seqNo aufsteigend
        found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

        if found_txns:
            print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

            for txn in found_txns:
                txn_metadata = txn.get('txnMetadata', {})
                txn_data = txn.get('txn', {})
                seqno = txn_metadata.get('seqNo')
                txn_type = txn_data.get('type')
                txn_time = txn_metadata.get('txnTime', 'N/A')

                # Typ-Mapping
                type_names = {
                    '0' : 'NODE (Validator Node Registration)',
                    '1' : 'NYM (DID Registration)',
                    '4' : 'TXN_AUTHOR_AGREEMENT',
                    '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                    '100': 'ATTRIB',
                    '101': 'SCHEMA',
                    '102': 'CRED_DEF',
                    '112': 'CHANGE_KEY',
                    '113': 'REVOC_REG_DEF',
                    '114': 'REVOC_REG_ENTRY',
                    '120': 'AUTH_RULE'
                }
                type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                print(f"============================================================")
                print(f"Seq No: {seqno} | Time: {txn_time}")
                print(f"Transaction Type: {type_name}")
                print(f"============================================================\n")

                # Zeige vollständige Transaction
                pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                print()

        else:
            print(f"\nNYM-Transaktion für DID {did_short} nicht gefunden")
            print(f"   (Möglicherweise noch nicht im Cache)")
    else:
        print(f"\nKonnte Ledger nicht abrufen: {ledger_response.status_code}")

except Exception as e:
    print(f"\nFehler beim Abrufen der Ledger-Transaktion: {e}")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-7-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 7 Output}, numbers=left, frame=single]
Credential Definition erstellen...

Credential Definition Parameter (Indy):
   Endpoint:            /credential-definitions
   Schema ID:           9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
   Tag:                 default
   Support Revocation:  True
   RevReg Size:         100

Credential Definition erstellt (Indy)
   Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
   für Schema 'kritis_emergency_maintenance_cert'

============================================================
LEDGER-TRANSAKTIONEN
============================================================

5 Transaktionen gefunden:

============================================================
Seq No: 9 | Time: 1765199023
Transaction Type: CRED_DEF
============================================================


============================================================
  Ledger Transaction (seqNo 9)
============================================================
{
  "auditPath": [
    "HXBvJYp4raWvxc2YPGZi63GAWfXaNfo3tnNMfzd1M1S4",
    "64KsDF1ChEFno9AQVi6QbzEZ6RNkEJTgtY4SmoSQierj",
    "5YBFLtPW8BxW8JhBXAct4wLUhP53zefZY3svqsoq12K2",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 13,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "4TqLMUsejUKG1EADKrcimaTtAxT27pk6Y1
        vByxWefhbJegk4R9dTZxkGNMa2P98F3y74NB4eDwaQj3pEfvUnbygD"
      }
    ]
  },
  "rootHash": "41ARQa41wPiJpbwoFr7w6BB3Y66iLWHbmuz84tBEFcnC",
  "txn": {
    "data": {
      "data": {
        "primary": {
          "n": "10281269393334237618332478833085297128592880416
          56445018770380780626224731955086402966982174592411507
          93759292164843375581672870792507549609025881070059029
          99569012059221624437458652649731002744035535072320510
          97344640555305786309596398862760811477348509755096682
          47559536388723322491759881501464967142625042021950634
          74590709379823755520735193613328104190124820485724717
          87489364047623394746143909278968769630514388888301865
          20447778436083953377999301856507517057286042679094343
          02052061179063103318370506401665278448306890418334730
          93881952461509631538635253749457994127628787342472808
          05983902362485870240931935428443715170617",
          "r": {
            "cert_type": "3525685885350642347250119958873755026811
            522957633518547243819961520253729521705163390662128281
            100659947377548873990726118958567587241884779850947015
            446851470518621174242260246721675406567109423807648585
            318611318059018156362361746259096067266038723705895596
            093220876250095280642963640906557260166112372721481849
            431989843865509453854888864631966692285404502280578057
            721918868775019396126017468321237588005951999342667101
            975567244876212861888855184811784221600579007710425171
            580609648567241669678198182453162758482023440157426590
            771691619172133149688851713491117156155008197673213358
            8753167139593207775727367218011311435",
            "epoch_valid_from": "380958242182960111101735725593219
            733656248567836504315284766910602146229358450478114539
            823025198949991149752275622931000244720454886784938724
            905450550272522283065578981652175306387996059549815777
            727544879899970458395759845156506549712625970899670388
            294207583619057021604284491705772446291970772201534494
            508126302873164672075230882118194194482629654114093467
            700426084200199029421813162173249246860419010465848195
            910707633747729737872324329237147300460971848764641435
            740152863156599866188656662672097588486655892715746421
            143307105793122123488392631257141209577017837760438217
            94580097290124958486287493355133147337003421",
            "epoch_valid_until": "14680973432549975411991157440156
            663509397726968803380045588761027724699355084476591883
            825212227668972786850841544036009660450745543286264699
            504034963264074899062628030258629527045078406239725981
            856646770616598113739789473990417637278478254731994688
            444076738842870892019139181820137596231408915354234357
            291843105341408637622019951283953321418913649847371144
            561934536619979055859996425133654370381497645561219662
            518390269065734498893970569554769539966440206291530507
            859191987490188350682362968089839830900632515414715474
            400865274288497289386900592043713923317231529369332054
            404647013055752325075709159974250910946425583",
            "facility_type": "595062717575710994668238940938734807
            858575150740865779816868809186588085276888258189260457
            326049240633993158525453995004516858128770077478764760
            662707092517416575220760082917285325018066989220270025
            711287446172813428258780648073156550805977523302997930
            686506926939014462528722726248601504558276505002584521
            825383154783835244235063460415019367052996073733314190
            193724578386714250833834627568238450593994554750661863
            214642667657206798973824064493931047108909153015901843
            795129099299478918553371157605482045951546345220520602
            240920027041744720666613936460560291287847757577474685
            27127573953191722275450519507670497177873",
            "first_name": "156121461519380424298292418718574326075
            563477678371398032007947311493792097457113994437563300
            441410991031578330544123120318869675398369848012575096
            520704231860406198878736598366391910707963475328379018
            314895620212026100081783916157950742330812507451654772
            195235072646107139333385682816746471881809335591903532
            612594619310336857535281350175402306344253831189945592
            318245150090617512552186258935479522734234542082002810
            461645859200972302475885788346000864398002658844962257
            148412130810688041122837038411449830832849097436729705
            718116029828253757455923845600999489690503882501644058
            78648451041284133390680301004111208528",
            "master_secret": "719150954133244752183971594948903479
            524386396044792841937602285125175381921943925950159304
            132642575681280883971815934361686747663056680829710335
            122664989324241304858850947417770121260898653831591484
            502093386296347400423866526127863855963487691236562850
            005973339049657354203655413953447224497040475714353790
            361203703948242307253413683020978096168369038903109479
            232973563084115278209020768503212503261781340334895483
            780220128186741136754232078123977754958748796014376633
            965241148577842676882513978031731187800872781548919835
            245340140217768127025745996617405150806112385829127806
            79067808174083253800413580518678417409503",
            "name": "507139475209330359412041956673646670382290502
            510997416327398817990410291634095878779739727910620391
            983813348489743782983974205910608455201402883022363030
            657204230300217006003472034232777595827700699275349376
            472726421030688458301642805319268862572699616380281698
            149256210425437977844531965423074013789873092642069860
            90753",
            "organisation": "9510167264021885614687405535743007524
            944893016889819260229094716729897861649969889502212036
            750229321538650734200306330127232773689469939865899689
            070492516615728465238729278988718066975226971042518665
            755377399212641580599260501592205033642808816133021976
            572658227614869828819166175593661966013773892694492844
            973525941689522644329814367767130334560379852921542238
            823637342059481674303290141817243620937852064276411051
            461547697738554530590173404965337504858475863740776919
            099876982003005084766640552204421675776338549528241998
            551725146322875329563300713279344259481053544683702840
            3340688276237893961510821932537077108757",
            "role": "395563530099701163940652627405054048882798766
            671108715408726324441215481630357827076196778750277568
            825696638995484876172410688858855685967884299877578926
            313443583940896238443350491578812374333978665353059699
            559596066426343077451261853141537961994523028629361166
            627603150886778871652810770119903019741562000480005972
            417108472300887340241335187817323918093045791638191059
            822767733463880782634841649824280767803969156144624834
            311404319066342132889616536476376325317953921950076691
            046725598460010439417490774915156339698785734342893700
            483387033327532788710191333995991069122120984481405860
            68776725335189772775700329133081",
            "security_clearance_level": "6242343544550071290094928
            907663448117977302593196784651825226373678939199440163
            058504073554154551504418212878512647463864530577679028
            748167713027065237097745671554621508963264018526322739
            581125596500064997781988824069404787807028616860954572
            100191611669962108830553784598980577471544467683754087
            439323758090986602678757392652507765667697136527869728
            357212776026127704628479865875630145065356272784507801
            744550339507529178589488999294507749879180468118466591
            599407835762727403094304443481881622890646807037243874
            678766414454127205619056010634586741238280596763388454
            1757073335422180206932822214748818701389818522631849"
          },
          "rctxt": "5947310527661084384748647312463192325141246383
          47616798040672521583722939106309568449503900893285255616
          50453598319402301909030992815747417296976389802026846055
          28040077217793513856935519667280507580873412342478848532
          63872861111841347560302528192995674941761133276177105758
          74232802793221771136257567273261623626309115862132166003
          29601336226710484891262929559472657348551152875499590412
          83480406987986371126564272173879797367626913562453294597
          38118109227774538122205390834311723056800063504883375713
          40020297094930055480852885349234400529877113567936709288
          95651459522039680721583208345252600137326987986347865707
          80935520753",
          "s": "12873617048395836551114021666870494025940732272258
          07142311975745755037449094854068242764183799610351256940
          96180757605115092996148538833713889914761563504267010009
          78655460346790310693584903494368553315154894854372130025
          03367492870403106872379854024361508037869672001645006624
          05480480616887078682437650173479706490426577514119682117
          07507287185289426693375309283840686182584386054276692070
          30049654122525975977503151500893197004178542123523919427
          65324441693655199998002576592059342673368070355581784832
          89960384295665116980339920365968094349605442941192654967
          93015158786601777049012146366575689229151842993912537090
          6478823",
          "z": "73384336442087516782473287267893145097355060302719
          33374171448084139560902622271515791540226664868914406492
          83363468521071504605528850748433868113694917847583347487
          81341004570547273626870861071958357386604074519867023492
          43514256733427273956300875684357024814017675721308215893
          04228817271509714380985223843374491993219609026706569014
          51244195541137304851988055172958032294840018986849268976
          52639443626296323070408856000040144453441420125396911859
          06307847139296560108552130246296647424156547384831686404
          99611136839595140962073025382619435298737836124060714924
          40489043493096096162490628969717340223967068619925497205
          8685399"
        },
        "revocation": {
          "g": "1 21FEEC46E1C43CCA3541F7C85036C038BB1D15F2346A70E7B6A99E97ACA2ABF3 1 0BF2338DA407B403FEEAA66A815BAB2CA947BC16FAAC0A27B0F75A24B40AF6AE 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "g_dash": "1 11C1BE23AA3B6D48C85D5CC35B72D5D2894C55F4EB807FA04D5718BBE8DEA58E 1 01E209A6288CE30FB4E5DDF374E67C7658457584DDCC13571814EF0B430DE863 1 07EE4C529386375295AE6C6C6EF7CC2CC3A47D9C57157D26ED4C827EA384E25B 1 1247469442701176BC4722E3A2569885495EDEF2B4FBD7C3BB31963E8DAB340F 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000",
          "h": "1 0350F44E32F3B7BBE9887975BE9ABFFD449491B9458B780FA63622643FABAA40 1 143013A6ADA0B41DC971B5B85488E4EEE4890D6F3FE5DD8FF580CB59CC41D35D 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "h0": "1 235A21285F19F8226B059F5D8A37BBE5B2610690846F187088245431EB86D84B 1 0379E3547D757C030526C22AC07604A2F23CE0738A1F2515FA046596DD37E548 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "h1": "1 0C50BA42B915AB0625073C631BD55A750F0D92330E1A23A919C2887498E6CC2E 1 139271C2D29FC899E4C6940E475CA17455260F9EBA1EEC691098F5AD655AF30C 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "h2": "1 091EC10601A87DD5B0ADB478965632AD286C4E9A3412605C27BD207DA858B1A9 1 16B306080938CA215F69474DE7D43E9E2DBDA9108EC9EE6E865D3DA60B3AA3BE 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "h_cap": "1 1BACBBBA47D6C001B252E778B3C4EA515D01685205C04F366B1EEC0928E98C80 1 1762246D1D158E081B4D58DE4D02E61CFB6C34E9BB04E1ECDDBDF1E4DF03468B 1 001D83077D1F49CFE0F78815D46648FAB04B19D90E2B3047D1AE97036E34C151 1 0F0E549AC7BDE19CF57CA7A171928B1ADF80F75FDF9D1501D9B026648C8E13DC 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000",
          "htilde": "1 048DFD4781E8D6A2CE01139C9F1C699A92B4721DDFC84A80CAFCEE8250717421 1 06257BF17957918929D295E4BDE19BFCBD87ED9E8AB77DCB7EF00268D00743BB 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "pk": "1 14E077B9F2F44D1B9D3AB71D8B81BD61A70BE10A98C48F5628EA8685A31675F7 1 032738CDA9092D0118B4D750F83B295A80C9474EB79C6A0300CBDDCB53A9761A 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8",
          "u": "1 108655DCC4B1EACAA4B7757E102535CA29A49F448C72CBFE1B82E0394CF29BBB 1 1CEF7513554127E657A8AB0D4BB54ED5BA530ED6B1DA28479A839091A27CA0A4 1 24568BC9D82CC22BC0A5014531E433D209F649DFB699773A1CDC5F5E1DF0797D 1 194FF39230149E9B956987B022F531258BB0BFBDE24B0A462D50E2C42CC5992C 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000",
          "y": "1 0031326F5BC293C99130DBB1DA86D9516DE05255178A7DA34540A011DD562BD1 1 1820A398F316EAA4B202CBCCBE99775786EFD9B9EC3AB0E539FDB46C1E636F54 1 0D3AE01F7205C54092E64B437F2BB906C3122D75809F8927B0D54F2DD4A5206D 1 1843FE39544CA216C342CAA311578749283C4EB89E530B760C404449E641FC81 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000"
        }
      },
      "ref": 8,
      "signature_type": "CL",
      "tag": "default"
    },
    "metadata": {
      "digest": "4f34ae74ad28812dc6614a99273e8bc02986d620b9faa87489825242f89f5d6b",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "478c9f4fb2f1ccdb7856ffd047d162b99f15bff518d3bcd1f45d67a2929ede65",
      "reqId": 1765199023412922267
    },
    "protocolVersion": 2,
    "type": "102"
  },
  "txnMetadata": {
    "seqNo": 9,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default",
    "txnTime": 1765199023
  },
  "ver": "1"
}

============================================================
Seq No: 10 | Time: 1765199026
Transaction Type: REVOC_REG_DEF
============================================================


============================================================
  Ledger Transaction (seqNo 10)
============================================================
{
  "auditPath": [
    "7iNZ3PH7berMB115bDa5CuipYJ5NYvwERM3qRDPsVLRi",
    "64KsDF1ChEFno9AQVi6QbzEZ6RNkEJTgtY4SmoSQierj",
    "5YBFLtPW8BxW8JhBXAct4wLUhP53zefZY3svqsoq12K2",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 13,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "522wXmkJ56tLc9zLBLpvjmQrrxXacwcUEmZpo
        zB1NNveECVAxiYxNV8GcYUJ1yXTKnVEBgUpQmQr5AgfD7S8oH9q"
      }
    ]
  },
  "rootHash": "41ARQa41wPiJpbwoFr7w6BB3Y66iLWHbmuz84tBEFcnC",
  "txn": {
    "data": {
      "credDefId": "9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default",
      "id": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
      "revocDefType": "CL_ACCUM",
      "tag": "9efc6971-b713-465f-85db-a192b107b73e",
      "value": {
        "issuanceType": "ISSUANCE_BY_DEFAULT",
        "maxCredNum": 100,
        "publicKeys": {
          "accumKey": {
            "z": "1 171D58BB195883545AB3E075FAEB7011618C90C2AB6A2E46E28989FC7D0E46F9 1 06EECD5903553820608EAC558B4CD9A8F294EB5C10ED8566A47CB8A8FAD82076 1 1AAB928BF34F09D918D7B1C74A1186F04AE6C94C27D77B94AB4DD276DCDD44EE 1 01815895F1C206294EE73CAB992CB672F58157C3F0238382E5B6917C8EEBA021 1 09B5A7ECED3FC84AF833983FB871CA7A46E1F18351AB52F5796BDF455A3BFDC1 1 0F6F11AB712B7192B397A9C4CC93946E2B455DF496F0070BF88B0A9452B4C8C3 1 1AD33C8292498B28A92BD819A0B08BBA8C063B59C83AE972085DE80AD625EA8E 1 225A4616A803A62202CA8191DCFE613BB8C70CC84591AF3271003716770E9A1A 1 22DAE538B68DD1FA88F79BDA08796E5674901D11F9BB187321C95029B9C32C83 1 0FA7E5B2A5AD251AFD24EBD7BE499C102C9ED488E44523FA6E766A3DB6FFB8CE 1 198ED4242B27EA4695A740C0D5DC808C86600A11AE747A4E4DBB2D4F83503AD5 1 1EBEF8A16DF96A7539AAD26BAD9B1FAEAF09ED140BBF0DEEB19592E1B3EDAE81"
          }
        },
        "tailsHash": "34NrwsR9LbHRHoarwVMmTP95Zr7KJhzqqFzCpFRUw1K4",
        "tailsLocation": "https://host.docker.internal:6543/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e"
      }
    },
    "metadata": {
      "digest": "e9a4d555b94282c9dddf5c9db035552eda5b7dedc9ee7ac654bed89e7631494e",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "5af313ec5288d6c19a24b359905b312a3f4ec57d376003b82b3447dcd76a44c0",
      "reqId": 1765199023589193080
    },
    "protocolVersion": 2,
    "type": "113"
  },
  "txnMetadata": {
    "seqNo": 10,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
    "txnTime": 1765199026
  },
  "ver": "1"
}

============================================================
Seq No: 11 | Time: 1765199029
Transaction Type: REVOC_REG_ENTRY
============================================================


============================================================
  Ledger Transaction (seqNo 11)
============================================================
{
  "auditPath": [
    "9t9iMH3XC8Km3ezBMpRgXb4MkCEmPzuuNZRpeCygJrsL",
    "5FKBYgm8sbJ8ppfyjrSSpcLeEzZLKDMCWNGuVxp19Ryu",
    "5YBFLtPW8BxW8JhBXAct4wLUhP53zefZY3svqsoq12K2",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 13,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "4aq5Nuq1o2GTScBfkxybeUvRCrELJmPu579An
        mBQmY2enkPW6h3F7KD8qD7YL2mupz8jnLa6WBjgicG4m14kWz5C"
      }
    ]
  },
  "rootHash": "41ARQa41wPiJpbwoFr7w6BB3Y66iLWHbmuz84tBEFcnC",
  "txn": {
    "data": {
      "revocDefType": "CL_ACCUM",
      "revocRegDefId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
      "value": {
        "accum": "1 081A884D0F31C44CED8D593690160326DDA4D4A302A40D1D6D9A2A1989C2AD04 1 1ADE5EFDE35825811BD2847263F6E8F5496CFBD3BB0CC22D6AAA6E7570C0705E 1 05FCCFA509A7F4B691FE817A590C6CAB35B6AF6E4466ED8D9F646F74015B3A73 1 09D63D90931D8601856540BD56CB6A1C39948A1DB96D313522FB3B039399A3DB 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000"
      }
    },
    "metadata": {
      "digest": "637759060bc8e849ae15df3595999bb4c6948c76de62394bb46528a78a3040de",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "a6cd676bcc893d0ea4ff87c43cbdad8c0bc7dba54577edb643b4e259c06a3992",
      "reqId": 1765199026544536436
    },
    "protocolVersion": 2,
    "type": "114"
  },
  "txnMetadata": {
    "seqNo": 11,
    "txnId": "5:9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
    "txnTime": 1765199029
  },
  "ver": "1"
}

============================================================
Seq No: 12 | Time: 1765199032
Transaction Type: REVOC_REG_DEF
============================================================


============================================================
  Ledger Transaction (seqNo 12)
============================================================
{
  "auditPath": [
    "8YK24RNxLiJUtE7tScUPkuPkZc8UPTyGx7YXV9b2NST2",
    "5FKBYgm8sbJ8ppfyjrSSpcLeEzZLKDMCWNGuVxp19Ryu",
    "5YBFLtPW8BxW8JhBXAct4wLUhP53zefZY3svqsoq12K2",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 13,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "2eZbXbYtMHW9k58hVgY2xqzsHbYaiGKMTUssNet6Qj
        Xjk6fYJhmAa2g6sW8aKxSpttmd3PZbjELJxsKZ9J5zqv4N"
      }
    ]
  },
  "rootHash": "41ARQa41wPiJpbwoFr7w6BB3Y66iLWHbmuz84tBEFcnC",
  "txn": {
    "data": {
      "credDefId": "9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default",
      "id": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a",
      "revocDefType": "CL_ACCUM",
      "tag": "746cf728-5192-4266-9903-2ffb4b52cc5a",
      "value": {
        "issuanceType": "ISSUANCE_BY_DEFAULT",
        "maxCredNum": 100,
        "publicKeys": {
          "accumKey": {
            "z": "1 073B5A8EC032BBB14E5446AB6212B6A57A47599E9550C70D221F61A7BA624785 1 22F3EAE2B892D7F59499DF026AFFD7C993AA6F210F74384330DA134D40CB7FA4 1 240218E4070D79AE393B2F99D560A6199DCECB58BB8FDB510A7591F3223B4F82 1 0DA7287AC0CAF306D3D3E2AEA87FBBDCB654B7FC1FB4AA3E2FDD679001E5B688 1 1D305894AC1175B74BF60774DF6B0D0D3214B68075F565FA62992434A7E7CA5A 1 0DBE7E3A553FBDF4CAB921611B04E7B0BEBC53C7F615679FA7992FF89F4E69BF 1 0920A371E24419599FC419BA7198373719D5AFAA89587BCE0113B707E808209C 1 22B0BE575646184858C0C0953643F3A222B76C3E556BD26CD4326DC6D3061F85 1 1F9C5406CCB61F1CF467DA7D8E9071AD5B8761103BFA03058E6955E71A2811BE 1 16736793784BC46DB164979D57393EB5DA106CF5830FA3A09AE2E4E0D7B655E2 1 0D8AE326C8981107394445B49F2E5B22FB758E74D7DCE5DDCA56A116DFAF8C99 1 10A48FDC682654E07668738E428E516034EA65A47A398396FD4E2262BB79FCC8"
          }
        },
        "tailsHash": "47xo7WNa2sbxHfnUkenzLgT5LotNci9Z1M5Dsp9vgnAN",
        "tailsLocation": "https://host.docker.internal:6543/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a"
      }
    },
    "metadata": {
      "digest": "fdbcb815ac6ca4c494e5e1f570cf25ea4b50635275b4a84b07c3491964f7025e",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "9ca29e738d1adf4584d3595f1e8b0af0baa655158b1cbc8997c2fec3c1e2c8d2",
      "reqId": 1765199029583049028
    },
    "protocolVersion": 2,
    "type": "113"
  },
  "txnMetadata": {
    "seqNo": 12,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a",
    "txnTime": 1765199032
  },
  "ver": "1"
}

============================================================
Seq No: 13 | Time: 1765199035
Transaction Type: REVOC_REG_ENTRY
============================================================


============================================================
  Ledger Transaction (seqNo 13)
============================================================
{
  "auditPath": [
    "8vLEbJ5pzVFmhXYiddtoRA8iz7KA8KCANaNKP2Xy2K7q",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 13,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "2Sx3jijAxTEjgquGZScqZVqJGmmqe8mZnSm4oXsEsMp
        pqj8G7wJoZ2X4zjXMn6qjYKkmxF5SysxhRRCPei6GvXjm"
      }
    ]
  },
  "rootHash": "41ARQa41wPiJpbwoFr7w6BB3Y66iLWHbmuz84tBEFcnC",
  "txn": {
    "data": {
      "revocDefType": "CL_ACCUM",
      "revocRegDefId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a",
      "value": {
        "accum": "1 1A824FF22739016F2E9B29B163A27ED474537890ADE3507951A90A8EE8D840E0 1 1A7231FF23B65C7C7A95F987730650ABDCA90F32B53E66C0D5F16A38E8C3A8D9 1 106DA5962E3A41237F7B0EB85C0406CD09D8394ECD035013970E21D26F070F81 1 0D87A81E52FEF500E88ABEFBD82C070981DECB3D1FE1F32CADFED36613090132 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000"
      }
    },
    "metadata": {
      "digest": "0053474469f6e7eead39784d154c1f205fba87c155685130a9903a4c21b3e8c9",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "cd4b7b74dae328ce4aa0330b8601a72befc1798a4b1a378b21981718d007ba87",
      "reqId": 1765199032565535792
    },
    "protocolVersion": 2,
    "type": "114"
  },
  "txnMetadata": {
    "seqNo": 13,
    "txnId": "5:9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a",
    "txnTime": 1765199035
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\subsubsection{Teil 4: Agent Connections - KRITIS-Akteure}
\label{sec:Teil4-Agent-Connections-KRITIS-Akteure}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-8}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 8}, numbers=left, frame=single]
# Cell 8: Issuer Discovery - Identifikation via Ledger Identifier

print("Issuer Discovery - Identifikation vertrauenswürdiger Aussteller")
print("="*80)
print("KRITIS-Anforderung: Identity Holder müssen vertrauenswürdige Issuers")
print("   für gewünschte Credentials entdecken und deren Identität validieren.\n")

# Phase 1: Ledger-basierte Issuer Discovery
print("Phase 1: Ledger-basierte Issuer Discovery\n")

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")
    
    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])
        
        # Filter: Nur NYM-Transaktionen (DID-Registrierungen, Type '1')
        nym_txns = [
            txn for txn in ledger_txns 
            if txn.get('txn', {}).get('type') == '1'
        ]
        
        # Filter: Nur Transaktionen mit TRUST_ANCHOR Role (= Issuers)
        issuer_identities = []
        for txn in nym_txns:
            txn_data = txn.get('txn', {}).get('data', {})
            role = txn_data.get('role')
            
            # Role '101' = TRUST_ANCHOR (berechtigt, Credentials auszustellen)
            if role == '101':
                full_verkey = txn_data.get('verkey', 'N/A')
                issuer_identities.append({
                    'did': txn_data.get('dest'),
                    'verkey_full': full_verkey,
                    'verkey_short': full_verkey[:30] + '...' if len(full_verkey) > 30 else full_verkey,
                    'alias': txn_data.get('alias', 'N/A'),
                    'registered_by': txn.get('txn', {}).get('metadata', {}).get('from', 'N/A'),
                    'txn_time': txn.get('txnMetadata', {}).get('txnTime'),
                    'seq_no': txn.get('txnMetadata', {}).get('seqNo')
                })
        
        print(f"{len(issuer_identities)} TRUST_ANCHOR Issuer(s) auf dem Ledger gefunden:\n")
        
        if issuer_identities:
            # Zeige nur relevante Spalten für Identifikation
            df_issuers = pd.DataFrame([{
                'Alias': i['alias'],
                'DID': i['did'],
                'Verkey (gekürzt)': i['verkey_short'],
                'Seq No': i['seq_no']
            } for i in issuer_identities])
            print(df_issuers.to_json(orient='records', indent=2))
        else:
            print("Keine TRUST_ANCHOR gefunden (Ledger noch leer?)")
            
except Exception as e:
    print(f"Fehler bei Issuer Discovery: {e}")
    import traceback
    traceback.print_exc()

# Phase 2: Issuer Identifier Verification
print(f"\n{'='*80}")
print("Phase 2: Issuer Identifier Verification\n")

def verify_issuer_identity(issuer_did, issuer_data, all_txns):
    """
    Verifiziert die Identität eines Issuers anhand von Ledger-Daten.
    
    Identitäts-Eigenschaften:
    1. DID (Decentralized Identifier)
    2. Verkey (Public Key für Signaturverifikation)
    3. TRUST_ANCHOR Role
    4. Registrierungs-Endorser (Wer hat die Identität bestätigt?)
    5. On-Ledger Aktivitäten (Schemas, CRED_DEFs, etc.)
    """
    identity_info = {
        'verified': True,
        'properties': []
    }
    
    # 1. DID Identifier
    identity_info['properties'].append({
        'property': 'DID (Identifier)',
        'value': issuer_did,
        'verified': True
    })
    
    # 2. Verkey (Public Key)
    verkey = issuer_data.get('verkey_full', 'N/A')
    identity_info['properties'].append({
        'property': 'Verkey (Public Key)',
        'value': verkey,
        'verified': verkey != 'N/A'
    })
    
    # 3. TRUST_ANCHOR Role
    identity_info['properties'].append({
        'property': 'TRUST_ANCHOR Role',
        'value': 'Ja (Role 101)',
        'verified': True
    })
    
    # 4. Endorser (Wer hat registriert?)
    registered_by = issuer_data.get('registered_by', 'N/A')
    identity_info['properties'].append({
        'property': 'Registriert von',
        'value': registered_by,
        'verified': registered_by != 'N/A'
    })
    
    # 5. On-Ledger Aktivitäten
    activities = []
    
    # Schemas (Type '101' im Ledger)
    schemas = [
        txn for txn in all_txns 
        if txn.get('txn', {}).get('type') == '101'
        and txn.get('txn', {}).get('metadata', {}).get('from') == issuer_did
    ]
    if schemas:
        activities.append(f"{len(schemas)} Schema(s)")
    
    # Credential Definitions (Type '102')
    cred_defs = [
        txn for txn in all_txns 
        if txn.get('txn', {}).get('type') == '102'
        and txn.get('txn', {}).get('metadata', {}).get('from') == issuer_did
    ]
    if cred_defs:
        activities.append(f"{len(cred_defs)} CRED_DEF(s)")
    
    # Revocation Registries (Type '113' oder '114')
    revoc_regs = [
        txn for txn in all_txns 
        if txn.get('txn', {}).get('type') in ['113', '114']
        and txn.get('txn', {}).get('metadata', {}).get('from') == issuer_did
    ]
    if revoc_regs:
        activities.append(f"{len(revoc_regs)} REVOC_REG(s)")
    
    activity_summary = ', '.join(activities) if activities else 'Keine'
    identity_info['properties'].append({
        'property': 'On-Ledger Aktivitäten',
        'value': activity_summary,
        'verified': len(activities) > 0
    })
    
    # 6. Zeitstempel (Registrierungszeitpunkt)
    import datetime
    txn_time = issuer_data.get('txn_time')
    if txn_time:
        registration_date = datetime.datetime.fromtimestamp(txn_time)
        age_days = (datetime.datetime.now() - registration_date).days
        identity_info['properties'].append({
            'property': 'Registriert seit',
            'value': f"{registration_date.strftime('%Y-%m-%d')} ({age_days} Tage)",
            'verified': True
        })
    
    return identity_info

# Zeige Identifier-Details für alle gefundenen Issuers
if 'issuer_identities' in locals() and issuer_identities:
    for issuer in issuer_identities[:3]:  # Zeige nur erste 3 Issuers
        print(f"{'='*80}")
        print(f"Issuer: {issuer['alias']}")
        print(f"{'='*80}")
        
        identity = verify_issuer_identity(issuer['did'], issuer, ledger_txns)
        
        for prop in identity['properties']:
            status = "verified" if prop['verified'] else "not verified"
            value = prop['value']
            # Kürze lange Werte (z.B. Verkey)
            if len(str(value)) > 50:
                value = str(value)[:47] + "..."
            print(f"   {status} {prop['property']:25s} {value}")
        
        print()

# Phase 3: Issuer-Suche für spezifisches Schema
print(f"{'='*80}")
print("Phase 3: Issuer-Identifikation für Schema\n")

# Suche nach dem Schema aus Cell 11
desired_schema_name = "kritis_emergency_maintenance_cert"

try:
    # Hole alle Schemas vom Ledger (Type '101')
    schemas = [
        txn for txn in ledger_txns 
        if txn.get('txn', {}).get('type') == '101'
    ]
    
    # Filter nach Schema-Namen
    matching_schemas = []
    for schema_txn in schemas:
        schema_data = schema_txn.get('txn', {}).get('data', {}).get('data', {})
        schema_name = schema_data.get('name', '')
        
        if desired_schema_name.lower() in schema_name.lower():
            schema_version = schema_data.get('version', 'N/A')
            created_by = schema_txn.get('txn', {}).get('metadata', {}).get('from', 'N/A')
            
            matching_schemas.append({
                'schema_name': schema_name,
                'schema_version': schema_version,
                'schema_id': f"{created_by}:2:{schema_name}:{schema_version}",
                'created_by_did': created_by
            })
    
    if matching_schemas:
        print(f"{len(matching_schemas)} Schema(s) gefunden für '{desired_schema_name}':\n")
        df_schemas = pd.DataFrame(matching_schemas)
        print(df_schemas.to_json(orient='records', indent=2))
        
        # Identifiziere die Issuers dieser Schemas
        print(f"\n{'='*80}")
        print("Issuer-Identifikation für gefundene Schemas:\n")
        
        for schema in matching_schemas:
            schema_issuer_did = schema['created_by_did']
            
            # Finde Issuer-Identität
            issuer_info = next(
                (i for i in issuer_identities if i['did'] == schema_issuer_did), 
                None
            )
            
            print(f"Schema: {schema['schema_name']} v{schema['schema_version']}")
            print(f"{'-'*80}")
            
            if issuer_info:
                # Zeige Issuer-Identifikation
                print(f"Issuer identifiziert:")
                print(f"   Alias:         {issuer_info['alias']}")
                print(f"   DID:           {issuer_info['did']}")
                print(f"   Verkey:        {issuer_info['verkey_short']}")
                print(f"   TRUST_ANCHOR:  Ja (Role 101)")
                print(f"   Ledger Seq No: {issuer_info['seq_no']}")
                
                # Zeige vollständige Identitäts-Verifikation
                print(f"\nVollständige Identitäts-Verifikation:")
                identity = verify_issuer_identity(schema_issuer_did, issuer_info, ledger_txns)
                for prop in identity['properties']:
                    status = "verified" if prop['verified'] else "not verified"
                    value = prop['value']
                    if len(str(value)) > 40:
                        value = str(value)[:37] + "..."
                    print(f"   {status} {prop['property']:20s} {value}")
                
                print(f"\nIssuer erfolgreich über DID identifiziert und verifiziert")
            else:
                print(f"Issuer NICHT als TRUST_ANCHOR registriert")
                print(f"   DID: {schema_issuer_did}")
                print(f"   Status: Keine TRUST_ANCHOR Role auf dem Ledger")
            
            print(f"\n{'='*80}\n")
    else:
        print(f"Keine Schemas gefunden für '{desired_schema_name}'")
        print(f"Tipp: Stelle sicher, dass Cell 11 (Schema erstellen) ausgeführt wurde")
        
except Exception as e:
    print(f"Fehler bei Schema-Suche: {e}")
    import traceback
    traceback.print_exc()

print(f"\n{'='*80}")
print("Issuer Identifier-basierte Discovery:")
print("   1. DID (Decentralized Identifier) - Eindeutiger Identifier")
print("   2. Verkey (Public Key) - Kryptografische Identität")
print("   3. TRUST_ANCHOR Role - Berechtigung zum Ausstellen")
print("   4. Ledger Endorsement - Wer hat die Identität bestätigt?")
print("   5. On-Ledger Aktivitäten - Schemas, CRED_DEFs, REVOC_REGs")
print(f"{'='*80}")
print("Identifikations-Methode:")
print("   Der Issuer wird über seinen DID (Decentralized Identifier) eindeutig")
print("   identifiziert. Der DID ist im Schema-Identifier eingebettet:")
print("   Format: <issuer_did>:2:<schema_name>:<version>")
print("   ")
print("   Die Vertrauenswürdigkeit wird durch die TRUST_ANCHOR Role auf dem")
print("   Hyperledger Indy Ledger garantiert (Byzantine Fault Tolerant).")
print(f"{'='*80}")
\end{lstlisting}

\pagebreak

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-8-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 8 Output}, numbers=left, frame=single]
Issuer Discovery - Identifikation vertrauenswürdiger Aussteller
================================================================================
KRITIS-Anforderung: Identity Holder müssen vertrauenswürdige Issuers
   für gewünschte Credentials entdecken und deren Identität validieren.

Phase 1: Ledger-basierte Issuer Discovery

1 TRUST_ANCHOR Issuer(s) auf dem Ledger gefunden:

[
  {
    "Alias":"Energienetzbetreiber (Issuer)",
    "DID":"9pbXiFBZZGwXKp61HQBz3J",
    "Verkey (gek\u00fcrzt)":"2zoa6G7aMfX8GnUEpDxxunFHE7fZkt...",
    "Seq No":6
  }
]

================================================================================
Phase 2: Issuer Identifier Verification

================================================================================
Issuer: Energienetzbetreiber (Issuer)
================================================================================
   DID (Identifier)          9pbXiFBZZGwXKp61HQBz3J
   Verkey (Public Key)       2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm
   TRUST_ANCHOR Role         Ja (Role 101)
   Registriert von           V4SGRU86Z58d6TV7PBUe6f
   On-Ledger Aktivitäten     1 Schema(s), 1 CRED_DEF(s), 4 REVOC_REG(s)
   Registriert seit          2025-12-08 (0 Tage)

================================================================================
Phase 3: Issuer-Identifikation für Schema

1 Schema(s) gefunden für 'kritis_emergency_maintenance_cert':

[
  {
    "schema_name":"kritis_emergency_maintenance_cert",
    "schema_version":"1.1",
    "schema_id":"9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1",
    "created_by_did":"9pbXiFBZZGwXKp61HQBz3J"
  }
]

================================================================================
Issuer-Identifikation für gefundene Schemas:

Schema: kritis_emergency_maintenance_cert v1.1
--------------------------------------------------------------------------------
Issuer identifiziert:
   Alias:         Energienetzbetreiber (Issuer)
   DID:           9pbXiFBZZGwXKp61HQBz3J
   Verkey:        2zoa6G7aMfX8GnUEpDxxunFHE7fZkt...
   TRUST_ANCHOR:  Ja (Role 101)
   Ledger Seq No: 6

   Vollständige Identitäts-Verifikation:
   DID (Identifier)     9pbXiFBZZGwXKp61HQBz3J
   Verkey (Public Key)  2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1v...
   TRUST_ANCHOR Role    Ja (Role 101)
   Registriert von      V4SGRU86Z58d6TV7PBUe6f
   On-Ledger Aktivitäten 1 Schema(s), 1 CRED_DEF(s), 4 REVOC_R...
   Registriert seit     2025-12-08 (0 Tage)

   Issuer erfolgreich über DID identifiziert und verifiziert

================================================================================


================================================================================
Issuer Identifier-basierte Discovery:
   1. DID (Decentralized Identifier) - Eindeutiger Identifier
   2. Verkey (Public Key) - Kryptografische Identität
   3. TRUST_ANCHOR Role - Berechtigung zum Ausstellen
   4. Ledger Endorsement - Wer hat die Identität bestätigt?
   5. On-Ledger Aktivitäten - Schemas, CRED_DEFs, REVOC_REGs
================================================================================
Identifikations-Methode:
   Der Issuer wird über seinen DID (Decentralized Identifier) eindeutig
   identifiziert. Der DID ist im Schema-Identifier eingebettet:
   Format: <issuer_did>:2:<schema_name>:<version>
   
   Die Vertrauenswürdigkeit wird durch die TRUST_ANCHOR Role auf dem
   Hyperledger Indy Ledger garantiert (Byzantine Fault Tolerant).
================================================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-9}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 9}, numbers=left, frame=single]
# Cell 9: Connection Issuer --> Holder erstellen

print("Connection: Issuer --> Holder erstellen...\n")

start_time = time.time()

# ========================================
# 1. PRE-CHECK: Existierende Connections prüfen
# ========================================
print("Pre-Check: Prüfe existierende Connections...\n")

existing_conn_issuer_holder = None
existing_conn_holder = None

# Prüfe Issuer-Seite: GET /connections
issuer_conns_response = api_get(ISSUER_ADMIN_URL, "/connections")
if issuer_conns_response and "results" in issuer_conns_response:
    print(f"   Issuer: {len(issuer_conns_response['results'])} Connection(s) gefunden")
    for conn in issuer_conns_response["results"]:
        if conn.get("state") == "completed":
            existing_conn_issuer_holder = conn["connection_id"]
            print(f"Connection {existing_conn_issuer_holder} (State: completed)")
            break
else:
    print("   Issuer: Keine Connections gefunden")

# Prüfe Holder-Seite: GET /connections
holder_conns_response = api_get(HOLDER_ADMIN_URL, "/connections")
if holder_conns_response and "results" in holder_conns_response:
    print(f"   Holder: {len(holder_conns_response['results'])} Connection(s) gefunden")
    for conn in holder_conns_response["results"]:
        if conn.get("state") == "completed":
            existing_conn_holder = conn["connection_id"]
            print(f"Connection {existing_conn_holder} (State: completed)")
            break
else:
    print("   Holder: Keine Connections gefunden")

if existing_conn_issuer_holder and existing_conn_holder:
    print(f"\nExistierende Connection gefunden!")
    print(f"   Issuer Connection ID: {existing_conn_issuer_holder}")
    print(f"   Holder Connection ID: {existing_conn_holder}")
    print("   Überspringe Connection-Erstellung\n")
    conn_id_issuer_holder = existing_conn_issuer_holder
    conn_id_holder = existing_conn_holder
else:
    print("\n   Keine vollständige existierende Connection gefunden")
    print("   Erstelle neue Connection mit did:peer:4...\n")
    
    # ========================================
    # 2. HAUPT-WORKFLOW: Connection mit did:peer erstellen
    # ========================================
    
    # Initialisiere Variablen
    conn_id_issuer_holder = None
    conn_id_holder = None
    invitation_msg_id = None
    invitation_key = None
    
    # Issuer erstellt Out-of-Band Invitation mit did:peer:4
    invitation_data = {
        "handshake_protocols": ["https://didcomm.org/didexchange/1.1"],
        "use_did_method": "did:peer:4",
        "my_label": "Issuer Agent",
        "goal": "Establish connection for credential issuance",
        "goal_code": "issue-vc",
        "accept": [
            "didcomm/aip1",
            "didcomm/aip2;env=rfc19"
        ]
    }
    
    print("Invitation-Modus: did:peer:4 (P2P Connection)")
    
    # API Call mit Query-Parametern für auto-accept
    invitation_response = api_post(
        ISSUER_ADMIN_URL,
        "/out-of-band/create-invitation?auto_accept=true&multi_use=false",
        invitation_data
    )
    
    if invitation_response and "invitation" in invitation_response:
        invitation = invitation_response["invitation"]
        oob_id = invitation_response.get("oob_id")
        invitation_url = invitation_response.get("invitation_url", "")
        state = invitation_response.get("state")
        
        # WICHTIG: Speichere invitation_msg_id aus der Invitation
        invitation_msg_id = invitation.get("@id")
        
        # Optional: Extrahiere invitation_key aus services (falls vorhanden)
        services = invitation.get("services", [])
        if services and len(services) > 0:
            # Bei did:peer:4 ist der Service oft inline
            service = services[0]
            if isinstance(service, dict):
                recipient_keys = service.get("recipientKeys", [])
                if recipient_keys:
                    invitation_key = recipient_keys[0]
        
        print(f"Issuer: Invitation erstellt")
        print(f"   OOB ID: {oob_id}")
        print(f"   State: {state}")
        print(f"   Invitation Msg ID: {invitation_msg_id}")
        if invitation_key:
            print(f"   Invitation Key: {invitation_key[:30]}...")
        print(f"   Services: {len(invitation.get('services', []))} service(s)")
        print(f"   Label: {invitation_data['my_label']}")
        print(f"   Goal: {invitation_data['goal']}")
        
        # Holder akzeptiert Invitation (mit auto-accept)
        receive_response = api_post(
            HOLDER_ADMIN_URL,
            "/out-of-band/receive-invitation?auto_accept=true",
            invitation
        )
        
        if receive_response is not None:
            conn_id_holder = receive_response.get("connection_id")
            print(f"\nHolder: Invitation akzeptiert")
            print(f"   Connection ID (Holder): {conn_id_holder}")
            
            # Issuer findet seine Connection via invitation_msg_id
            print(f"\n   Issuer sucht Connection via invitation_msg_id...")
            time.sleep(2)  # Kurze Wartezeit für DIDExchange Protocol
            
            # Hole alle Issuer Connections
            issuer_conns = api_get(ISSUER_ADMIN_URL, "/connections")
            if issuer_conns and "results" in issuer_conns:
                connections = issuer_conns["results"]
                
                print(f"     Prüfe {len(connections)} Connection(s)...")
                
                # Finde Connection anhand invitation_msg_id (exaktes Match!)
                connection_found = False
                for conn in connections:
                    conn_invitation_msg_id = conn.get("invitation_msg_id")
                    conn_invitation_key = conn.get("invitation_key")
                    
                    # Match via invitation_msg_id (bevorzugt)
                    if invitation_msg_id and conn_invitation_msg_id == invitation_msg_id:
                        conn_id_issuer_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\nIssuer: Connection gefunden via invitation_msg_id!")
                        print(f"Connection ID: {conn_id_issuer_holder}")
                        print(f"State: {conn_state}")
                        print(f"Invitation Msg ID: {conn_invitation_msg_id}")
                        print(f"Their Label: {conn.get('their_label', 'N/A')}")
                        print(f"Updated At: {conn.get('updated_at', 'N/A')}")
                        connection_found = True
                        break
                    
                    # Fallback: Match via invitation_key
                    elif invitation_key and conn_invitation_key == invitation_key:
                        conn_id_issuer_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\nIssuer: Connection gefunden via invitation_key!")
                        print(f"Connection ID: {conn_id_issuer_holder}")
                        print(f"State: {conn_state}")
                        print(f"Invitation Key: {conn_invitation_key[:30]}...")
                        print(f"Their Label: {conn.get('their_label', 'N/A')}")
                        connection_found = True
                        break
                
                if not connection_found:
                    print(f"Keine Connection mit matching invitation_msg_id gefunden")
                    print(f"Gesuchte invitation_msg_id: {invitation_msg_id}")
            else:
                print(f"Konnte Issuer Connections nicht abrufen")
        else:
            print("Holder konnte Invitation nicht akzeptieren")
    else:
        print("Fehler beim Erstellen der Invitation")

# ========================================
# 3. POST-VALIDATION: Connection-Status verifizieren
# ========================================
print("\nPost-Validation: Verifiziere Connection-Status...\n")

validation_success = False
issuer_state = None
holder_state = None

if conn_id_issuer_holder:
    # Prüfe Issuer-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    issuer_conn_detail = api_get(
        ISSUER_ADMIN_URL,
        f"/connections/{conn_id_issuer_holder}"
    )
    
    if issuer_conn_detail:
        issuer_state = issuer_conn_detail.get("state")
        issuer_their_label = issuer_conn_detail.get("their_label")
        issuer_their_role = issuer_conn_detail.get("their_role")
        issuer_invitation_msg_id = issuer_conn_detail.get("invitation_msg_id")
        
        print(f"Issuer Connection {conn_id_issuer_holder}:")
        print(f"State: {issuer_state}")
        print(f"Their Label: {issuer_their_label}")
        print(f"Their Role: {issuer_their_role}")
        print(f"Invitation Msg ID: {issuer_invitation_msg_id}")

# Prüfe Holder-Seite (falls conn_id_holder bekannt)
if conn_id_holder:
    # Prüfe Holder-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    holder_conn_detail = api_get(
        HOLDER_ADMIN_URL,
        f"/connections/{conn_id_holder}"
    )
    
    if holder_conn_detail:
        holder_state = holder_conn_detail.get("state")
        holder_their_label = holder_conn_detail.get("their_label")
        holder_their_role = holder_conn_detail.get("their_role")
        holder_invitation_msg_id = holder_conn_detail.get("invitation_msg_id")
        
        print(f"\nHolder Connection {conn_id_holder}:")
        print(f"State: {holder_state}")
        print(f"Their Label: {holder_their_label}")
        print(f"Their Role: {holder_their_role}")
        print(f"Invitation Msg ID: {holder_invitation_msg_id}")
        
        # Verifiziere dass invitation_msg_id übereinstimmt
        if invitation_msg_id and issuer_invitation_msg_id == holder_invitation_msg_id:
            print(f"Invitation Msg IDs stimmen überein!")

# Validierung
if issuer_state == "active" and holder_state == "active":
    validation_success = True
elif issuer_state == "active" and not conn_id_holder:
    # Bei existierender Connection nur Issuer-Seite bekannt
    validation_success = True

# Finale Ausgabe
print(f"\n{'='*60}")
if validation_success:
    print(f"CONNECTION ESTABLISHED (did:peer:4): Issuer <--> Holder")
    print(f"   Issuer Connection ID: {conn_id_issuer_holder}")
    if conn_id_holder:
        print(f"   Holder Connection ID: {conn_id_holder}")
    print(f"   Issuer State: {issuer_state}")
    if holder_state:
        print(f"   Holder State: {holder_state}")
    if invitation_msg_id:
        print(f"   Invitation Msg ID: {invitation_msg_id}")
else:
    print(f"CONNECTION STATUS UNCLEAR")
    print(f"   Issuer State: {issuer_state if issuer_state else 'N/A'}")
    print(f"   Holder State: {holder_state if holder_state else 'N/A'}")
    if conn_id_issuer_holder:
        print(f"   Issuer Connection ID: {conn_id_issuer_holder}")
    if conn_id_holder:
        print(f"   Holder Connection ID: {conn_id_holder}")

print(f"{'='*60}")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-9-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 9 Output}, numbers=left, frame=single]
Connection: Issuer --> Holder erstellen...

Pre-Check: Prüfe existierende Connections...

   Issuer: 0 Connection(s) gefunden
   Holder: 0 Connection(s) gefunden

   Keine vollständige existierende Connection gefunden
   Erstelle neue Connection mit did:peer:4...

Invitation-Modus: did:peer:4 (P2P Connection)
Issuer: Invitation erstellt
   OOB ID: 3b5fe1c5-a9b0-46d1-8cd9-0ead4720f4aa
   State: initial
   Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094
   Services: 1 service(s)
   Label: Issuer Agent
   Goal: Establish connection for credential issuance

Holder: Invitation akzeptiert
   Connection ID (Holder): 860434c0-7632-41d9-90ea-addaafa68a33

   Issuer sucht Connection via invitation_msg_id...
     Prüfe 1 Connection(s)...

   Issuer: Connection gefunden via invitation_msg_id!
      Connection ID: c5399b17-85f2-4adf-99a5-5aeeee8647d9
      State: active
      Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094
      Their Label: Holder Agent
      Updated At: 2025-12-08T13:27:30.289055Z

Post-Validation: Verifiziere Connection-Status...

   Issuer Connection c5399b17-85f2-4adf-99a5-5aeeee8647d9:
      State: active
      Their Label: Holder Agent
      Their Role: invitee
      Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094

   Holder Connection 860434c0-7632-41d9-90ea-addaafa68a33:
      State: active
      Their Label: Issuer Agent
      Their Role: inviter
      Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094
      Invitation Msg IDs stimmen überein!

============================================================
CONNECTION ESTABLISHED (did:peer:4): Issuer <--> Holder
   Issuer Connection ID: c5399b17-85f2-4adf-99a5-5aeeee8647d9
   Holder Connection ID: 860434c0-7632-41d9-90ea-addaafa68a33
   Issuer State: active
   Holder State: active
   Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094
============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-10}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 10}, numbers=left, frame=single]
# Cell 10: Connection Holder --> Verifier erstellen

print("Connection: Holder --> Verifier erstellen...\n")

start_time = time.time()

# ========================================
# 1. PRE-CHECK: Existierende Connections prüfen
# ========================================
print("Pre-Check: Prüfe existierende Connections...\n")

existing_conn_verifier_holder = None
existing_conn_holder_verifier = None

# Prüfe Verifier-Seite: GET /connections
verifier_conns_response = api_get(VERIFIER_ADMIN_URL, "/connections")
if verifier_conns_response and "results" in verifier_conns_response:
    print(f"   Verifier: {len(verifier_conns_response['results'])} Connection(s) gefunden")
    for conn in verifier_conns_response["results"]:
        if conn.get("state") == "completed":
            existing_conn_verifier_holder = conn["connection_id"]
            print(f"     --> Connection {existing_conn_verifier_holder} (State: completed)")
            break
else:
    print("   Verifier: Keine Connections gefunden")

# Prüfe Holder-Seite: GET /connections (für Holder --> Verifier)
holder_conns_response = api_get(HOLDER_ADMIN_URL, "/connections")
if holder_conns_response and "results" in holder_conns_response:
    print(f"   Holder: {len(holder_conns_response['results'])} Connection(s) gefunden")
    # Finde Connection zum Verifier (basierend auf Label oder neueste)
    holder_verifier_conns = [c for c in holder_conns_response["results"] if c.get("their_label") == "Verifier Agent"]
    if holder_verifier_conns:
        for conn in holder_verifier_conns:
            if conn.get("state") == "completed":
                existing_conn_holder_verifier = conn["connection_id"]
                print(f"     --> Connection {existing_conn_holder_verifier} (State: completed, Partner: Verifier)")
                break
else:
    print("   Holder: Keine Connections gefunden")

if existing_conn_verifier_holder and existing_conn_holder_verifier:
    print(f"\nExistierende Connection gefunden!")
    print(f"   Verifier Connection ID: {existing_conn_verifier_holder}")
    print(f"   Holder Connection ID: {existing_conn_holder_verifier}")
    print("   Überspringe Connection-Erstellung\n")
    conn_id_verifier_holder = existing_conn_verifier_holder
    conn_id_holder_verifier = existing_conn_holder_verifier
else:
    print("\n   Keine vollständige existierende Connection gefunden")
    print("   Erstelle neue Connection mit did:peer:4...\n")
    
    # ========================================
    # 2. HAUPT-WORKFLOW: Connection mit did:peer erstellen
    # ========================================
    
    # Initialisiere Variablen
    conn_id_verifier_holder = None
    conn_id_holder_verifier = None
    invitation_msg_id = None
    invitation_key = None
    
    # Verifier erstellt Out-of-Band Invitation mit did:peer:4
    invitation_data = {
        "handshake_protocols": ["https://didcomm.org/didexchange/1.1"],
        "use_did_method": "did:peer:4",
        "my_label": "Verifier Agent",
        "goal": "Establish connection for credential verification",
        "goal_code": "verify-vc",
        "accept": [
            "didcomm/aip1",
            "didcomm/aip2;env=rfc19"
        ]
    }
    
    print("Invitation-Modus: did:peer:4 (P2P Connection)")
    
    # API Call mit Query-Parametern für auto-accept
    invitation_response = api_post(
        VERIFIER_ADMIN_URL,
        "/out-of-band/create-invitation?auto_accept=true&multi_use=false",
        invitation_data
    )
    
    if invitation_response and "invitation" in invitation_response:
        invitation = invitation_response["invitation"]
        oob_id = invitation_response.get("oob_id")
        invitation_url = invitation_response.get("invitation_url", "")
        state = invitation_response.get("state")
        
        # WICHTIG: Speichere invitation_msg_id aus der Invitation
        invitation_msg_id = invitation.get("@id")
        
        # Optional: Extrahiere invitation_key aus services (falls vorhanden)
        services = invitation.get("services", [])
        if services and len(services) > 0:
            service = services[0]
            if isinstance(service, dict):
                recipient_keys = service.get("recipientKeys", [])
                if recipient_keys:
                    invitation_key = recipient_keys[0]
        
        print(f"Verifier: Invitation erstellt")
        print(f"   OOB ID: {oob_id}")
        print(f"   State: {state}")
        print(f"   Invitation Msg ID: {invitation_msg_id}")
        if invitation_key:
            print(f"   Invitation Key: {invitation_key}")
        print(f"   Services: {len(invitation.get('services', []))} service(s)")
        print(f"   Label: {invitation_data['my_label']}")
        print(f"   Goal: {invitation_data['goal']}")
        
        # Holder akzeptiert Invitation (mit auto-accept)
        receive_response = api_post(
            HOLDER_ADMIN_URL,
            "/out-of-band/receive-invitation?auto_accept=true",
            invitation
        )
        
        if receive_response is not None:
            conn_id_holder_verifier = receive_response.get("connection_id")
            print(f"\nHolder: Invitation akzeptiert")
            print(f"   Connection ID (Holder): {conn_id_holder_verifier}")
            
            # Verifier findet seine Connection via invitation_msg_id
            print(f"\n   Verifier sucht Connection via invitation_msg_id...")
            time.sleep(2)  # Kurze Wartezeit für DIDExchange Protocol
            
            # Hole alle Verifier Connections
            verifier_conns = api_get(VERIFIER_ADMIN_URL, "/connections")
            if verifier_conns and "results" in verifier_conns:
                connections = verifier_conns["results"]
                
                print(f"     Prüfe {len(connections)} Connection(s)...")
                
                # Finde Connection anhand invitation_msg_id (exaktes Match!)
                connection_found = False
                for conn in connections:
                    conn_invitation_msg_id = conn.get("invitation_msg_id")
                    conn_invitation_key = conn.get("invitation_key")
                    
                    # Match via invitation_msg_id (bevorzugt)
                    if invitation_msg_id and conn_invitation_msg_id == invitation_msg_id:
                        conn_id_verifier_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\n   Verifier: Connection gefunden via invitation_msg_id!")
                        print(f"      Connection ID: {conn_id_verifier_holder}")
                        print(f"      State: {conn_state}")
                        print(f"      Invitation Msg ID: {conn_invitation_msg_id}")
                        print(f"      Their Label: {conn.get('their_label', 'N/A')}")
                        print(f"      Updated At: {conn.get('updated_at', 'N/A')}")
                        connection_found = True
                        break
                    
                    # Fallback: Match via invitation_key
                    elif invitation_key and conn_invitation_key == invitation_key:
                        conn_id_verifier_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\n   Verifier: Connection gefunden via invitation_key!")
                        print(f"      Connection ID: {conn_id_verifier_holder}")
                        print(f"      State: {conn_state}")
                        print(f"      Invitation Key: {conn_invitation_key}...")
                        print(f"      Their Label: {conn.get('their_label', 'N/A')}")
                        connection_found = True
                        break
                
                if not connection_found:
                    print(f"     Keine Connection mit matching invitation_msg_id gefunden")
                    print(f"     Gesuchte invitation_msg_id: {invitation_msg_id}")
            else:
                print(f"     Konnte Verifier Connections nicht abrufen")
        else:
            print("Holder konnte Invitation nicht akzeptieren")
    else:
        print("Fehler beim Erstellen der Invitation")

# ========================================
# 3. POST-VALIDATION: Connection-Status verifizieren
# ========================================
print("\nPost-Validation: Verifiziere Connection-Status...\n")

validation_success = False
verifier_state = None
holder_state = None

if conn_id_verifier_holder:
    # Prüfe Verifier-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    verifier_conn_detail = api_get(
        VERIFIER_ADMIN_URL,
        f"/connections/{conn_id_verifier_holder}"
    )
    
    if verifier_conn_detail:
        verifier_state = verifier_conn_detail.get("state")
        verifier_their_label = verifier_conn_detail.get("their_label")
        verifier_their_role = verifier_conn_detail.get("their_role")
        verifier_invitation_msg_id = verifier_conn_detail.get("invitation_msg_id")
        
        print(f"   Verifier Connection {conn_id_verifier_holder}:")
        print(f"      State: {verifier_state}")
        print(f"      Their Label: {verifier_their_label}")
        print(f"      Their Role: {verifier_their_role}")
        print(f"      Invitation Msg ID: {verifier_invitation_msg_id}")

# Prüfe Holder-Seite (falls conn_id_holder_verifier bekannt)
if conn_id_holder_verifier:
    # Prüfe Holder-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    holder_conn_detail = api_get(
        HOLDER_ADMIN_URL,
        f"/connections/{conn_id_holder_verifier}"
    )
    
    if holder_conn_detail:
        holder_state = holder_conn_detail.get("state")
        holder_their_label = holder_conn_detail.get("their_label")
        holder_their_role = holder_conn_detail.get("their_role")
        holder_invitation_msg_id = holder_conn_detail.get("invitation_msg_id")
        
        print(f"\n   Holder Connection {conn_id_holder_verifier}:")
        print(f"      State: {holder_state}")
        print(f"      Their Label: {holder_their_label}")
        print(f"      Their Role: {holder_their_role}")
        print(f"      Invitation Msg ID: {holder_invitation_msg_id}")
        
        # Verifiziere dass invitation_msg_id übereinstimmt
        if invitation_msg_id and verifier_invitation_msg_id == holder_invitation_msg_id:
            print(f"      Invitation Msg IDs stimmen überein!")

# Validierung
if verifier_state == "active" and holder_state == "active":
    validation_success = True
elif verifier_state == "active" and not conn_id_holder_verifier:
    # Bei existierender Connection nur Verifier-Seite bekannt
    validation_success = True

# Finale Ausgabe
print(f"\n{'='*60}")
if validation_success:
    print(f"CONNECTION ESTABLISHED (did:peer:4): Holder <--> Verifier")
    print(f"   Verifier Connection ID: {conn_id_verifier_holder}")
    if conn_id_holder_verifier:
        print(f"   Holder Connection ID: {conn_id_holder_verifier}")
    print(f"   Verifier State: {verifier_state}")
    if holder_state:
        print(f"   Holder State: {holder_state}")
    if invitation_msg_id:
        print(f"   Invitation Msg ID: {invitation_msg_id}")
else:
    print(f"CONNECTION STATUS UNCLEAR")
    print(f"   Verifier State: {verifier_state if verifier_state else 'N/A'}")
    print(f"   Holder State: {holder_state if holder_state else 'N/A'}")
    if conn_id_verifier_holder:
        print(f"   Verifier Connection ID: {conn_id_verifier_holder}")
    if conn_id_holder_verifier:
        print(f"   Holder Connection ID: {conn_id_holder_verifier}")

print(f"{'='*60}")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-10-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 10 Output}, numbers=left, frame=single]
Connection: Holder --> Verifier erstellen...

Pre-Check: Prüfe existierende Connections...

   Verifier: 0 Connection(s) gefunden
   Holder: 1 Connection(s) gefunden

   Keine vollständige existierende Connection gefunden
   Erstelle neue Connection mit did:peer:4...

Invitation-Modus: did:peer:4 (P2P Connection)
Verifier: Invitation erstellt
   OOB ID: da555564-e80d-44e7-92ef-265dbfad8da0
   State: initial
   Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc
   Services: 1 service(s)
   Label: Verifier Agent
   Goal: Establish connection for credential verification

Holder: Invitation akzeptiert
   Connection ID (Holder): 925d0cbb-6f5b-43ac-9f64-199961e9af5b

   Verifier sucht Connection via invitation_msg_id...
     Prüfe 1 Connection(s)...

   Verifier: Connection gefunden via invitation_msg_id!
      Connection ID: 19df1913-7858-439c-9089-5a947b671c47
      State: active
      Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc
      Their Label: Holder Agent
      Updated At: 2025-12-08T13:39:34.353708Z

Post-Validation: Verifiziere Connection-Status...

   Verifier Connection 19df1913-7858-439c-9089-5a947b671c47:
      State: active
      Their Label: Holder Agent
      Their Role: invitee
      Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc

   Holder Connection 925d0cbb-6f5b-43ac-9f64-199961e9af5b:
      State: active
      Their Label: Verifier Agent
      Their Role: inviter
      Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc
      Invitation Msg IDs stimmen überein!

============================================================
CONNECTION ESTABLISHED (did:peer:4): Holder <--> Verifier
   Verifier Connection ID: 19df1913-7858-439c-9089-5a947b671c47
   Holder Connection ID: 925d0cbb-6f5b-43ac-9f64-199961e9af5b
   Verifier State: active
   Holder State: active
   Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc
============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-11}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 11}, numbers=left, frame=single]
# Cell 11: Connection-Übersicht anzeigen

print("Connection-Übersicht:\n")

connections_overview = []

# Issuer Connections
issuer_conns = api_get(ISSUER_ADMIN_URL, "/connections")
if issuer_conns and "results" in issuer_conns:
    for conn in issuer_conns["results"]:
        connections_overview.append({
            "Agent": "Issuer",
            "Partner": conn.get("their_label", "Unknown"),
            "Connection ID": conn["connection_id"],
            "State": conn["state"],
            "Invitation Msg ID": conn.get("invitation_msg_id", "N/A") if conn.get("invitation_msg_id") else "N/A",
            "Status": "active" if conn["state"] == "active" else "N/A"
        })

# Holder Connections
holder_conns = api_get(HOLDER_ADMIN_URL, "/connections")
if holder_conns and "results" in holder_conns:
    for conn in holder_conns["results"]:
        partner = conn.get("their_label", "Unknown")
        connections_overview.append({
            "Agent": "Holder",
            "Partner": partner,
            "Connection ID": conn["connection_id"],
            "State": conn["state"],
            "Invitation Msg ID": conn.get("invitation_msg_id", "N/A") if conn.get("invitation_msg_id") else "N/A",
            "Status": "active" if conn["state"] == "active" else "N/A"
        })

# Verifier Connections
verifier_conns = api_get(VERIFIER_ADMIN_URL, "/connections")
if verifier_conns and "results" in verifier_conns:
    for conn in verifier_conns["results"]:
        connections_overview.append({
            "Agent": "Verifier",
            "Partner": conn.get("their_label", "Unknown"),
            "Connection ID": conn["connection_id"],
            "State": conn["state"],
            "Invitation Msg ID": conn.get("invitation_msg_id", "N/A") if conn.get("invitation_msg_id") else "N/A",
            "Status": "active" if conn["state"] == "active" else "N/A"
        })

# Gruppiere nach invitation_msg_id um zusammengehörige Connections zu finden
all_connections = []
if issuer_conns and "results" in issuer_conns:
    for conn in issuer_conns["results"]:
        all_connections.append(("Issuer", conn))
if holder_conns and "results" in holder_conns:
    for conn in holder_conns["results"]:
        all_connections.append(("Holder", conn))
if verifier_conns and "results" in verifier_conns:
    for conn in verifier_conns["results"]:
        all_connections.append(("Verifier", conn))

# Gruppiere nach invitation_msg_id
from collections import defaultdict
connection_groups = defaultdict(list)
for agent, conn in all_connections:
    invitation_msg_id = conn.get("invitation_msg_id")
    if invitation_msg_id:
        connection_groups[invitation_msg_id].append((agent, conn))

# Zeige gruppierte Connections
for idx, (inv_msg_id, group) in enumerate(connection_groups.items(), 1):
    print(f"Connection Group {idx}:")
    print(f"   Invitation Msg ID: {inv_msg_id}")
    for agent, conn in group:
        print(f"   {agent}: {conn['connection_id']} (State: {conn['state']})")
    print()
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-11-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 11 Output}, numbers=left, frame=single]
Connection-Übersicht:

Connection Group 1:
   Invitation Msg ID: c22bf8d7-0784-49d0-be53-563a23aa6094
   Issuer: c5399b17-85f2-4adf-99a5-5aeeee8647d9 (State: active)
   Holder: 860434c0-7632-41d9-90ea-addaafa68a33 (State: active)

Connection Group 2:
   Invitation Msg ID: 6892dae2-a930-47ed-9f5e-cfc23ce856bc
   Holder: 925d0cbb-6f5b-43ac-9f64-199961e9af5b (State: active)
   Verifier: 19df1913-7858-439c-9089-5a947b671c47 (State: active)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-12}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 12}, numbers=left, frame=single]
# Cell 12 Wallet DID Übersicht anzeigen

print("Wallet-Übersicht - Alle DIDs:\n")

# Alle DIDs von allen Agenten abrufen
issuer_dids = api_get(ISSUER_ADMIN_URL, "/wallet/did")
holder_dids = api_get(HOLDER_ADMIN_URL, "/wallet/did")
verifier_dids = api_get(VERIFIER_ADMIN_URL, "/wallet/did")

#Zeige originale Responses
print("Originale /wallet/did Responses:\n")
if issuer_dids:
    pretty_print(issuer_dids, "Issuer Wallet DIDs")
if holder_dids:
    pretty_print(holder_dids, "Holder Wallet DIDs")
if verifier_dids:
    pretty_print(verifier_dids, "Verifier Wallet DIDs")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-12-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 12 Output}, numbers=left, frame=single]
Wallet-Übersicht - Alle DIDs:

Originale /wallet/did Responses:


============================================================
  Issuer Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:indy:9pbXiFBZZGwXKp61HQBz3J",
      "verkey": "2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm",
      "posture": "posted",
      "key_type": "ed25519",
      "method": "indy",
      "metadata": {
        "posted": true,
        "endpoint": "https://host.docker.internal:8020"
      }
    },
    {
      "did": "did:peer:4zQmP1fQXhQPJtqUh7rFgcDJ4pGkqoB9fq7bJeEiLtGnrtZj",
      "verkey": "4QNeR3HyZsU...sJcEJ1ZntCEgzpzLFNEqjUcFjb26CTetW7oYHLG2WbtnjTjPdCWGjAJCPwv5T",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmP1fQXhQPJtqUh7rFgcDJ4pGkqoB9fq7bJeEiLtGnrtZj:z25gYmQoBS9XWQb...ZApXw2BMeAy2#key-1",
        "kem_verkey": "V2cNxGP72k5zsX...ToX",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmP1fQXhQPJtqUh7rFgcDJ4pGkqoB9fq7bJeEiLtGnrtZj:z25gYmQoBS9XWQbLxd...Ay2",
      "verkey": "4QNeR3HyZsU...WGjAJCPwv5T",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmP1fQXhQPJtqUh7rFgcDJ4pGkqoB9fq7bJeEiLtGnrtZj:z25gYmQoBS9XWQ...ApXw2BMeAy2#key-1",
        "kem_verkey": "V2cNxGP72k5zsXR1b...Ls7ToX",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmQUH2nBvShqTckzHpZ4AQPECCvGYuvbY3YY2UwVzizw92:z25gYmQoBS9XW...GasR6p19J7Yx",
      "verkey": "JQFmycxRr3QrqiZ8dpbo...yZLiB",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "invitation_reuse": "true",
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmQUH2nBvShqTckzHpZ4AQPECCvGYuvbY3YY2UwVzizw92:z25gYmQoBS9XWQbLxdKX...R6p19J7Yx#key-1",
        "kem_verkey": "L9v7qzvr41...ekX45M",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    }
  ]
}

============================================================
  Holder Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:peer:4zQmaDykrygY16ZsPyPzjNFN6TH37xUrH5PmbG6ZsbcU5Yd2",
      "verkey": "4uJptUWFo15yXNCVddmfFJ7...YiQ68EggAH",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmaDykrygY16ZsPyPzjNFN6TH37xUrH5PmbG6ZsbcU5Yd2:z25gYmQoBS9XWQbLxdK...uBZFKwd5v#key-1",
        "kem_verkey": "dxUgqCJku6PcsEzG4mJ...H17FhgbruM",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmaDykrygY16ZsPyPzjNFN6TH37xUrH5PmbG6ZsbcU5Yd2:z25gYmQoBS9XWQbLxdK...KuBZFKwd5v",
      "verkey": "4uJptUWFo15yX...iQ68EggAH",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmaDykrygY16ZsPyPzjNFN6TH37xUrH5PmbG6ZsbcU5Yd2:z25gYmQoBS9XWQ...FKwd5v#key-1",
        "kem_verkey": "dxUgqCJk...FhgbruM",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmdb3wygVriVPEkZFUYfss4JB3Yf2ptp9Y9DoWj45ar5D4",
      "verkey": "FvTibokwn...YratQBsBPi",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmdb3wygVriVPEkZFUYfss4JB3Yf2ptp9Y9DoWj45ar5D4:z25gYmQoBS9XWQbL...ARuBPZmfJ#key-1",
        "kem_verkey": "EMjPS1V2X...DAYjGpyEYD",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmdb3wygVriVPEkZFUYfss4JB3Yf2ptp9Y9DoWj45ar5D4:z25gYmQoBS9XWQb...RuBPZmfJ",
      "verkey": "FvTibo...DYratQBsBPi",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmdb3wygVriVPEkZFUYfss4JB3Yf2ptp9Y9DoWj45ar5D4:z25gYmQoBS9XWQbL...ybARuBPZmfJ#key-1",
        "kem_verkey": "EMjPS1...yEYD",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    }
  ]
}

============================================================
  Verifier Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:peer:4zQmP9sGhMY7ziFdBZg2nGHQ4CxtpJ5HGwNNbYuT7QMm4PFV",
      "verkey": "NgRnsKJrZs...PoWxAqX1z1kM",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmP9sGhMY7ziFdBZg2nGHQ4CxtpJ5HGwNNbYuT7QMm4PFV:z25gYmQoBS9XWQ...B4pXQ3CY#key-1",
        "kem_verkey": "GJGw4G8...hLKbdh",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmP9sGhMY7ziFdBZg2nGHQ4CxtpJ5HGwNNbYuT7QMm4PFV:z25gYmQoBS9XW...Q3CY",
      "verkey": "NgRn...1z1kM",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmP9sGhMY7ziFdBZg2nGHQ4CxtpJ5HGwNNbYuT7QMm4PFV:z25gYmQ...pXQ3CY#key-1",
        "kem_verkey": "GJGw4...bdh",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    },
    {
      "did": "did:peer:4zQmRPZ9j5LyppRgJhrow3vfCRZ9CsxuLAJMza2hhhVrbDRG:z25gYmQoBS9XWQbL...EMa",
      "verkey": "27cheNeC...7AVQs",
      "posture": "wallet_only",
      "key_type": "ml-dsa-65",
      "method": "did:peer:4",
      "metadata": {
        "invitation_reuse": "true",
        "pqc_enabled": true,
        "signature_algorithm": "ml-dsa-65",
        "key_agreement_algorithm": "ml-kem-768",
        "kem_key_kid": "did:peer:4zQmRPZ9j5LyppRgJhrow3vfCRZ9CsxuLAJMza2hhhVrbDRG:z25gYmQoBS9XWQ...shmhEMa#key-1",
        "kem_verkey": "cNPQCfa2K9...BDf",
        "plugin": "pqc_didpeer4_fm",
        "version": "0.1.0"
      }
    }
  ]
}
\end{lstlisting}

\subsubsection{Teil 5: Credential Issuance - Notfall-Wartungszertifikat}
\label{sec:Teil5-Credential-Issuance-Notfall-Wartungszertifikat}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-13}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 13}, numbers=left, frame=single]
# Cell 13: Credential Offer senden (Issuer --> Holder)

print("Credential Offer senden (Issuer --> Holder)...\n")

start_time = time.time()

# Credential-Daten vorbereiten
credential_offer_data = {
    "comment": "KRITIS Notfall-Wartungszertifikat",
    "connection_id": conn_id_issuer_holder,
    "filter": {
        "indy": {
            "cred_def_id": cred_def_id
        }
    },
    "credential_preview": {
        "@type": "issue-credential/2.0/credential-preview",
        "attributes": [
            {"name": "first_name", "value": "Max"},
            {"name": "name", "value": "Mustermann"},
            {"name": "organisation", "value": "Musterfirma GmbH"},
            {"name": "role", "value": "Notfalltechniker"},
            {"name": "cert_type", "value": "Notfall-Wartungsberechtigung"},
            {"name": "facility_type", "value": "Umspannwerk Nord-Ost"},
            {"name": "epoch_valid_from", "value": "1765026000"},
            {"name": "epoch_valid_until", "value": "1765033200"},
            {"name": "security_clearance_level", "value": "2"}  # Ü2 - Erweiterte Sicherheitsüberprüfung
        ]
    },
    "trace": False
}

# Credential Offer senden
cred_offer_response = api_post(
    ISSUER_ADMIN_URL,
    "/issue-credential-2.0/send-offer",
    credential_offer_data
)

if cred_offer_response is not None:
    cred_ex_id = cred_offer_response.get("cred_ex_id")
    cred_ex_state = cred_offer_response.get("state")

    print(f"Credential Offer gesendet")
    print(f"   Exchange ID: {cred_ex_id}")
    print(f"   State:       {cred_ex_state}")

    # ========================================
    # WARTE AUF CREDENTIAL-AUSSTELLUNG
    # ========================================
    print("\n   Warte auf Credential-Ausstellung (auto-store auf Holder)...")
    time.sleep(5)

    # ========================================
    # STATUS-CHECK AUF BEIDEN SEITEN
    # ========================================
    print("\nStatus-Check:\n")

    # 1. Issuer-Seite Status (BENÖTIGT für Revocation IDs!)
    cred_status_issuer = api_get(
        ISSUER_ADMIN_URL,
        f"/issue-credential-2.0/records/{cred_ex_id}"
    )

    preserve_flag_active = False
    issuer_state = None

    if cred_status_issuer:
        preserve_flag_active = True
        issuer_state = cred_status_issuer.get("cred_ex_record", {}).get("state")
        print(f"   Issuer Exchange:")
        print(f"      State: {issuer_state}")
        print(f"      --preserve-exchange-records aktiv!")
    else:
        print(f"   Issuer Exchange nicht abrufbar")
        print(f"      --preserve-exchange-records ist NICHT aktiv!")

    # 2. Holder Credentials prüfen
    print("\n   Holder Credentials:")
    holder_creds = api_get(HOLDER_ADMIN_URL, "/credentials")

    credential_stored = False
    credential_referent = None

    if holder_creds and "results" in holder_creds:
        for cred in holder_creds["results"]:
            if cred.get("cred_def_id") == cred_def_id:
                credential_stored = True
                credential_referent = cred.get("referent")
                print(f"      Credential ID: {credential_referent}")
                print(f"      Schema ID: {cred.get('schema_id', 'N/A')[:50]}...")

                # Zeige Attribute
                attrs = cred.get("attrs", {})
                if attrs:
                    print(f"      Attributes:")
                    for key, value in list(attrs.items()):
                        print(f"         - {key}: {value}")
                break

    # ========================================
    # REVOCATION IDs SPEICHERN (KRITISCH!)
    # ========================================
    rev_reg_id = None
    cred_rev_id = None

    if cred_status_issuer:
        indy_data = cred_status_issuer.get("indy", {})
        rev_reg_id = indy_data.get("rev_reg_id")
        cred_rev_id = indy_data.get("cred_rev_id")

        if rev_reg_id and cred_rev_id:
            print("\n" + "="*60)
            print("REVOCATION-INFORMATIONEN")
            print("="*60)
            print(f" Revocation Registry ID: {rev_reg_id}")
            print(f" Credential Revocation ID: {cred_rev_id}")
            print(" IDs gespeichert für Revocation-Workflow (Cell 19-21)")
            print("="*60 + "\n")

    # ========================================
    # ISSUER CREDENTIAL REGISTRY
    # ========================================
    print("="*60)
    print("ISSUER: Alle ausgestellten Credentials")
    print("="*60)

    all_issuer_records = api_get(ISSUER_ADMIN_URL, "/issue-credential-2.0/records")

    if all_issuer_records and "results" in all_issuer_records:
        results = all_issuer_records["results"]

        if len(results) > 0:
            print(f"\nIssuer hat {len(results)} Credential Exchange Record(s)\n")

            for idx, record in enumerate(results, 1):
                print(f"   Credential #{idx}:")
                print(f"      Exchange ID: {record['cred_ex_record']['cred_ex_id']}")
                print(f"      State: {record['cred_ex_record']['state']}")

                # Cred Def ID
                cred_def_from_record = None
                if "cred_offer" in record:
                    cred_def_from_record = record.get("cred_offer", {}).get("cred_def_id")

                if cred_def_from_record:
                    print(f"      Cred Def ID: {cred_def_from_record[:50]}...")

                # Attributes aus Preview
                if "cred_preview" in record:
                    attrs = record.get("cred_preview", {}).get("attributes", [])
                    if attrs:
                        print(f"     Attributes:")
                        for attr in attrs[:3]:
                            print(f"         - {attr.get('name')}: {attr.get('value')}")
                        if len(attrs) > 3:
                            print(f"         ... und {len(attrs)-3} weitere")
                print()
        else:
            print("\nRegistry ist LEER\n")

            if not preserve_flag_active:
                print("   --preserve-exchange-records ist NICHT aktiv!")
                print("   Lösung: docker-compose restart issuer")
            else:
                print("   Credential erscheint nach erneutem Ausführen dieser Cell")
            print()

    # ========================================
    # FINALE SUMMARY
    # ========================================

    print("="*60)
    if credential_stored:
        print(f"KRITIS-NOTFALL-WARTUNGSZERTIFIKAT ERFOLGREICH AUSGESTELLT")
        print(f"   Issuer State: {issuer_state if issuer_state else 'done'}")
        print(f"   Credential im Holder Wallet: Yes")
        print(f"   Credential Referent: {credential_referent}")

        if rev_reg_id and cred_rev_id:
            print(f"   Revocation IDs gespeichert: Yes")
        else:
            print(f"   Revocation IDs nicht gefunden!")

        if preserve_flag_active:
            print(f"   Issuer Registry verfügbar: Yes")
        else:
            print(f"   Issuer Registry verfügbar: NO (restart issuer)")
    else:
        print(f"CREDENTIAL STATUS UNKLAR")
        print(f"   Issuer State: {issuer_state if issuer_state else 'N/A'}")
        print(f"   Credential im Wallet: NO")

    print(f"{'='*60}")

    # Zeige vollständige Response
    # pretty_print(cred_status_issuer, "Credential Offer Response (KRITIS)")

else:
    print("Fehler beim Senden des Credential Offers")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-13-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 13 Output}, numbers=left, frame=single]
Credential Offer senden (Issuer --> Holder)...

Credential Offer gesendet
   Exchange ID: 4cd96e9e-ec5f-4b54-8df3-2ee1d5e0607d
   State:       offer-sent

  Warte auf Credential-Ausstellung (auto-store auf Holder)...

Status-Check:

   Issuer Exchange:
       State: done
      --preserve-exchange-records aktiv!

   Holder Credentials:
       Credential ID: 39ac5fc4-efc2-45eb-9a21-01c589757b65
       Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintena...
       Attributes:
         - facility_type: Umspannwerk Nord-Ost
         - epoch_valid_from: 1765026000
         - organisation: Musterfirma GmbH
         - cert_type: Notfall-Wartungsberechtigung
         - first_name: Max
         - role: Notfalltechniker
         - epoch_valid_until: 1765033200
         - security_clearance_level: 2
         - name: Mustermann

============================================================
REVOCATION-INFORMATIONEN
============================================================
   Revocation Registry ID: 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e
    Credential Revocation ID: 1
    IDs gespeichert für Revocation-Workflow (Cell 19-21)
============================================================

============================================================
ISSUER: Alle ausgestellten Credentials
============================================================

Issuer hat 1 Credential Exchange Record(s)

   Credential #1:
      Exchange ID: 4cd96e9e-ec5f-4b54-8df3-2ee1d5e0607d
      State: done

============================================================
KRITIS-NOTFALL-WARTUNGSZERTIFIKAT ERFOLGREICH AUSGESTELLT
   Issuer State: done
   Credential im Holder Wallet: Yes
   Credential Referent: 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Revocation IDs gespeichert: Yes
   Issuer Registry verfügbar: Yes
============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-14}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 14}, numbers=left, frame=single]
# Cell 14: Holder Credentials im Detail anzeigen

print("Holder Wallet - Gespeicherte Credentials:\n")

holder_creds = api_get(HOLDER_ADMIN_URL, "/credentials")

if holder_creds and "results" in holder_creds and len(holder_creds["results"]) > 0:
    for idx, cred in enumerate(holder_creds["results"], 1):
        print(f"\n{'='*60}")
        print(f"Credential #{idx}")
        print(f"{'='*60}")
        print(f"Referent: {cred.get('referent', 'N/A')}")
        print(f"Schema ID: {cred.get('schema_id', 'N/A')}")
        print(f"Cred Def ID: {cred.get('cred_def_id', 'N/A')}")
        
        # Credentials Revoked Abfrage gegen /credential/revoked/{credential_id ==> Referent}
        print(f"Revoked Status: {api_get(HOLDER_ADMIN_URL, f'/credential/revoked/{cred.get('referent', 'N/A')}').get('revoked', 'N/A')}")

        if "attrs" in cred:
            print("\nAttribute:")
            for attr_name, attr_value in cred["attrs"].items():
                print(f"  - {attr_name}: {attr_value}")
else:
    print("Holder hat noch keine Credentials")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-14-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 14 Output}, numbers=left, frame=single]
Holder Wallet - Gespeicherte Credentials:


============================================================
Credential #1
============================================================
Referent: 39ac5fc4-efc2-45eb-9a21-01c589757b65
Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
Revoked Status: False

Attribute:
  - first_name: Max
  - cert_type: Notfall-Wartungsberechtigung
  - organisation: Musterfirma GmbH
  - epoch_valid_from: 1765026000
  - epoch_valid_until: 1765033200
  - role: Notfalltechniker
  - facility_type: Umspannwerk Nord-Ost
  - security_clearance_level: 2
  - name: Mustermann
\end{lstlisting}

\pagebreak

\subsubsection{Teil 6: Proof Presentation - Zutrittskontrolle am Umspannwerk}
\label{sec:Teil6-Proof-Presentation-Zutrittskontrolle-am-Umspannwerk}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-15}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 15}, numbers=left, frame=single]
# Cell 15: Proof Request senden (Verifier --> Holder) - mit REVOCATION ALWAYS-ON

print("Proof Request senden (Verifier --> Holder)...\n")

start_time = time.time()

# Get current timestamp for non-revocation interval
current_timestamp = int(time.time())

# Proof Request Data (ACA-Py API) - Privacy-Preserving ZKP!
# API: POST /present-proof-2.0/send-request
# REVEALED: cert_type, facility_type, epoch_valid_from, epoch_valid_until, role (5 Attribute)
# ZKP PREDICATE: security_clearance_level >= 2 (Ü2) - OHNE Offenlegung der exakten Stufe!
# UNREVEALED: first_name, name, organisation (3 Attribute - ZKP ohne Offenlegung!)
proof_request_data = {
    "connection_id": conn_id_verifier_holder,  # from Cell 15
    "presentation_request": {
        "indy": {
            "name": "Proof of KRITIS Emergency Maintenance Authorization",
            "version": "1.0",
            "requested_attributes": {
                "attr1_referent": {
                    "name": "cert_type",
                    "restrictions": [{"cred_def_id": cred_def_id}],
                    "non_revoked": {"from": 0, "to": current_timestamp}
                },
                "attr2_referent": {
                    "name": "facility_type",
                    "restrictions": [{"cred_def_id": cred_def_id}],
                    "non_revoked": {"from": 0, "to": current_timestamp}
                },
                "attr3_referent": {
                    "name": "epoch_valid_from",
                    "restrictions": [{"cred_def_id": cred_def_id}],
                    "non_revoked": {"from": 0, "to": current_timestamp}
                },
                "attr4_referent": {
                    "name": "epoch_valid_until",
                    "restrictions": [{"cred_def_id": cred_def_id}],
                    "non_revoked": {"from": 0, "to": current_timestamp}
                },
                "attr5_referent": {
                    "name": "role",
                    "restrictions": [{"cred_def_id": cred_def_id}],
                    "non_revoked": {"from": 0, "to": current_timestamp}
                }
            },
            "requested_predicates": {
            "pred1_clearance": {
                "name": "security_clearance_level",
                "p_type": ">=",
                "p_value": 2,
                "restrictions": [{"cred_def_id": cred_def_id}],
                "non_revoked": {"from": 0, "to": current_timestamp}
            }
        }
        }
    }
}

print(f"Proof Request für KRITIS-Zutrittsberechtigung:")
print(f"   Endpoint:      /present-proof-2.0/send-request")
print(f"   Connection:    {conn_id_verifier_holder}")
print(f"   Cred Def ID:   {cred_def_id}")
print(f"   Proof Name:    Proof of KRITIS Emergency Maintenance Authorization")
print(f"   REVEALED:      Berechtigung, Anlage, Sicherheitsstufe, Gültigkeitszeitraum, Rolle (6 Attribute)")
print(f"   DATENSCHUTZ: Identität (Vor-/Nachname) und Organisation werden NICHT offengelegt (Selective-Disclosure)\n")

# Send Proof Request (ACA-Py endpoint)
proof_request_response = api_post(
    VERIFIER_ADMIN_URL,
    "/present-proof-2.0/send-request",
    proof_request_data
)

if proof_request_response is not None:
    # Response Format: {"pres_ex_id": "...", "state": "...", ...}
    pres_ex_id = proof_request_response.get("pres_ex_id")
    pres_ex_state = proof_request_response.get("state")

    print(f"Proof Request gesendet")
    print(f"   Exchange ID: {pres_ex_id}")
    print(f"   State:       {pres_ex_state}")

    # Show full Proof Request Response
    # pretty_print(proof_request_response, "Proof Request Response (KRITIS)")

else:
    print("Fehler beim Senden des Proof Requests")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-15-Output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 15 Output}, numbers=left, frame=single]
Proof Request senden (Verifier --> Holder)...

Proof Request für KRITIS-Zutrittsberechtigung:
   Endpoint:      /present-proof-2.0/send-request
   Connection:    19df1913-7858-439c-9089-5a947b671c47
   Cred Def ID:   9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
   Proof Name:    Proof of KRITIS Emergency Maintenance Authorization
   REVEALED:      Berechtigung, Anlage, Sicherheitsstufe, Gültigkeitszeitraum, Rolle (6 Attribute)
   DATENSCHUTZ: Identität (Vor-/Nachname) und Organisation werden NICHT offengelegt (Selective-Disclosure)

Proof Request gesendet
   Exchange ID: 64b89588-48f2-4316-b363-d1bc6afcc841
   State:       request-sent
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-16}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 16}, numbers=left, frame=single]
# Cell 16: Holder - Proof Request empfangen und Credentials auswählen

print("Holder: Proof Request verarbeiten...\n")

if pres_ex_id:
    # Holder prüft empfangene Proof Requests
    time.sleep(2)
    
    holder_pres_ex = api_get(HOLDER_ADMIN_URL, "/present-proof-2.0/records")
    
    if holder_pres_ex and "results" in holder_pres_ex:
        # Finde den aktuellen Presentation Exchange
        current_pres = None
        for pres in holder_pres_ex["results"]:
            if pres.get("state") == "request-received":
                current_pres = pres
                holder_pres_ex_id = pres["pres_ex_id"]
                break
        
        if current_pres:
            print(f" Holder hat Proof Request empfangen")
            print(f"   Presentation Exchange ID: {holder_pres_ex_id}")
            
            # Credentials für die Presentation abrufen
            creds_for_pres = api_get(
                HOLDER_ADMIN_URL,
                f"/present-proof-2.0/records/{holder_pres_ex_id}/credentials"
            )
            
            if creds_for_pres:
                print(f"\n Verfügbare Credentials für Presentation:")
                
                # API kann entweder list oder dict zurückgeben
                total_creds = 0
                
                if isinstance(creds_for_pres, list):
                    # Liste von Credential Records
                    total_creds = len(creds_for_pres)
                    print(f"   Gefunden: {total_creds} Credential(s)")
                    
                    # Zeige Details der ersten paar Credentials
                    for idx, cred in enumerate(creds_for_pres[:3], 1):
                        print(f"\n   Credential #{idx}:")
                        if "cred_info" in cred:
                            cred_info = cred["cred_info"]
                            print(f"       Referent: {cred_info.get('referent', 'N/A')}")
                            print(f"       Schema ID: {cred_info.get('schema_id', 'N/A')}")
                            print(f"       Cred Def ID: {cred_info.get('cred_def_id', 'N/A')}")
                            
                            # Zeige Attribute
                            attrs = cred_info.get("attrs", {})
                            if attrs:
                                print(f"       Attributes:")
                                # Zeige erste 3 Attribute
                                shown = 0
                                for key, value in attrs.items():
                                    print(f"         - {key}: {value}")
                    
                elif isinstance(creds_for_pres, dict):
                    # Dictionary mit referent als key
                    for referent, creds in creds_for_pres.items():
                        if isinstance(creds, list):
                            total_creds += len(creds)
                            print(f"   {referent}: {len(creds)} Credential(s)")
                            
                            # Zeige Details des ersten Credentials
                            if len(creds) > 0:
                                cred = creds[0]
                                if "cred_info" in cred:
                                    cred_info = cred["cred_info"]
                                    print(f"       Cred Def ID: {cred_info.get('cred_def_id', 'N/A')}")
                                    attrs = cred_info.get("attrs", {})
                                    if attrs:
                                        attr_count = len(attrs)
                                        print(f"       {attr_count} Attribute(s) verfügbar")
                else:
                    print(f"     Unbekanntes Format: {type(creds_for_pres).__name__}")
                    total_creds = 0
                
                print(f"\n Total: {total_creds} passende Credential(s) gefunden")
                
                if total_creds == 0:
                    print(f"\n  WARNUNG: Keine passenden Credentials gefunden!")
                    print(f"    Stelle sicher dass Cell 16 erfolgreich war")
                    print(f"    Prüfe ob Credential im Holder Wallet ist (Cell 16 Output)")
                    print(f"    Schema/Cred Def müssen mit Proof Request übereinstimmen")
            else:
                print(f"\n  Keine Credentials für Presentation verfügbar")
                print(f"   API-Antwort war leer oder None")
        else:
            print("  Kein Proof Request im Status 'request-received'")
            print("\n   Verfügbare Presentation Exchanges:")
            for pres in holder_pres_ex["results"]:
                print(f"       {pres['pres_ex_id']} State: {pres.get('state')}")
    else:
        print("  Keine Presentation Exchanges gefunden")
        print("   Führe Cell 18 aus um Proof Request zu senden")
else:
    print("  Übersprungen: Kein Presentation Exchange vorhanden")
    print("   Führe Cell 18 aus um Proof Request zu erstellen")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-16-Output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 16 Output}, numbers=left, frame=single]
Holder: Proof Request verarbeiten...

Holder hat Proof Request empfangen
   Presentation Exchange ID: fb85c70d-79c9-40a7-b0cd-d75c032cfc86

Verfügbare Credentials für Presentation:
   Gefunden: 1 Credential(s)

   Credential #1:
      Referent: 39ac5fc4-efc2-45eb-9a21-01c589757b65
      Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
      Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
      Attributes:
         - facility_type: Umspannwerk Nord-Ost
         - organisation: Musterfirma GmbH
         - epoch_valid_until: 1765033200
         - name: Mustermann
         - epoch_valid_from: 1765026000
         - cert_type: Notfall-Wartungsberechtigung
         - role: Notfalltechniker
         - security_clearance_level: 2
         - first_name: Max

Total: 1 passende Credential(s) gefunden
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-17}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 17}, numbers=left, frame=single]
# Cell 17: Holder - Presentation senden

print("Holder: Presentation senden...\n")

if pres_ex_id and holder_pres_ex_id:
    start_time = time.time()
    
    # Hole Credentials nochmal
    creds_for_pres = api_get(
        HOLDER_ADMIN_URL,
        f"/present-proof-2.0/records/{holder_pres_ex_id}/credentials"
    )
    
    if creds_for_pres:
        print(f"Credentials für Presentation gefunden")
        
        # Baue Presentation Request
        requested_credentials = {
            "indy": {
                "requested_attributes": {},
                "requested_predicates": {},
                "self_attested_attributes": {}
            }
        }
        
        # Handle verschiedene API-Response-Formate
        if isinstance(creds_for_pres, list):
            # Liste von Credential-Objekten
            print(f"   Format: Liste mit {len(creds_for_pres)} Credential(s)\n")
            
            # Jedes Credential-Objekt hat presentation_referents
            for cred_item in creds_for_pres:
                if "cred_info" in cred_item:
                    cred_id = cred_item["cred_info"].get("referent")
                    
                    # Hole presentation_referents (welche Felder dieses Cred erfüllen kann)
                    pres_referents = cred_item.get("presentation_referents", [])
                    
                    for ref in pres_referents:
                        # Prüfe ob es Attribute oder Predicate ist
                        if "attr" in ref or "attribute" in ref.lower():
                            requested_credentials["indy"]["requested_attributes"][ref] = {
                                "cred_id": cred_id,
                                "revealed": True
                            }
                            print(f"   Mapped attribute '{ref}' --> Credential {cred_id}")
                        elif "pred" in ref or "predicate" in ref.lower():
                            requested_credentials["indy"]["requested_predicates"][ref] = {
                                "cred_id": cred_id
                            }
                            print(f"   Mapped predicate '{ref}' --> Credential {cred_id}")
        
        elif isinstance(creds_for_pres, dict):
            # Dictionary mit referent als key (Legacy-Format)
            print(f"   Format: Dictionary mit {len(creds_for_pres)} Referent(s)\n")
            
            for referent, creds in creds_for_pres.items():
                if isinstance(creds, list) and len(creds) > 0:
                    first_cred = creds[0]
                    cred_id = first_cred.get("cred_info", {}).get("referent")
                    
                    if "attr" in referent:
                        requested_credentials["indy"]["requested_attributes"][referent] = {
                            "cred_id": cred_id,
                            "revealed": True
                        }
                        print(f"   Mapped attribute '{referent}' --> Credential {cred_id[:20]}...")
                    elif "predicate" in referent:
                        requested_credentials["indy"]["requested_predicates"][referent] = {
                            "cred_id": cred_id
                        }
                        print(f"   Mapped predicate '{referent}' --> Credential {cred_id[:20]}...")
        
        # Prüfe ob wir Credentials gemappt haben
        total_mapped = (
            len(requested_credentials["indy"]["requested_attributes"]) +
            len(requested_credentials["indy"]["requested_predicates"])
        )
        
        if total_mapped == 0:
            print("\n  WARNUNG: Keine Credentials gemappt!")
            print("   Versuche automatisches Mapping...\n")
            
            # Fallback: Verwende erstes verfügbares Credential für alle Requests
            # Hole Proof Request Details
            pres_ex_details = api_get(
                HOLDER_ADMIN_URL,
                f"/present-proof-2.0/records/{holder_pres_ex_id}"
            )
            
            if pres_ex_details and "pres_request" in pres_ex_details:
                pres_request = pres_ex_details["pres_request"]
                
                # Hole erstes verfügbares Credential
                first_cred_id = None
                if isinstance(creds_for_pres, list) and len(creds_for_pres) > 0:
                    first_cred_id = creds_for_pres[0].get("cred_info", {}).get("referent")
                elif isinstance(creds_for_pres, dict):
                    for creds in creds_for_pres.values():
                        if isinstance(creds, list) and len(creds) > 0:
                            first_cred_id = creds[0].get("cred_info", {}).get("referent")
                            break
                
                if first_cred_id:
                    # Map alle requested attributes
                    if "requested_attributes" in pres_request:
                        for attr_ref in pres_request["requested_attributes"].keys():
                            requested_credentials["indy"]["requested_attributes"][attr_ref] = {
                                "cred_id": first_cred_id,
                                "revealed": True
                            }
                            print(f"   Auto-mapped attribute '{attr_ref}' --> {first_cred_id[:20]}...")
                    
                    # Map alle requested predicates
                    if "requested_predicates" in pres_request:
                        for pred_ref in pres_request["requested_predicates"].keys():
                            requested_credentials["indy"]["requested_predicates"][pred_ref] = {
                                "cred_id": first_cred_id
                            }
                            print(f"   Auto-mapped predicate '{pred_ref}' --> {first_cred_id[:20]}...")
                    
                    total_mapped = (
                        len(requested_credentials["indy"]["requested_attributes"]) +
                        len(requested_credentials["indy"]["requested_predicates"])
                    )
        
        print(f"\nPresentation Mapping:")
        print(f"    Requested Attributes: {len(requested_credentials['indy']['requested_attributes'])}")
        print(f"    Requested Predicates: {len(requested_credentials['indy']['requested_predicates'])}")
        print(f"    Total Mapped: {total_mapped}")
        
        if total_mapped > 0:
            # Sende Presentation
            print("\nSende Presentation...")
            presentation_response = api_post(
                HOLDER_ADMIN_URL,
                f"/present-proof-2.0/records/{holder_pres_ex_id}/send-presentation",
                requested_credentials
            )
            
            if presentation_response is not None:
                print(f"\nHolder hat Presentation gesendet")
                print(f"   Presentation Exchange ID: {holder_pres_ex_id}")
                print(f"   State: {presentation_response.get('state', 'N/A')}")
                print(f"   Role: {presentation_response.get('role', 'N/A')}")
                
                # Warte auf Verification
                print("\n   Warte auf Verification durch Verifier...")
                time.sleep(3)
                
                # Prüfe finalen Status (kann 404 geben wenn abgeschlossen!)
                final_status = api_get(
                    HOLDER_ADMIN_URL,
                    f"/present-proof-2.0/records/{holder_pres_ex_id}"
                )
                
                if final_status:
                    # Record noch vorhanden (--preserve-exchange-records aktiv)
                    final_state = final_status.get("state")
                    print(f"\n   Finaler Holder Status: {final_state}")
                    
                    if final_state in ["done", "presentation-sent"]:
                        print(f"   Presentation erfolgreich abgeschlossen!")
                        print(f"   Record bleibt verfügbar (--preserve-exchange-records aktiv)")
                    elif final_state == "abandoned":
                        print(f"     Presentation wurde abgebrochen")
                    else:
                        print(f"   Presentation in Bearbeitung...")
                else:
                    # 404 Error = Record wurde gelöscht = erfolgreich abgeschlossen!
                    print(f"\n   Finaler Holder Status: completed/archived (404)")
                    print(f"    Presentation erfolgreich abgeschlossen!")
                    print(f"     Record wurde nach Completion gelöscht (--preserve-exchange-records nicht aktiv)")
                    print(f"\n    Hinweis: Für dauerhafte Records, starte Holder neu:")
                    print(f"      docker-compose restart holder")
                
            else:
                print("\n Fehler beim Senden der Presentation")
                print("   API-Response war leer oder fehlerhaft")
        else:
            print("\n Kann Presentation nicht senden - keine Credentials gemappt")
            print("    Prüfe ob Credentials die Proof Request Anforderungen erfüllen")
            print("    Prüfe Schema und Cred Def IDs")
    else:
        print("  Keine Credentials für Presentation gefunden")
        print("    Führe Cell 16 aus um Credential auszustellen")
        print("    Prüfe ob Credential im Holder Wallet ist")
else:
    print("  Übersprungen: Presentation Exchange nicht vorhanden")
    if not pres_ex_id:
        print("    Verifier Presentation Exchange ID fehlt (führe Cell 18 aus)")
    if not holder_pres_ex_id:
        print("    Holder Presentation Exchange ID fehlt (führe Cell 19 aus)")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-17-Output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 17 Output}, numbers=left, frame=single]
Holder: Presentation senden...

Credentials für Presentation gefunden
   Format: Liste mit 1 Credential(s)

   Mapped attribute 'attr3_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr1_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped predicate 'pred1_clearance' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr4_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr2_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr5_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65

Presentation Mapping:
   Requested Attributes: 5
   Requested Predicates: 1
   Total Mapped: 6

Sende Presentation...

Holder hat Presentation gesendet
   Presentation Exchange ID: fb85c70d-79c9-40a7-b0cd-d75c032cfc86
   State: presentation-sent
   Role: prover

   Warte auf Verification durch Verifier...

   Finaler Holder Status: done
   Presentation erfolgreich abgeschlossen!
   Record bleibt verfügbar (--preserve-exchange-records aktiv)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-18}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 18}, numbers=left, frame=single]
# Cell 18: Verifier - Presentation verifizieren (mit Revocation Detection + Zeitgültigkeit)

print(" Presentation verifizieren (Verifier)...\n")

start_time = time.time()

# Wait for presentation to be received
import time as t
t.sleep(5)  # Give Holder time to generate and send proof

# Get presentation exchange record (ACA-Py endpoint)
# API: GET /present-proof-2.0/records/{pres_ex_id}
pres_ex_record = api_get(
    VERIFIER_ADMIN_URL,
    f"/present-proof-2.0/records/{pres_ex_id}"
)

if pres_ex_record is not None:
    # Response Format: {"state": "...", "verified": "...", "pres": {...}, "by_format": {...}, ...}
    pres_state = pres_ex_record.get("state")
    pres_verified = pres_ex_record.get("verified")

    # ========================================
    # ATTRIBUTE MAPPING: referent -> name -> value
    # ========================================
    # Step 1: Get requested_attributes from by_format.pres_request.indy (referent -> name mapping)
    by_format = pres_ex_record.get("by_format", {})
    pres_request_indy = by_format.get("pres_request", {}).get("indy", {})
    requested_attributes = pres_request_indy.get("requested_attributes", {})

    # Step 2: Get revealed_attrs from by_format.pres.indy.requested_proof (referent -> value mapping)
    pres_indy = by_format.get("pres", {}).get("indy", {})
    requested_proof = pres_indy.get("requested_proof", {})
    revealed_attrs_by_referent = requested_proof.get("revealed_attrs", {})

    # Step 3: Build name -> value mapping
    revealed_attrs = {}
    for referent, attr_data in revealed_attrs_by_referent.items():
        # Get the attribute name from requested_attributes
        if referent in requested_attributes:
            attr_name = requested_attributes[referent].get("name")
            attr_value = attr_data.get("raw")
            if attr_name and attr_value:
                revealed_attrs[attr_name] = attr_value

    print(f" Proof erfolgreich verifiziert!")

    print(f"\nVerifizierte Attribute (REVEALED):")
    for name, value in revealed_attrs.items():
        print(f"   - {name}: {value}")

    print(f"\n PER ZKP VERIFIZIERT ABER DURCH DATENSCHUTZ GESCHÜTZT (UNREVEALED):")
    print(f"   - Vorname: NICHT offengelegt (Zero-Knowledge-Proof)")
    print(f"   - Nachname: NICHT offengelegt (Zero-Knowledge-Proof)")
    print(f"   - Organisation: NICHT offengelegt (Zero-Knowledge-Proof)")

    print(f"\n   State:     {pres_state}")
    print(f"   Verified:  {pres_verified}")

    # ========================================
    # REVOCATION CHECK
    # ========================================
    is_revoked = False
    if pres_state == "done" and pres_verified == "true":
        print(f"\n Credential ist NICHT revoked (gültig)")
    else:
        print(f"\n Credential ist REVOKED!")
        is_revoked = True

    # ========================================
    # ZEITGÜLTIGKEITS-PRÜFUNG
    # ========================================
    print("\n" + "="*60)
    print("ZEITGÜLTIGKEITS-PRÜFUNG")
    print("="*60)

    current_epoch = 1765029600  # Beispiel: Fester Zeitpunkt für Test (2024-09-05 12:00:00 UTC)
    epoch_valid_from = None
    epoch_valid_until = None
    is_time_valid = False

    # Extrahiere epoch_valid_from und epoch_valid_until aus revealed_attrs (name -> value mapping)
    if "epoch_valid_from" in revealed_attrs:
        epoch_valid_from = int(revealed_attrs["epoch_valid_from"])

    if "epoch_valid_until" in revealed_attrs:
        epoch_valid_until = int(revealed_attrs["epoch_valid_until"])

    if epoch_valid_from is not None and epoch_valid_until is not None:
        # Konvertiere zu lesbaren Timestamps
        from datetime import datetime
        valid_from_dt = datetime.fromtimestamp(epoch_valid_from)
        valid_until_dt = datetime.fromtimestamp(epoch_valid_until)
        current_dt = datetime.fromtimestamp(current_epoch)

        print(f"   Aktueller Zeitpunkt: {current_dt.strftime('%Y-%m-%d %H:%M:%S')} (Epoch: {current_epoch}) ==> Beispielwert")
        print(f"   Gültig ab:            {valid_from_dt.strftime('%Y-%m-%d %H:%M:%S')} (Epoch: {epoch_valid_from})")
        print(f"   Gültig bis:           {valid_until_dt.strftime('%Y-%m-%d %H:%M:%S')} (Epoch: {epoch_valid_until})")

        # Prüfe Zeitgültigkeit
        if epoch_valid_from <= current_epoch <= epoch_valid_until:
            is_time_valid = True
            print(f"\n    Zertifikat ist ZEITLICH GÜLTIG")
        else:
            is_time_valid = False
            if current_epoch < epoch_valid_from:
                print(f"\n    Zertifikat ist NOCH NICHT gültig (zu früh)")
            else:
                print(f"\n    Zertifikat ist ABGELAUFEN (zu spät)")
    else:
        print(f"    Zeitgültigkeits-Attribute nicht gefunden!")
        print(f"      epoch_valid_from: {'gefunden' if epoch_valid_from else 'FEHLT'}")
        print(f"      epoch_valid_until: {'gefunden' if epoch_valid_until else 'FEHLT'}")

    print("="*60)


    # ========================================
    # PREDICATE AUSWERTUNG (ZKP)
    # ========================================
    print("\n" + "="*60)
    print("ZERO-KNOWLEDGE-PROOF AUSWERTUNG")
    print("="*60)

    predicates = requested_proof.get("predicates", {})
    has_required_clearance = False

    if predicates:
        for ref, pred_data in predicates.items():
            print(f"    Predicate erfüllt: security_clearance_level >= 2 (Ü2)")
            print(f"   Zero-Knowledge-Proof: Exakte Sicherheitsstufe NICHT offengelegt!")
            print(f"   Techniker hat mindestens Ü2 (Erweiterte Sicherheitsüberprüfung)")
            has_required_clearance = True
    else:
        print(f"    Keine Predicates im Proof gefunden!")

    print("="*60)

    # ========================================
    # FINALE ZUGRIFFSENTSCHEIDUNG
    # ========================================
    print("\n" + "="*60)
    print("FINALE ZUGRIFFSENTSCHEIDUNG")
    print("="*60)

    # Alle drei Bedingungen müssen erfüllt sein
    if not is_revoked and is_time_valid and has_required_clearance:
        print(f"\n ZUGANG GEWÄHRT")
        print(f"   Credential ist gültig (nicht revoked)")
        print(f"   Zertifikat ist zeitlich gültig")
        print(f"   Sicherheitsfreigabe >= Ü2 (Zero-Knowledge-Proof)")
        print(f"\n   Zugang zum Umspannwerk Nord-Ost GEWÄHRT")
    else:
        print(f"\n ZUGANG VERWEIGERT")
        if is_revoked:
            print(f"   Credential ist REVOKED")
        if not is_time_valid:
            print(f"   Zertifikat ist NICHT zeitlich gültig")
        if not has_required_clearance:
            print(f"   Sicherheitsfreigabe NICHT ausreichend (< Ü2)")
        print(f"\n   Zugang zum Umspannwerk Nord-Ost VERWEIGERT")

    print("="*60)

    print(f"\n Privacy-Preserving Verification erfolgreich (DSGVO-konform)")

    # Show full Presentation Record
    # pretty_print(pres_ex_record, "Presentation Record (KRITIS)")

else:
    print(" Fehler beim Abrufen der Presentation")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-18-Output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 18 Output}, numbers=left, frame=single]
Presentation verifizieren (Verifier)...

Proof erfolgreich verifiziert!

Verifizierte Attribute (REVEALED):
   - facility_type: Umspannwerk Nord-Ost
   - epoch_valid_until: 1765033200
   - cert_type: Notfall-Wartungsberechtigung
   - role: Notfalltechniker
   - epoch_valid_from: 1765026000

PER ZKP VERIFIZIERT ABER DURCH DATENSCHUTZ GESCHÜTZT (UNREVEALED):
   - Vorname: NICHT offengelegt (Zero-Knowledge-Proof)
   - Nachname: NICHT offengelegt (Zero-Knowledge-Proof)
   - Organisation: NICHT offengelegt (Zero-Knowledge-Proof)

   State:     done
   Verified:  true

Credential ist NICHT revoked (gültig)

============================================================
ZEITGÜLTIGKEITS-PRÜFUNG
============================================================
   Aktueller Zeitpunkt: 2025-12-06 14:00:00 (Epoch: 1765029600) ==> Beispielwert
   Gültig ab:            2025-12-06 13:00:00 (Epoch: 1765026000)
   Gültig bis:           2025-12-06 15:00:00 (Epoch: 1765033200)

   Zertifikat ist ZEITLICH GÜLTIG
============================================================

============================================================
ZERO-KNOWLEDGE-PROOF AUSWERTUNG
============================================================
   Predicate erfüllt: security_clearance_level >= 2 (Ü2)
   Zero-Knowledge-Proof: Exakte Sicherheitsstufe NICHT offengelegt!
   Techniker hat mindestens Ü2 (Erweiterte Sicherheitsüberprüfung)
============================================================

============================================================
FINALE ZUGRIFFSENTSCHEIDUNG
============================================================

ZUGANG GEWÄHRT
  Credential ist gültig (nicht revoked)
  Zertifikat ist zeitlich gültig
  Sicherheitsfreigabe >= Ü2 (Zero-Knowledge-Proof)

   Zugang zum Umspannwerk Nord-Ost GEWÄHRT
============================================================

Privacy-Preserving Verification erfolgreich (DSGVO-konform)
\end{lstlisting}

\subsubsection{Teil 7: Revocation \& Deletion- Beendigung der Wartungsberechtigung}
\label{sec:Teil7-Revocation-Deletion}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-19}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 19}, numbers=left, frame=single]
# Cell 19: Revocation Registries anzeigen

print("="*60)
print("ISSUER: Revocation Registries")
print("="*60 + "\n")

registries = api_get(
    ISSUER_ADMIN_URL,
    "/revocation/registries/created?state=active"
)

if registries and "rev_reg_ids" in registries:
    reg_ids = registries["rev_reg_ids"]
    print(f"Aktive Revocation Registries: {len(reg_ids)}\n")
    
    for idx, reg_id in enumerate(reg_ids, 1):
        print(f"Registry #{idx}: {reg_id}")
        
        # Registry Details abrufen
        reg_info = api_get(ISSUER_ADMIN_URL, f"/revocation/registry/{reg_id}").get('result', 'N/A')
        if reg_info:
            print(f"   State: {reg_info.get('state')}")
            print(f"   Max Credentials: {reg_info.get('max_cred_num')}")
            print(f"   Issuer DID: {reg_info.get('issuer_did', 'N/A')}")
            print(f"   Tails Hash: {reg_info.get('tails_hash', 'N/A')}")
            print(f"   Tails Location: {reg_info.get('tails_local_path', 'N/A')}")
            print(f"   Created: {reg_info.get('created_at', 'N/A')}\n")
else:
    print("Keine aktiven Revocation Registries gefunden")
    print("   Stelle sicher dass Cell 7 mit support_revocation: True läuft")
    print("   Führe Cell 13 aus um Credential mit Revocation auszustellen")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-19-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 19 Output}, numbers=left, frame=single]
============================================================
ISSUER: Revocation Registries
============================================================

Aktive Revocation Registries: 2

Registry #1: 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e
   State: active
   Max Credentials: 100
   Issuer DID: did:indy:9pbXiFBZZGwXKp61HQBz3J
   Tails Hash: 34NrwsR9LbHRHoarwVMmTP95Zr7KJhzqqFzCpFRUw1K4
   Tails Location: /home/aries/.indy_client/tails/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e/34NrwsR9LbHRHoarwVMmTP95Zr7KJhzqqFzCpFRUw1K4
   Created: 2025-12-08T13:03:43.498262Z

Registry #2: 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a
   State: active
   Max Credentials: 100
   Issuer DID: did:indy:9pbXiFBZZGwXKp61HQBz3J
   Tails Hash: 47xo7WNa2sbxHfnUkenzLgT5LotNci9Z1M5Dsp9vgnAN
   Tails Location: /home/aries/.indy_client/tails/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:746cf728-5192-4266-9903-2ffb4b52cc5a/47xo7WNa2sbxHfnUkenzLgT5LotNci9Z1M5Dsp9vgnAN
   Created: 2025-12-08T13:03:49.508221Z

============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-20}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 20}, numbers=left, frame=single]
# Cell 20: Credential REVOKEN (staged)

print("="*60)
print("Credential REVOKEN (staged)")
print("="*60 + "\n")

# Verwende gespeicherte Werte aus Cell 13
if 'rev_reg_id' in locals() and 'cred_rev_id' in locals():
    print(f"Zu revokendes Credential:")
    print(f"   Rev Reg ID: {rev_reg_id}")
    print(f"   Cred Rev ID: {cred_rev_id}\n")
    
    revoke_request = {
        "rev_reg_id": rev_reg_id,
        "cred_rev_id": cred_rev_id,
        "notify": False,  # Kein Notification-Webhook
        "publish": False,  # Staging only, publishe später in Cell 21
        "comment": "Revoked for testing purposes"
    }
    
    print("Sende Revocation Request (staging)...")
    start_time = time.time()
    
    revoke_response = api_post(
        ISSUER_ADMIN_URL,
        "/revocation/revoke",
        revoke_request
    )
    
    if revoke_response is not None:
        print(f"\nCredential erfolgreich REVOKED (staged)")
        print(f"   Status: Pending (noch nicht auf Ledger)")
        print(f"\nNächster Schritt: Führe Cell 22 aus um auf Ledger zu publishen!")
    else:
        print(f"\nRevocation fehlgeschlagen")
        print(f"   Prüfe ob die IDs korrekt sind")
else:
    print("Keine Revocation IDs gefunden!")
    print("   Führe Cell 13 aus um Credential auszustellen")
    print("   Die IDs werden automatisch in rev_reg_id und cred_rev_id gespeichert")

print("\n" + "="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-20-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 20 Output}, numbers=left, frame=single]
============================================================
Credential REVOKEN (staged)
============================================================

Zu revokendes Credential:
   Rev Reg ID: 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e
   Cred Rev ID: 1

Sende Revocation Request (staging)...

Credential erfolgreich REVOKED (staged)
   Status: Pending (noch nicht auf Ledger)

Nächster Schritt: Führe Cell 21 aus um auf Ledger zu publishen!

============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-21}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 21}, numbers=left, frame=single]
# Cell 21: Revocations auf Ledger PUBLISHEN

print("="*60)
print("PUBLISH Revocations auf Ledger")
print("="*60 + "\n")

print("Publishing alle pending Revocations auf Ledger...")

publish_response = api_post(
    ISSUER_ADMIN_URL,
    "/revocation/publish-revocations",
    {}
)

if publish_response and "rrid2crid" in publish_response:
    revoked_regs = publish_response["rrid2crid"]
    total_revoked = sum(len(creds) for creds in revoked_regs.values())
    
    print(f"\nERFOLGREICH auf Ledger published:")
    print(f"   Revocation Registries: {len(revoked_regs)}")
    print(f"   Total revoked Credentials: {total_revoked}")
    
    for reg_id, cred_ids in revoked_regs.items():
        print(f"   Registry: {reg_id}")
        print(f"      Revoked Credential IDs: {cred_ids}")
        print()
    
    print("="*60)
    print("Revocations sind jetzt auf dem VON Ledger!")
    print("   Holder kann nun KEINEN Non-Revocation-Proof mehr erstellen")
    print("="*60)
elif publish_response:
    print(f"\n Keine pending Revocations zum Publishen")
    print(f"   Response: {publish_response}")
else:
    print(f"\nPublish fehlgeschlagen")

print("Jetzt erneut Cell 14 - 18 ausführen um Revocation zu testen!")

# ========================================
# Zeige Ledger-Transaktionen
# ========================================
print("\n" + "="*60)
print("LEDGER-TRANSAKTIONEN")
print("="*60)

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])

        # Filtere nach seqNo 14
        target_seqnos = [14]
        found_txns = []

        for txn in ledger_txns:
            txn_metadata = txn.get('txnMetadata', {})
            seqno = txn_metadata.get('seqNo')

            if seqno in target_seqnos:
               found_txns.append(txn)

        # Sortiere nach seqNo aufsteigend
        found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

        if found_txns:
            print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

            for txn in found_txns:
                txn_metadata = txn.get('txnMetadata', {})
                txn_data = txn.get('txn', {})
                seqno = txn_metadata.get('seqNo')
                txn_type = txn_data.get('type')
                txn_time = txn_metadata.get('txnTime', 'N/A')

                # Typ-Mapping
                type_names = {
                    '0' : 'NODE (Validator Node Registration)',
                    '1' : 'NYM (DID Registration)',
                    '4' : 'TXN_AUTHOR_AGREEMENT',
                    '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                    '100': 'ATTRIB',
                    '101': 'SCHEMA',
                    '102': 'CRED_DEF',
                    '112': 'CHANGE_KEY',
                    '113': 'REVOC_REG_DEF',
                    '114': 'REVOC_REG_ENTRY',
                    '120': 'AUTH_RULE'
                }
                type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                print(f"============================================================")
                print(f"Seq No: {seqno} | Time: {txn_time}")
                print(f"Transaction Type: {type_name}")
                print(f"============================================================\n")

                # Zeige vollständige Transaction
                pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                print()

        else:
            print(f"\n NYM-Transaktion für DID {did_short} nicht gefunden")
            print(f"   (Möglicherweise noch nicht im Cache)")
    else:
        print(f"\n Konnte Ledger nicht abrufen: {ledger_response.status_code}")

except Exception as e:
    print(f"\n Fehler beim Abrufen der Ledger-Transaktion: {e}")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-21-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 21 Output}, numbers=left, frame=single]
============================================================
PUBLISH Revocations auf Ledger
============================================================

Publishing alle pending Revocations auf Ledger...

ERFOLGREICH auf Ledger published:
   Revocation Registries: 1
   Total revoked Credentials: 1
   Registry: 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e
      Revoked Credential IDs: ['1']

============================================================
Revocations sind jetzt auf dem VON Ledger!
   Holder kann nun KEINEN Non-Revocation-Proof mehr erstellen
============================================================
Jetzt erneut Cell 14 - 18 ausführen um Revocation zu testen!

============================================================
LEDGER-TRANSAKTIONEN
============================================================

1 Transaktionen gefunden:

============================================================
Seq No: 14 | Time: 1765203987
Transaction Type: REVOC_REG_ENTRY
============================================================


============================================================
  Ledger Transaction (seqNo 14)
============================================================
{
  "auditPath": [
    "5YBFLtPW8BxW8JhBXAct4wLUhP53zefZY3svqsoq12K2",
    "8vLEbJ5pzVFmhXYiddtoRA8iz7KA8KCANaNKP2Xy2K7q",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP"
  ],
  "ledgerSize": 14,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "3PQ8M9mis3aFgJUtiZ967NuzfiNETFgoMVFTSoFb2X
        XjxSXbRvpY3XyCGQQ3x5Wq9F59tmC4XjiH2vmeqeRvmXFz"
      }
    ]
  },
  "rootHash": "9CzKs9zaUrzJQZSM6kNBFPGWSK4vhWHio8awwFmor89h",
  "txn": {
    "data": {
      "revocDefType": "CL_ACCUM",
      "revocRegDefId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
      "value": {
        "accum": "21 10FFD4FEA22501333DD90FE2ACAED00D89DDEBD4326C222A556E3D4DC966F90A6 21 1119735266EC6AB3CEA26BD9DF760C432DEF72F05170081D3D21AA08E1925F853 6 658131B72B1FAA445702E4E54BE53B6EAFBDB82C8E5A48A43196E2667C1C7343 4 138A7DE4DE0713F07E6C973744ACE5BB2BD75E2025119E8E974E1B3249A52B58 6 5CEC9C8657AACB8B09DEBC2A8D359DCA50F697224BD5072EAAE0E0B9FA14858A 4 2AC552A3DBBCB37A4571A3DF16362A4E39E14DE1A2988E1E07FC33D4903208A1",
        "prevAccum": "1 081A884D0F31C44CED8D593690160326DDA4D4A302A40D1D6D9A2A1989C2AD04 1 1ADE5EFDE35825811BD2847263F6E8F5496CFBD3BB0CC22D6AAA6E7570C0705E 1 05FCCFA509A7F4B691FE817A590C6CAB35B6AF6E4466ED8D9F646F74015B3A73 1 09D63D90931D8601856540BD56CB6A1C39948A1DB96D313522FB3B039399A3DB 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000",
        "revoked": [
          1
        ]
      }
    },
    "metadata": {
      "digest": "c32f4f2146f75733a911846b3443fbbbe19f7216a6ca30bfe75391fb7fb8f243",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "d996231ba1751d4a86d27050e963be69c8c3f72fafccfa3e1911e5b8be583505",
      "reqId": 1765203987850691081
    },
    "protocolVersion": 2,
    "type": "114"
  },
  "txnMetadata": {
    "seqNo": 14,
    "txnId": "5:9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:9efc6971-b713-465f-85db-a192b107b73e",
    "txnTime": 1765203987
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\pagebreak

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-14-output-revocation}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 14 Output Revocation}, numbers=left, frame=single]
Holder Wallet - Gespeicherte Credentials:


============================================================
Credential #1
============================================================
Referent: 39ac5fc4-efc2-45eb-9a21-01c589757b65
Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
Revoked Status: True

Attribute:
  - security_clearance_level: 2
  - name: Mustermann
  - facility_type: Umspannwerk Nord-Ost
  - epoch_valid_from: 1765026000
  - epoch_valid_until: 1765033200
  - cert_type: Notfall-Wartungsberechtigung
  - first_name: Max
  - organisation: Musterfirma GmbH
  - role: Notfalltechniker
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-15-output-revocation}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 15 Output Revocation}, numbers=left, frame=single]
Proof Request senden (Verifier --> Holder)...

Proof Request für KRITIS-Zutrittsberechtigung:
   Endpoint:      /present-proof-2.0/send-request
   Connection:    19df1913-7858-439c-9089-5a947b671c47
   Cred Def ID:   9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
   Proof Name:    Proof of KRITIS Emergency Maintenance Authorization
   REVEALED:      Berechtigung, Anlage, Sicherheitsstufe, Gültigkeitszeitraum, Rolle (6 Attribute)
   DATENSCHUTZ: Identität (Vor-/Nachname) und Organisation werden NICHT offengelegt (Selective-Disclosure)

Proof Request gesendet
   Exchange ID: e46cd865-d9b8-47c5-a551-258c1be4b82e
   State:       request-sent
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-16-output-revocation}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 16 Output Revocation}, numbers=left, frame=single]
Holder: Proof Request verarbeiten...

Holder hat Proof Request empfangen
   Presentation Exchange ID: e87389ed-d2a8-47f4-9f41-ef3309d2f62b

Verfügbare Credentials für Presentation:
   Gefunden: 1 Credential(s)

   Credential #1:
      Referent: 39ac5fc4-efc2-45eb-9a21-01c589757b65
      Schema ID: 9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintenance_cert:1.1
      Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default
      Attributes:
         - cert_type: Notfall-Wartungsberechtigung
         - name: Mustermann
         - role: Notfalltechniker
         - epoch_valid_from: 1765026000
         - facility_type: Umspannwerk Nord-Ost
         - first_name: Max
         - organisation: Musterfirma GmbH
         - epoch_valid_until: 1765033200
         - security_clearance_level: 2

Total: 1 passende Credential(s) gefunden
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-17-output-revocation}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 17 Output Revocation}, numbers=left, frame=single]
Holder: Presentation senden...

Credentials für Presentation gefunden
   Format: Liste mit 1 Credential(s)

   Mapped attribute 'attr3_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr1_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped predicate 'pred1_clearance' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr4_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr2_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65
   Mapped attribute 'attr5_referent' --> Credential 39ac5fc4-efc2-45eb-9a21-01c589757b65

Presentation Mapping:
   Requested Attributes: 5
   Requested Predicates: 1
   Total Mapped: 6

Sende Presentation...

Holder hat Presentation gesendet
   Presentation Exchange ID: e87389ed-d2a8-47f4-9f41-ef3309d2f62b
   State: presentation-sent
   Role: prover

   Warte auf Verification durch Verifier...

   Finaler Holder Status: done
   Presentation erfolgreich abgeschlossen!
   Record bleibt verfügbar (--preserve-exchange-records aktiv)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-18-output-revocation}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 18 Output Revocation}, numbers=left, frame=single]
Presentation verifizieren (Verifier)...

Proof erfolgreich verifiziert!

Verifizierte Attribute (REVEALED):
   - epoch_valid_from: 1765026000
   - epoch_valid_until: 1765033200
   - facility_type: Umspannwerk Nord-Ost
   - cert_type: Notfall-Wartungsberechtigung
   - role: Notfalltechniker

PER ZKP VERIFIZIERT ABER DURCH DATENSCHUTZ GESCHÜTZT (UNREVEALED):
   - Vorname: NICHT offengelegt (Zero-Knowledge-Proof)
   - Nachname: NICHT offengelegt (Zero-Knowledge-Proof)
   - Organisation: NICHT offengelegt (Zero-Knowledge-Proof)

   State:     done
   Verified:  false

Credential ist REVOKED!

============================================================
ZEITGÜLTIGKEITS-PRÜFUNG
============================================================
   Aktueller Zeitpunkt: 2025-12-06 14:00:00 (Epoch: 1765029600) ==> Beispielwert
   Gültig ab:            2025-12-06 13:00:00 (Epoch: 1765026000)
   Gültig bis:           2025-12-06 15:00:00 (Epoch: 1765033200)

   Zertifikat ist ZEITLICH GÜLTIG
============================================================

============================================================
ZERO-KNOWLEDGE-PROOF AUSWERTUNG
============================================================
   Predicate erfüllt: security_clearance_level >= 2 (Ü2)
   Zero-Knowledge-Proof: Exakte Sicherheitsstufe NICHT offengelegt!
   Techniker hat mindestens Ü2 (Erweiterte Sicherheitsüberprüfung)
============================================================

============================================================
FINALE ZUGRIFFSENTSCHEIDUNG
============================================================

ZUGANG VERWEIGERT
   Credential ist REVOKED

   Zugang zum Umspannwerk Nord-Ost VERWEIGERT
============================================================

Privacy-Preserving Verification erfolgreich (DSGVO-konform)
\end{lstlisting}

\pagebreak

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-22}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 22}, numbers=left, frame=single]
# Cell 22: Erst Zelle 14-18 ausführen... Credential Deletion - Holder löscht Credential aus Wallet

print("Credential Deletion - Holder löscht Credential aus Wallet\n")

# ========================================
# VORHER: Zeige alle Credentials
# ========================================
print("="*60)
print("HOLDER WALLET - CREDENTIALS VORHER")
print("="*60)

holder_creds_before = api_get(HOLDER_ADMIN_URL, "/credentials")

credentials_to_delete = []

if holder_creds_before and "results" in holder_creds_before and len(holder_creds_before["results"]) > 0:
    print(f"\n Holder hat {len(holder_creds_before['results'])} Credential(s)\n")

    for idx, cred in enumerate(holder_creds_before["results"], 1):
      referent = cred.get('referent', 'N/A')
      schema_id = cred.get('schema_id', 'N/A')
      cred_def_id = cred.get('cred_def_id', 'N/A')

      print(f"Credential #{idx}:")
      print(f"   Referent:    {referent}")
      print(f"   Schema ID:   {schema_id[:50]}...")
      print(f"   Cred Def ID: {cred_def_id[:50]}...")

      # Revoked Status prüfen
      revoked_response = api_get(HOLDER_ADMIN_URL, f'/credential/revoked/{referent}')
      revoked_status = revoked_response.get('revoked', 'N/A') if revoked_response else 'N/A'
      print(f"   Revoked:     {revoked_status}")

      # Attribute anzeigen
      if "attrs" in cred:
          print(f"   Attribute:")
          for attr_name, attr_value in list(cred["attrs"].items()):
              print(f"      - {attr_name}: {attr_value}")

      # Speichere Referent für Deletion
      credentials_to_delete.append(referent)
      print()
else:
    print("  Holder hat noch keine Credentials im Wallet")
    print("   Führe zuerst Cell 14-18 aus (Schema --> Cred Def --> Issuance)")

print("="*60)

# ========================================
# DELETION: Lösche alle Credentials
# ========================================
if len(credentials_to_delete) > 0:
    print("\n  CREDENTIAL DELETION STARTEN\n")

    deleted_count = 0
    failed_count = 0

    for idx, credential_id in enumerate(credentials_to_delete, 1):
        print(f"  Lösche Credential #{idx} (Referent: {credential_id[:30]}...)")

        # API: DELETE /credential/{credential_id}
        delete_response = api_delete(
            HOLDER_ADMIN_URL,
            f"/credential/{credential_id}"
        )

        if delete_response is not None or delete_response == {}:
            print(f"    Credential gelöscht")
            deleted_count += 1
        else:
            print(f"    Fehler beim Löschen")
            failed_count += 1
        print()

    print("="*60)
    print(f" DELETION SUMMARY")
    print("="*60)
    print(f"   Gelöscht: {deleted_count}/{len(credentials_to_delete)}")
    if failed_count > 0:
        print(f"   Fehler:   {failed_count}")
    print("="*60)
else:
    print("\n  Keine Credentials zum Löschen vorhanden")

# ========================================
# NACHHER: Zeige verbleibende Credentials
# ========================================
print("\n" + "="*60)
print(" HOLDER WALLET - CREDENTIALS NACHHER")
print("="*60)

holder_creds_after = api_get(HOLDER_ADMIN_URL, "/credentials")

if holder_creds_after and "results" in holder_creds_after and len(holder_creds_after["results"]) > 0:
    print(f"\n Holder hat noch {len(holder_creds_after['results'])} Credential(s)\n")

    for idx, cred in enumerate(holder_creds_after["results"], 1):
        referent = cred.get('referent', 'N/A')
        schema_id = cred.get('schema_id', 'N/A')
        cred_def_id = cred.get('cred_def_id', 'N/A')

        print(f"Credential #{idx}:")
        print(f"   Referent:    {referent}")
        print(f"   Schema ID:   {schema_id[:50]}...")
        print(f"   Cred Def ID: {cred_def_id[:50]}...")

        # Revoked Status prüfen
        revoked_response = api_get(HOLDER_ADMIN_URL, f'/credential/revoked/{referent}')
        revoked_status = revoked_response.get('revoked', 'N/A') if revoked_response else 'N/A'
        print(f"   Revoked:     {revoked_status}")

        # Attribute anzeigen
        if "attrs" in cred:
            print(f"   Attribute:")
            for attr_name, attr_value in list(cred["attrs"].items()):
                print(f"      - {attr_name}: {attr_value}")
        print()
else:
    print("\n Holder Wallet ist jetzt LEER (alle Credentials gelöscht)")

print("="*60)

print("\nHinweis:")
print("   Das Löschen eines Credentials aus dem Holder Wallet bedeutet:")
print("   -  Credential ist LOKAL im Wallet gelöscht")
print("   -  Credential ist NICHT auf dem Ledger revoked")
print("   -   Issuer kann das Credential weiterhin sehen (--preserve-exchange-records)")
print("   -   Für echte Revocation: Siehe Cell 32-34 (Revocation Workflow)")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-22-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 22 Output}, numbers=left, frame=single]
Credential Deletion - Holder löscht Credential aus Wallet

============================================================
HOLDER WALLET - CREDENTIALS VORHER
============================================================

Holder hat 1 Credential(s)

Credential #1:
   Referent:    39ac5fc4-efc2-45eb-9a21-01c589757b65
   Schema ID:   9pbXiFBZZGwXKp61HQBz3J:2:kritis_emergency_maintena...
   Cred Def ID: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default...
   Revoked:     True
   Attribute:
      - role: Notfalltechniker
      - organisation: Musterfirma GmbH
      - cert_type: Notfall-Wartungsberechtigung
      - name: Mustermann
      - facility_type: Umspannwerk Nord-Ost
      - epoch_valid_until: 1765033200
      - security_clearance_level: 2
      - epoch_valid_from: 1765026000
      - first_name: Max

============================================================

CREDENTIAL DELETION STARTEN

Lösche Credential #1 (Referent: 39ac5fc4-efc2-45eb-9a21-01c589...)
   Credential gelöscht

============================================================
DELETION SUMMARY
============================================================
   Gelöscht: 1/1
============================================================

============================================================
HOLDER WALLET - CREDENTIALS NACHHER
============================================================

Holder Wallet ist jetzt LEER (alle Credentials gelöscht)
============================================================

Hinweis:
   Das Löschen eines Credentials aus dem Holder Wallet bedeutet:
   - Credential ist LOKAL im Wallet gelöscht
   - Credential ist NICHT auf dem Ledger revoked
   -  Issuer kann das Credential weiterhin sehen (--preserve-exchange-records)
   -  Für echte Revocation: Siehe Cell 32-34 (Revocation Workflow)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-23}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 23}, numbers=left, frame=single]
# Cell 23: Revocation Registry ROTATION (Optional)

print("="*60)
print("Revocation Registry ROTATION")
print("="*60 + "\n")

print("Registry Rotation ist nötig wenn:")
print("   Registry voll ist (100 Credentials mit TAILS_FILE_COUNT=100)")
print("   Neue Registry für weitere Credentials benötigt wird\n")

if 'cred_def_id' in locals() and cred_def_id:
    print(f"Rotiere Registry für Cred Def: {cred_def_id[:50]}...\n")
    
    start_time = time.time()
    
    rotate_response = api_post(
        ISSUER_ADMIN_URL,
        f"/revocation/active-registry/{cred_def_id}/rotate",
        {}
    )
    
    duration = time.time() - start_time
    
    if rotate_response and "rev_reg_ids" in rotate_response:
        decom = rotate_response["rev_reg_ids"]
        print(f"Registry erfolgreich rotiert!")
        print(f"   Decommissioned Registries: {len(decom)}")
        for reg_id in decom:
            print(f"      - {reg_id[:50]}...")
        print(f"   Zeit: {duration:.3f}s")
        print(f"\n   Neue aktive Registry wurde erstellt")
        print(f"   Weitere Credentials können jetzt ausgestellt werden")
    else:
        print(f"Rotation nicht möglich")
        print(f"   Möglicherweise ist die Registry noch nicht voll")
        print(f"   Oder es gibt keine aktive Registry für diese Cred Def")
        if rotate_response:
            print(f"   Response: {rotate_response}")
else:
    print("Keine Credential Definition ID gefunden")
    print("   Führe Cell 7 aus um Cred Def zu erstellen")

# ========================================
# Zeige Ledger-Transaktionen
# ========================================
print("\n" + "="*60)
print("LEDGER-TRANSAKTIONEN")
print("="*60)

try:
    # Hole Domain Ledger Transaktionen
    ledger_response = requests.get(f"{VON_NETWORK_URL}/ledger/domain")

    if ledger_response.status_code == 200:
        ledger_data = ledger_response.json()
        ledger_txns = ledger_data.get('results', [])

        # Filtere nach seqNo 15-18
        target_seqnos = [15,16,17,18]
        found_txns = []

        for txn in ledger_txns:
            txn_metadata = txn.get('txnMetadata', {})
            seqno = txn_metadata.get('seqNo')

            if seqno in target_seqnos:
               found_txns.append(txn)

        # Sortiere nach seqNo aufsteigend
        found_txns.sort(key=lambda x: x.get('txnMetadata', {}).get('seqNo', 0))

        if found_txns:
            print(f"\n{len(found_txns)} Transaktionen gefunden:\n")

            for txn in found_txns:
                txn_metadata = txn.get('txnMetadata', {})
                txn_data = txn.get('txn', {})
                seqno = txn_metadata.get('seqNo')
                txn_type = txn_data.get('type')
                txn_time = txn_metadata.get('txnTime', 'N/A')

                # Typ-Mapping
                type_names = {
                    '0' : 'NODE (Validator Node Registration)',
                    '1' : 'NYM (DID Registration)',
                    '4' : 'TXN_AUTHOR_AGREEMENT',
                    '5' : 'TXN_AUTHOR_AGREEMENT_AML',
                    '100': 'ATTRIB',
                    '101': 'SCHEMA',
                    '102': 'CRED_DEF',
                    '112': 'CHANGE_KEY',
                    '113': 'REVOC_REG_DEF',
                    '114': 'REVOC_REG_ENTRY',
                    '120': 'AUTH_RULE'
                }
                type_name = type_names.get(str(txn_type), f'Type {txn_type}')

                print(f"============================================================")
                print(f"Seq No: {seqno} | Time: {txn_time}")
                print(f"Transaction Type: {type_name}")
                print(f"============================================================\n")

                # Zeige vollständige Transaction
                pretty_print(txn, f"Ledger Transaction (seqNo {seqno})")
                print()

        else:
            print(f"\n  NYM-Transaktion für DID {did_short} nicht gefunden")
            print(f"   (Möglicherweise noch nicht im Cache)")
    else:
        print(f"\n  Konnte Ledger nicht abrufen: {ledger_response.status_code}")

except Exception as e:
    print(f"\n  Fehler beim Abrufen der Ledger-Transaktion: {e}")

print("="*60)
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-23-output}
\begin{lstlisting}[language=python, caption={Jupyter Notebook Cell 23 Output}, numbers=left, frame=single]
============================================================
Revocation Registry ROTATION
============================================================

Registry Rotation ist nötig wenn:
   Registry voll ist (100 Credentials mit TAILS_FILE_COUNT=100)
   Neue Registry für weitere Credentials benötigt wird

Rotiere Registry für Cred Def: 9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default...

Registry erfolgreich rotiert!
   Decommissioned Registries: 2
      - 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:...
      - 9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:...
   Zeit: 9.240s

   Neue aktive Registry wurde erstellt
   Weitere Credentials können jetzt ausgestellt werden

============================================================
 LEDGER-TRANSAKTIONEN
============================================================

4 Transaktionen gefunden:

============================================================
Seq No: 15 | Time: 1765204680
Transaction Type: REVOC_REG_DEF
============================================================


============================================================
  Ledger Transaction (seqNo 15)
============================================================
{
  "auditPath": [
    "DYTvAiJWJquEG4T6nczCMN5cDmhQnwGueDpD1xac18cR",
    "EMGMNnNpvSpg7QReuEGGpeH5PmGacDcmfxQJFHnnPk3j",
    "8vLEbJ5pzVFmhXYiddtoRA8iz7KA8KCANaNKP2Xy2K7q",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP",
    "EXMbifQNEjrLsthAMg6fV1cMyYecd7Zq6X9XnJxmT69D"
  ],
  "ledgerSize": 18,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "4UtWmZaPxLan3Bd1bj1dyC2oHAfW4UZ6afzxuEQKas
        PfZvBdPf71shyUGc3asTDtdBaxK8iEohD1c5duzaRrV9Wb"
      }
    ]
  },
  "rootHash": "Ds9kSEoz9MyFezpb7VzUGxW6UcCeVievHkP3kcywNMRK",
  "txn": {
    "data": {
      "credDefId": "9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default",
      "id": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:f4627aa8-cd65-4d82-933d-3926a0c8dfbf",
      "revocDefType": "CL_ACCUM",
      "tag": "f4627aa8-cd65-4d82-933d-3926a0c8dfbf",
      "value": {
        "issuanceType": "ISSUANCE_BY_DEFAULT",
        "maxCredNum": 100,
        "publicKeys": {
          "accumKey": {
            "z": "1 04DE04E87D0BDE75DFB33D3E50068E355BA1EC188B6D842F3EFFE7C1AF0BC9CF 1 039F921C3145E6E8398446E8D5EE70984B65C311C4983807569DD7A8C46C643F 1 1D663B4362C20BF47EF4B241309D68BFC2FE601884D899B0C24ADB8DB005CD54 1 2116CBF8301A5153CB6C7101A6DC6A26EE1857A5B192A4948FC63F7C2371284D 1 118054A8CA7BF15588D913C51C59DB368B71D007323DFD6517DB5978CA8ADF4F 1 1A59E88D132973B0F820EF157BF3F1AEAE0CB7E79EB193BC637A26DC4EAA3B62 1 20F1CC03F7562924321AF23336877D138FE046B4CEFA667BC20ED2F4B673275D 1 00F1E196990CCE6AEF2CADA00E59B421D8091B277651FB061DBD8F5C58418C00 1 07A3A63EDDCFFEB6A8F66C1B48F32FCA31CDAB982F8778CED5322A28BB0815D3 1 1235D079A86E966BF1DE7E3CDBEE2EE106090EB34991350C62F312DC7341B269 1 00C0C58D37DC4FC9964322C1A2B80EF43202B80C1FF24DA3CF6F72593EBD52F8 1 1363C943EF8C33FF2C0EF30547CDD8BCA6DFD94FEE4C05DF655F6750B21FF1D7"
          }
        },
        "tailsHash": "3WUqN4BfHNpg5fG3Kv7BScb6jX83u178qpDWwix1CqSD",
        "tailsLocation": "https://host.docker.internal:6543/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:f4627aa8-cd65-4d82-933d-3926a0c8dfbf"
      }
    },
    "metadata": {
      "digest": "98c20262e889aee4c4a671b0809905b164557ba4b0e274dca8b56e8031b21807",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "fa828434660d5cbeb4f5075dff26d8952125935ebd26bdf6ace85e2a15df5722",
      "reqId": 1765204680753177624
    },
    "protocolVersion": 2,
    "type": "113"
  },
  "txnMetadata": {
    "seqNo": 15,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:f4627aa8-cd65-4d82-933d-3926a0c8dfbf",
    "txnTime": 1765204680
  },
  "ver": "1"
}

============================================================
Seq No: 16 | Time: 1765204683
Transaction Type: REVOC_REG_ENTRY
============================================================


============================================================
  Ledger Transaction (seqNo 16)
============================================================
{
  "auditPath": [
    "22mQjUSSjktyj47kdfBTJymGGZvnWGwa4kyjtF7K1PcR",
    "EMGMNnNpvSpg7QReuEGGpeH5PmGacDcmfxQJFHnnPk3j",
    "8vLEbJ5pzVFmhXYiddtoRA8iz7KA8KCANaNKP2Xy2K7q",
    "3edQ5ymPLwVpBdhaYGETeewRj1X48jeuZxzHnoDYCgNP",
    "EXMbifQNEjrLsthAMg6fV1cMyYecd7Zq6X9XnJxmT69D"
  ],
  "ledgerSize": 18,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "4TgYaZSqiVgmmpM3hxytd2N92gtPY8kEFiKh8F2KZA
        X9gnDwRZgTGZM3f7L9qTm7UBB5LBXGcvFBVkFtECYAihpn"
      }
    ]
  },
  "rootHash": "Ds9kSEoz9MyFezpb7VzUGxW6UcCeVievHkP3kcywNMRK",
  "txn": {
    "data": {
      "revocDefType": "CL_ACCUM",
      "revocRegDefId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:f4627aa8-cd65-4d82-933d-3926a0c8dfbf",
      "value": {
        "accum": "1 0DAF27F4FEB436691788A87FD1835C3F8DC35830BC5DA79EACD252AE15C46771 1 097E08BD45E7EC731682EB021FB932035BA43B7C81AC5DF02D80B1613F8F8994 1 2004CC2C27326D40B7E13B1A2A49BD937A8654B7952A7F20BDCCAFF26CD40F9A 1 09592D9C5C66487700093A72D6AD084CECDF38F9EAFA18B114BE7FC44020BA2B 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000"
      }
    },
    "metadata": {
      "digest": "493ebaf254ef338f2bd1e694ce8bb73a7797abe1ed868eb406e5116108fede2e",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "b9a64019cd34dafdf400e91b1147e91e3d5b9fb40bd61c1399caf06f011c2652",
      "reqId": 1765204680863160586
    },
    "protocolVersion": 2,
    "type": "114"
  },
  "txnMetadata": {
    "seqNo": 16,
    "txnId": "5:9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:f4627aa8-cd65-4d82-933d-3926a0c8dfbf",
    "txnTime": 1765204683
  },
  "ver": "1"
}

============================================================
Seq No: 17 | Time: 1765204686
Transaction Type: REVOC_REG_DEF
============================================================


============================================================
  Ledger Transaction (seqNo 17)
============================================================
{
  "auditPath": [
    "AKwZjqUk3GtqKFoB3hWPcT8zXn9LiVPvVdgtsfKQawcJ",
    "HevxLnvDxDJPFdQv9pt2ancM2MRGnkHrD2stXzvvTFzg"
  ],
  "ledgerSize": 18,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "ta5zuAF4hQ6oEJwGpq2CJwnwUaGaY97BVZDskDQU338
        iAvTr7FJ9dLy5qnQDj9ihuuQAzEzxq2v5Pqp4dfLRcT6"
      }
    ]
  },
  "rootHash": "Ds9kSEoz9MyFezpb7VzUGxW6UcCeVievHkP3kcywNMRK",
  "txn": {
    "data": {
      "credDefId": "9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default",
      "id": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:742a69d5-a130-42e4-b052-3e4d9c80910b",
      "revocDefType": "CL_ACCUM",
      "tag": "742a69d5-a130-42e4-b052-3e4d9c80910b",
      "value": {
        "issuanceType": "ISSUANCE_BY_DEFAULT",
        "maxCredNum": 100,
        "publicKeys": {
          "accumKey": {
            "z": "1 242AC12CD4E32554A453156CF77DDEE8E4375F1B9491D8D0DDF19D55D5C7803E 1 20579E13429F5C578DC66E0E3AF1C86B38A17E2BA7EF7647E6534056DBEAA832 1 1E769B6633DB9FF6293E55551F6E22A59351F416CB95F194BC7ECA4C008FE660 1 20E8FE1746CCBAB6B66E2F1CFD629C5FEDC872AAE116A39CB18F0965CBE24DD6 1 201411E1DD59576410D5EA133EFB08EEA7E01210BCE71852D2D10E2112427C12 1 11C0F13B16ADC12C769A4A66CB6FC601199084BD0D08E7D7EFDA2BFAF6B88B06 1 036C3030B009C9B78D253BAF8D68DE13C85F952DF6BABC92BB28F19008A74B99 1 0686D758926A30CBE9718B560F19FE6AE0FCBF6F202638EDD0ED14782E1F6778 1 09391DC25B35D363BC70113BF5360BA9E01DD4F6783F4F40DDE490DC75ACF45F 1 07A3A5CF83D42076BEE4C7351DE05E336BCBC188DD457519B0CF668E691919A9 1 1906826C9F44FB77D6F892CCC2DF56B814F86025D035C721FCA49D07DAE56695 1 1B3056E4E0147E7282F4E7B5F5F225D7DF344EDD1044F95725C62900EA0C132C"
          }
        },
        "tailsHash": "BwauSxtMkyPJbZhzAUvQos5WEm3vnbrPyXASQcXoFwfT",
        "tailsLocation": "https://host.docker.internal:6543/9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:742a69d5-a130-42e4-b052-3e4d9c80910b"
      }
    },
    "metadata": {
      "digest": "8ded22d128f2c9d934b943f16f279b79b5bbe739ff27e8513a22213cfe6c3de1",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "ada7b69a040cc9494a218b0d0e7385cb7e71b77f5ac9d7400f5d70b0b00e3419",
      "reqId": 1765204683896845321
    },
    "protocolVersion": 2,
    "type": "113"
  },
  "txnMetadata": {
    "seqNo": 17,
    "txnId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:742a69d5-a130-42e4-b052-3e4d9c80910b",
    "txnTime": 1765204686
  },
  "ver": "1"
}

============================================================
Seq No: 18 | Time: 1765204689
Transaction Type: REVOC_REG_ENTRY
============================================================


============================================================
  Ledger Transaction (seqNo 18)
============================================================
{
  "auditPath": [
    "7HAwp9wERniLASxMrjgJEVvrYb91kFaXEa8WyoB3geC9",
    "HevxLnvDxDJPFdQv9pt2ancM2MRGnkHrD2stXzvvTFzg"
  ],
  "ledgerSize": 18,
  "reqSignature": {
    "type": "ED25519",
    "values": [
      {
        "from": "9pbXiFBZZGwXKp61HQBz3J",
        "value": "PgyVEwJqR79KnfXqSydKr42cWRDXqxEC8mf8js17cu
        iB7LHf342DFmkybmniNn4KSPgZqKeanrPfAw94oaHk7bd"
      }
    ]
  },
  "rootHash": "Ds9kSEoz9MyFezpb7VzUGxW6UcCeVievHkP3kcywNMRK",
  "txn": {
    "data": {
      "revocDefType": "CL_ACCUM",
      "revocRegDefId": "9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:742a69d5-a130-42e4-b052-3e4d9c80910b",
      "value": {
        "accum": "1 0606EE56A5BCF6BA9891C46CFF9F35683D16921F296582C8D6527BFF9635C227 1 14512AB48B9285E95C10CB1C68C725BA8BDC6AC91E97C0452AA9CE43666EE1FF 1 22CFE64BBD9A9FC9E19D044A2B882A22E32F4143883305C6D8483F97FABAC88A 1 02E993ED2B5FF74156D62A156465813375320E239A2CDA80B0A41050E14F0A7D 2 095E45DDF417D05FB10933FFC63D474548B7FFFF7888802F07FFFFFF7D07A8A8 1 0000000000000000000000000000000000000000000000000000000000000000"
      }
    },
    "metadata": {
      "digest": "4b39d39e7a96a913eb3d081ef24bbc3b4818975d376f27ab45e0a76296a17f89",
      "from": "9pbXiFBZZGwXKp61HQBz3J",
      "payloadDigest": "5cfc62ed993545382455ec924af6dd8c62eec159c2c39311bacc8bd47dc5c808",
      "reqId": 1765204686874332420
    },
    "protocolVersion": 2,
    "type": "114"
  },
  "txnMetadata": {
    "seqNo": 18,
    "txnId": "5:9pbXiFBZZGwXKp61HQBz3J:4:9pbXiFBZZGwXKp61HQBz3J:3:CL:8:default:CL_ACCUM:742a69d5-a130-42e4-b052-3e4d9c80910b",
    "txnTime": 1765204689
  },
  "ver": "1"
}

============================================================
\end{lstlisting}

\subsection{Validierung der KRITIS-Compliance-Anforderungen}

\textbf{Protokollierung Sicherheitsrelevanter Ereignisse}

\refstepcounter{manualListingCounterA}
\label{lst:issuer-acapy-agent-logs}
\begin{lstlisting}[language=bash, caption={Issuer acapy agent logs}, numbers=left, frame=single]
2025-12-10 18:55:12,652 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:12 +0000] "GET /connections HTTP/1.0" 200 3425 "-" "python-requests/2.32.5"
2025-12-10 18:55:12,671 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:12 +0000] "GET /connections/d75e516d-66dd-473a-8aa4-ee417a85ef4e HTTP/1.0" 200 3410 "-" "python-requests/2.32.5"
2025-12-10 18:55:22,852 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:22 +0000] "POST /issue-credential-2.0/send-offer HTTP/1.0" 200 22208 "-" "python-requests/2.32.5"
2025-12-10 18:55:22,854 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO Using PQC pack mode: sender_pqc=True, recipients=['pqc']
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO Using PQC unpack mode: alg=PQC-Authcrypt
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO ================================================================================
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Starting recipient decryption loop
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Found 1 recipients in JWE
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Recipient KIDs: ['39zdn...sovF']
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Trying recipient: 39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG...
2025-12-10 18:55:23,008 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Calling session.fetch_key(39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG...)
2025-12-10 18:55:23,009 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] fetch_key result: PQCKey(algorithm=ml-kem-768)
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Verifying ML-DSA-65 signature...
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]   sender_vk (base58): 5sN6ZWoHgEv8dwS8z7MGrdhGoZvwsV8foHf2tPiT3E5DVrcr28cH5L9JK2JT...
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]   sender_public (bytes): 1952 bytes
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]  Signature verified!
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]  SUCCESS! recip_vk set to: 39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Loop finished. recip_vk = 39zd...ovF
2025-12-10 18:55:23,013 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO ================================================================================
2025-12-10 18:55:23,014 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:23 +0000] "POST / HTTP/1.0" 200 75 "-" "Python/3.12 aiohttp/3.12.15"
2025-12-10 18:55:23,107 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO Using PQC pack mode: sender_pqc=True, recipients=['pqc']
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO Using PQC unpack mode: alg=PQC-Authcrypt
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO ================================================================================
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Starting recipient decryption loop
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Found 1 recipients in JWE
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Recipient KIDs: ['39zdnQamP...TAsovF']
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Trying recipient: 39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG...
2025-12-10 18:55:23,250 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Calling session.fetch_key(39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG...)
2025-12-10 18:55:23,251 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] fetch_key result: PQCKey(algorithm=ml-kem-768)
2025-12-10 18:55:23,254 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Verifying ML-DSA-65 signature...
2025-12-10 18:55:23,254 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]   sender_vk (base58): 5sN6ZWoHgEv8dwS8z7MGrdhGoZvwsV8foHf2tPiT3E5DVrcr28cH5L9JK2JT...
2025-12-10 18:55:23,254 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]   sender_public (bytes): 1952 bytes
2025-12-10 18:55:23,255 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] ok    Signature verified!
2025-12-10 18:55:23,255 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG]  SUCCESS! recip_vk set to: 39zdnQamPoRAWZiAT3YAA3ns1XnqMi9TtiCTVeSJW215p5pQUKJWm5bvYekG
2025-12-10 18:55:23,255 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO [PQC UNPACK DEBUG] Loop finished. recip_vk = 39zd...LEj9gsyymTAsovF
2025-12-10 18:55:23,255 pqc_didpeer4_fm.v1_0.pqc_didcomm_v1 INFO ================================================================================
2025-12-10 18:55:23,256 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:23 +0000] "POST / HTTP/1.0" 200 75 "-" "Python/3.12 aiohttp/3.12.15"
2025-12-10 18:55:27,891 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:27 +0000] "GET /issue-credential-2.0/records/90e02709-9ab2-4fca-b2e4-bb43d51f2191 HTTP/1.0" 200 43641 "-" "python-requests/2.32.5"
2025-12-10 18:55:27,937 aiohttp.access INFO 172.23.0.3 [10/Dec/2025:18:55:27 +0000] "GET /issue-credential-2.0/records HTTP/1.0" 200 43656 "-" "python-requests/2.32.5"
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:issuer-nginx-logs}
\begin{lstlisting}[language=bash, caption={Issuer Sidecar-Proxy logs}, numbers=left, frame=single]
172.23.0.1 - - [10/Dec/2025:18:55:10 +0000] "POST /out-of-band/create-invitation?auto_accept=true&multi_use=false HTTP/1.1" 200 16180 "-" "python-requests/2.32.5" "-"
2025/12/10 18:55:10 [info] 8#0: *23 client 172.23.0.1 closed keepalive connection
2025/12/10 18:55:10 [warn] 8#0: *25 a client request body is buffered to a temporary file /opt/nginx/client_body_temp/0000000001, client: 172.23.0.1, server: pqc-sidecarproxy-issuer, request: "POST / HTTP/1.1", host: "host.docker.internal:8020"
172.23.0.1 - - [10/Dec/2025:18:55:10 +0000] "POST / HTTP/1.1" 200 0 "-" "Python/3.12 aiohttp/3.12.15" "-"
2025/12/10 18:55:10 [warn] 8#0: *25 a client request body is buffered to a temporary file /opt/nginx/client_body_temp/0000000002, client: 172.23.0.1, server: pqc-sidecarproxy-issuer, request: "POST / HTTP/1.1", host: "host.docker.internal:8020"
172.23.0.1 - - [10/Dec/2025:18:55:10 +0000] "POST / HTTP/1.1" 200 0 "-" "Python/3.12 aiohttp/3.12.15" "-"
2025/12/10 18:55:11 [warn] 8#0: *25 a client request body is buffered to a temporary file /opt/nginx/client_body_temp/0000000003, client: 172.23.0.1, server: pqc-sidecarproxy-issuer, request: "POST / HTTP/1.1", host: "host.docker.internal:8020"
172.23.0.1 - - [10/Dec/2025:18:55:11 +0000] "POST / HTTP/1.1" 200 0 "-" "Python/3.12 aiohttp/3.12.15" "-"
172.23.0.1 - - [10/Dec/2025:18:55:12 +0000] "GET /connections HTTP/1.1" 200 3300 "-" "python-requests/2.32.5" "-"
2025/12/10 18:55:12 [info] 8#0: *29 client 172.23.0.1 closed keepalive connection
172.23.0.1 - - [10/Dec/2025:18:55:12 +0000] "GET /connections/d75e516d-66dd-473a-8aa4-ee417a85ef4e HTTP/1.1" 200 3285 "-" "python-requests/2.32.5" "-"
2025/12/10 18:55:12 [info] 8#0: *31 client 172.23.0.1 closed keepalive connection
172.23.0.1 - - [10/Dec/2025:18:55:22 +0000] "POST /issue-credential-2.0/send-offer HTTP/1.1" 200 22082 "-" "python-requests/2.32.5" "-"
2025/12/10 18:55:22 [info] 8#0: *33 client 172.23.0.1 closed keepalive connection
2025/12/10 18:55:23 [warn] 8#0: *25 a client request body is buffered to a temporary file /opt/nginx/client_body_temp/0000000004, client: 172.23.0.1, server: pqc-sidecarproxy-issuer, request: "POST / HTTP/1.1", host: "host.docker.internal:8020"
172.23.0.1 - - [10/Dec/2025:18:55:23 +0000] "POST / HTTP/1.1" 200 0 "-" "Python/3.12 aiohttp/3.12.15" "-"
2025/12/10 18:55:23 [warn] 8#0: *25 a client request body is buffered to a temporary file /opt/nginx/client_body_temp/0000000005, client: 172.23.0.1, server: pqc-sidecarproxy-issuer, request: "POST / HTTP/1.1", host: "host.docker.internal:8020"
172.23.0.1 - - [10/Dec/2025:18:55:23 +0000] "POST / HTTP/1.1" 200 0 "-" "Python/3.12 aiohttp/3.12.15" "-"
172.23.0.1 - - [10/Dec/2025:18:55:27 +0000] "GET /issue-credential-2.0/records/90e02709-9ab2-4fca-b2e4-bb43d51f2191 HTTP/1.1" 200 43515 "-" "python-requests/2.32.5" "-"
2025/12/10 18:55:46 [info] 11#0: *102 SSL_do_handshake() failed (SSL: error:0A000102:SSL routines::unsupported protocol) while SSL handshaking, client: 172.23.0.1, server: 0.0.0.0:8021
\end{lstlisting}

\subsection{Validierung der Kryptoagilität - Applikationslayer}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-9-Demonstration-Kryptoagilität-ed25519}
\begin{lstlisting}[language=bash, caption={Jupyter Notebook Cell 9 Demonstration Kryptoagilität ed25519}, numbers=left, frame=single]
# Cell 9: Connection Issuer Holder erstellen

print("Connection: Issuer --> Holder erstellen...\n")

start_time = time.time()

# ========================================
# 1. PRE-CHECK: Existierende Connections prüfen
# ========================================
print("Pre-Check: Prüfe existierende Connections...\n")

existing_conn_issuer_holder = None
existing_conn_holder = None

# Prüfe Issuer-Seite: GET /connections
issuer_conns_response = api_get(ISSUER_ADMIN_URL, "/connections")
if issuer_conns_response and "results" in issuer_conns_response:
    print(f"   Issuer: {len(issuer_conns_response['results'])} Connection(s) gefunden")
    for conn in issuer_conns_response["results"]:
        if conn.get("state") == "completed":
            existing_conn_issuer_holder = conn["connection_id"]
            print(f"     Connection {existing_conn_issuer_holder} (State: completed)")
            break
else:
    print("   Issuer: Keine Connections gefunden")

# Prüfe Holder-Seite: GET /connections
holder_conns_response = api_get(HOLDER_ADMIN_URL, "/connections")
if holder_conns_response and "results" in holder_conns_response:
    print(f"   Holder: {len(holder_conns_response['results'])} Connection(s) gefunden")
    for conn in holder_conns_response["results"]:
        if conn.get("state") == "completed":
            existing_conn_holder = conn["connection_id"]
            print(f"     Connection {existing_conn_holder} (State: completed)")
            break
else:
    print("   Holder: Keine Connections gefunden")

if existing_conn_issuer_holder and existing_conn_holder:
    print(f"\nExistierende Connection gefunden!")
    print(f"   Issuer Connection ID: {existing_conn_issuer_holder}")
    print(f"   Holder Connection ID: {existing_conn_holder}")
    print("   Überspringe Connection-Erstellung\n")
    conn_id_issuer_holder = existing_conn_issuer_holder
    conn_id_holder = existing_conn_holder
else:
    print("\n   Keine vollständige existierende Connection gefunden")
    print("   Erstelle neue Connection mit did:peer:4...\n")
    
    # ========================================
    # 2. HAUPT-WORKFLOW: Connection mit did:peer erstellen
    # ========================================
    
    # Initialisiere Variablen
    conn_id_issuer_holder = None
    conn_id_holder = None
    invitation_msg_id = None
    invitation_key = None
    
    # Issuer erstellt Out-of-Band Invitation mit did:peer:4
    invitation_data = {
        "handshake_protocols": ["https://didcomm.org/didexchange/1.1"],
        "use_did_method": "did:peer:4",
        "metadata": {"key_type": "ed25519"},
        "my_label": "Issuer Agent",
        "goal": "Establish connection for credential issuance",
        "goal_code": "issue-vc",
        "accept": [
            "didcomm/aip1",
            "didcomm/aip2;env=rfc19"
        ]
    }
    
    print("Invitation-Modus: did:peer:4 (P2P Connection)")
    
    # API Call mit Query-Parametern für auto-accept
    invitation_response = api_post(
        ISSUER_ADMIN_URL,
        "/out-of-band/create-invitation?auto_accept=true&multi_use=false",
        invitation_data
    )
    
    if invitation_response and "invitation" in invitation_response:
        invitation = invitation_response["invitation"]
        oob_id = invitation_response.get("oob_id")
        invitation_url = invitation_response.get("invitation_url", "")
        state = invitation_response.get("state")
        
        # WICHTIG: Speichere invitation_msg_id aus der Invitation
        invitation_msg_id = invitation.get("@id")
        
        # Optional: Extrahiere invitation_key aus services (falls vorhanden)
        services = invitation.get("services", [])
        if services and len(services) > 0:
            # Bei did:peer:4 ist der Service oft inline
            service = services[0]
            if isinstance(service, dict):
                recipient_keys = service.get("recipientKeys", [])
                if recipient_keys:
                    invitation_key = recipient_keys[0]
        
        print(f"Issuer: Invitation erstellt")
        print(f"   OOB ID: {oob_id}")
        print(f"   State: {state}")
        print(f"   Invitation Msg ID: {invitation_msg_id}")
        if invitation_key:
            print(f"   Invitation Key: {invitation_key[:30]}...")
        print(f"   Services: {len(invitation.get('services', []))} service(s)")
        print(f"   Label: {invitation_data['my_label']}")
        print(f"   Goal: {invitation_data['goal']}")
        
        # Holder akzeptiert Invitation (mit auto-accept)
        receive_response = api_post(
            HOLDER_ADMIN_URL,
            "/out-of-band/receive-invitation?auto_accept=true",
            invitation
        )
        
        if receive_response is not None:
            conn_id_holder = receive_response.get("connection_id")
            print(f"\nHolder: Invitation akzeptiert")
            print(f"   Connection ID (Holder): {conn_id_holder}")
            
            # Issuer findet seine Connection via invitation_msg_id
            print(f"\n   Issuer sucht Connection via invitation_msg_id...")
            time.sleep(2)  # Kurze Wartezeit für DIDExchange Protocol
            
            # Hole alle Issuer Connections
            issuer_conns = api_get(ISSUER_ADMIN_URL, "/connections")
            if issuer_conns and "results" in issuer_conns:
                connections = issuer_conns["results"]
                
                print(f"     Prüfe {len(connections)} Connection(s)...")
                
                # Finde Connection anhand invitation_msg_id (exaktes Match!)
                connection_found = False
                for conn in connections:
                    conn_invitation_msg_id = conn.get("invitation_msg_id")
                    conn_invitation_key = conn.get("invitation_key")
                    
                    # Match via invitation_msg_id (bevorzugt)
                    if invitation_msg_id and conn_invitation_msg_id == invitation_msg_id:
                        conn_id_issuer_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\n   Issuer: Connection gefunden via invitation_msg_id!")
                        print(f"      Connection ID: {conn_id_issuer_holder}")
                        print(f"      State: {conn_state}")
                        print(f"      Invitation Msg ID: {conn_invitation_msg_id}")
                        print(f"      Their Label: {conn.get('their_label', 'N/A')}")
                        print(f"      Updated At: {conn.get('updated_at', 'N/A')}")
                        connection_found = True
                        break
                    
                    # Fallback: Match via invitation_key
                    elif invitation_key and conn_invitation_key == invitation_key:
                        conn_id_issuer_holder = conn["connection_id"]
                        conn_state = conn.get("state")
                        print(f"\n   Issuer: Connection gefunden via invitation_key!")
                        print(f"     Connection ID: {conn_id_issuer_holder}")
                        print(f"     State: {conn_state}")
                        print(f"     Invitation Key: {conn_invitation_key[:30]}...")
                        print(f"     Their Label: {conn.get('their_label', 'N/A')}")
                        connection_found = True
                        break
                
                if not connection_found:
                    print(f"      Keine Connection mit matching invitation_msg_id gefunden")
                    print(f"     Gesuchte invitation_msg_id: {invitation_msg_id}")
            else:
                print(f"     Konnte Issuer Connections nicht abrufen")
        else:
            print("Holder konnte Invitation nicht akzeptieren")
    else:
        print("Fehler beim Erstellen der Invitation")

# ========================================
# 3. POST-VALIDATION: Connection-Status verifizieren
# ========================================
print("\nPost-Validation: Verifiziere Connection-Status...\n")

validation_success = False
issuer_state = None
holder_state = None

if conn_id_issuer_holder:
    # Prüfe Issuer-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    issuer_conn_detail = api_get(
        ISSUER_ADMIN_URL,
        f"/connections/{conn_id_issuer_holder}"
    )
    
    if issuer_conn_detail:
        issuer_state = issuer_conn_detail.get("state")
        issuer_their_label = issuer_conn_detail.get("their_label")
        issuer_their_role = issuer_conn_detail.get("their_role")
        issuer_invitation_msg_id = issuer_conn_detail.get("invitation_msg_id")
        
        print(f"   Issuer Connection {conn_id_issuer_holder}:")
        print(f"    State: {issuer_state}")
        print(f"    Their Label: {issuer_their_label}")
        print(f"    Their Role: {issuer_their_role}")
        print(f"    Invitation Msg ID: {issuer_invitation_msg_id}")

# Prüfe Holder-Seite (falls conn_id_holder bekannt)
if conn_id_holder:
    # Prüfe Holder-Seite mit spezifischem Endpoint: GET /connections/{conn_id}
    holder_conn_detail = api_get(
        HOLDER_ADMIN_URL,
        f"/connections/{conn_id_holder}"
    )
    
    if holder_conn_detail:
        holder_state = holder_conn_detail.get("state")
        holder_their_label = holder_conn_detail.get("their_label")
        holder_their_role = holder_conn_detail.get("their_role")
        holder_invitation_msg_id = holder_conn_detail.get("invitation_msg_id")
        
        print(f"\n   Holder Connection {conn_id_holder}:")
        print(f"      State: {holder_state}")
        print(f"      Their Label: {holder_their_label}")
        print(f"      Their Role: {holder_their_role}")
        print(f"      Invitation Msg ID: {holder_invitation_msg_id}")
        
        # Verifiziere dass invitation_msg_id übereinstimmt
        if invitation_msg_id and issuer_invitation_msg_id == holder_invitation_msg_id:
            print(f"      Invitation Msg IDs stimmen überein!")

# Validierung
if issuer_state == "active" and holder_state == "active":
    validation_success = True
elif issuer_state == "active" and not conn_id_holder:
    # Bei existierender Connection nur Issuer-Seite bekannt
    validation_success = True

# Finale Ausgabe
print(f"\n{'='*60}")
if validation_success:
    print(f"CONNECTION ESTABLISHED (did:peer:4): Issuer <--> Holder")
    print(f"   Issuer Connection ID: {conn_id_issuer_holder}")
    if conn_id_holder:
        print(f"   Holder Connection ID: {conn_id_holder}")
    print(f"   Issuer State: {issuer_state}")
    if holder_state:
        print(f"   Holder State: {holder_state}")
    if invitation_msg_id:
        print(f"   Invitation Msg ID: {invitation_msg_id}")
else:
    print(f"CONNECTION STATUS UNCLEAR")
    print(f"   Issuer State: {issuer_state if issuer_state else 'N/A'}")
    print(f"   Holder State: {holder_state if holder_state else 'N/A'}")
    if conn_id_issuer_holder:
        print(f"   Issuer Connection ID: {conn_id_issuer_holder}")
    if conn_id_holder:
        print(f"   Holder Connection ID: {conn_id_holder}")

print(f"{'='*60}")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-9-Demonstration-Kryptoagilität-ed25519-output}
\begin{lstlisting}[language=bash, caption={Jupyter Notebook Cell 9 Demonstration Kryptoagilität ed25519 Output}, numbers=left, frame=single]
Connection: Issuer --> Holder erstellen...

Pre-Check: Prüfe existierende Connections...

   Issuer: 0 Connection(s) gefunden
   Holder: 0 Connection(s) gefunden

   Keine vollständige existierende Connection gefunden
   Erstelle neue Connection mit did:peer:4...

Invitation-Modus: did:peer:4 (P2P Connection)
Issuer: Invitation erstellt
   OOB ID: 6def3ce1-a8d2-49bd-ad8d-9a43a0fde93d
   State: initial
   Invitation Msg ID: a4d6d5e4-53b2-420b-a7c1-d50e3aa6efe9
   Services: 1 service(s)
   Label: Issuer Agent
   Goal: Establish connection for credential issuance

Holder: Invitation akzeptiert
   Connection ID (Holder): 539bd397-21c2-4330-9d61-428a0e5a544d

   Issuer sucht Connection via invitation_msg_id...
     Prüfe 1 Connection(s)...

   Issuer: Connection gefunden via invitation_msg_id!
      Connection ID: 3dd04340-dc11-4fc7-afda-86b011a9ac11
      State: active
      Invitation Msg ID: a4d6d5e4-53b2-420b-a7c1-d50e3aa6efe9
      Their Label: Holder Agent
      Updated At: 2025-12-12T12:03:58.769683Z

Post-Validation: Verifiziere Connection-Status...

   Issuer Connection 3dd04340-dc11-4fc7-afda-86b011a9ac11:
      State: active
      Their Label: Holder Agent
      Their Role: invitee
      Invitation Msg ID: a4d6d5e4-53b2-420b-a7c1-d50e3aa6efe9

   Holder Connection 539bd397-21c2-4330-9d61-428a0e5a544d:
      State: active
      Their Label: Issuer Agent
      Their Role: inviter
      Invitation Msg ID: a4d6d5e4-53b2-420b-a7c1-d50e3aa6efe9
      Invitation Msg IDs stimmen überein!

============================================================
 CONNECTION ESTABLISHED (did:peer:4): Issuer <--> Holder
   Issuer Connection ID: 3dd04340-dc11-4fc7-afda-86b011a9ac11
   Holder Connection ID: 539bd397-21c2-4330-9d61-428a0e5a544d
   Issuer State: active
   Holder State: active
   Invitation Msg ID: a4d6d5e4-53b2-420b-a7c1-d50e3aa6efe9
============================================================
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-11-Demonstration-Kryptoagilität-ed25519}
\begin{lstlisting}[language=bash, caption={Jupyter Notebook Cell 11 Demonstration Kryptoagilität ed25519}, numbers=left, frame=single]
# Cell 12 Wallet DID Übersicht anzeigen

print("Wallet-Übersicht - Alle DIDs:\n")

# Alle DIDs von allen Agenten abrufen
issuer_dids = api_get(ISSUER_ADMIN_URL, "/wallet/did")
holder_dids = api_get(HOLDER_ADMIN_URL, "/wallet/did")
verifier_dids = api_get(VERIFIER_ADMIN_URL, "/wallet/did")

#Zeige originale Responses
print("Originale /wallet/did Responses:\n")
if issuer_dids:
    pretty_print(issuer_dids, "Issuer Wallet DIDs")
if holder_dids:
    pretty_print(holder_dids, "Holder Wallet DIDs")
if verifier_dids:
    pretty_print(verifier_dids, "Verifier Wallet DIDs")
\end{lstlisting}

\refstepcounter{manualListingCounterA}
\label{lst:Jupyter-Notebook-Cell-11-Demonstration-Kryptoagilität-ed25519-output}
\begin{lstlisting}[language=bash, caption={Jupyter Notebook Cell 11 Demonstration Kryptoagilität ed25519 Output}, numbers=left, frame=single]
Wallet-Übersicht - Alle DIDs:

Originale /wallet/did Responses:


============================================================
  Issuer Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:peer:4zQmVgo4trzrW244SytguXneycfvEgHHnsZKF7SibioFfk3a",
      "verkey": "C4ypQhnn6R6g6Nth5LFrx7FNAwWjWiHSG1JDLfpRr3Ud",
      "posture": "wallet_only",
      "key_type": "ed25519",
      "method": "did:peer:4",
      "metadata": {}
    },
    {
      "did": "did:peer:4zQmVgo4trzrW244SytguXneycfvEgHHnsZKF7SibioFfk3a:zKXbgJ6nwB...6R7R5Tu6g8XDQSx",
      "verkey": "C4ypQhnn6R6g6Nth5LFrx7FNAwWjWiHSG1JDLfpRr3Ud",
      "posture": "wallet_only",
      "key_type": "ed25519",
      "method": "did:peer:4",
      "metadata": {}
    },
    {
      "did": "did:peer:4zQme8gBSG5x813d5KGKWig9tihua8dPSbENuqW7KNvkzo1h:zKXbgJ...gnfJBRT3cUDa",
      "verkey": "GqqE1rZfqJ8koS7CYznRwm1729YxXkHyaAK9YeRcedKT",
      "posture": "wallet_only",
      "key_type": "ed25519",
      "method": "did:peer:4",
      "metadata": {
        "invitation_reuse": "true"
      }
    }
  ]
}

============================================================
  Holder Wallet DIDs
============================================================
{
  "results": [
    {
      "did": "did:peer:4zQmd6WegsSPVbYbxDG5gcTTPgV748PBeKg3fxVw3Q6fYysv",
      "verkey": "9rVcKr3YMR3nCbcPcxyx8GyEhJ3rMtcwxuTtgoTpzpQv",
      "posture": "wallet_only",
      "key_type": "ed25519",
      "method": "did:peer:4",
      "metadata": {}
    },
    {
      "did": "did:peer:4zQmd6WegsSPVbYbxDG5gcTTPgV748PBeKg3fxVw3Q6fYysv:zKXbgJ6nwBf...XpiV5oNAvYMrxt",
      "verkey": "9rVcKr3YMR3nCbcPcxyx8GyEhJ3rMtcwxuTtgoTpzpQv",
      "posture": "wallet_only",
      "key_type": "ed25519",
      "method": "did:peer:4",
      "metadata": {}
    }
  ]
}

============================================================
  Verifier Wallet DIDs
============================================================
{
  "results": []
}
\end{lstlisting}